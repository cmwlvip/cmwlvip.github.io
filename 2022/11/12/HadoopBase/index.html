<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Hadoop 生态 | Ofra Serendipity</title><meta name="author" content="Shiqing Huang"><meta name="copyright" content="Shiqing Huang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="HDFS shell 命令行常用操作  Hive安装(v3.1.2)  Hive使用  大数据导论企业数据分析方向数据是什么 数据是指对客观事件进行记录并可以鉴别的符号，是对客观事物的性质、状态以及相互关系等进行记载的物理符号或这些物理符号的组合，它是可识别的、抽象的符号。 它不仅指狭义上的数字，还可以是具有一定意义的文字、字母、数字符号的组合、图形、图像、视频、音频等，也是客观事物的属性、数量、">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop 生态">
<meta property="og:url" content="https://www.huangshiqing.website/2022/11/12/HadoopBase/index.html">
<meta property="og:site_name" content="Ofra Serendipity">
<meta property="og:description" content="HDFS shell 命令行常用操作  Hive安装(v3.1.2)  Hive使用  大数据导论企业数据分析方向数据是什么 数据是指对客观事件进行记录并可以鉴别的符号，是对客观事物的性质、状态以及相互关系等进行记载的物理符号或这些物理符号的组合，它是可识别的、抽象的符号。 它不仅指狭义上的数字，还可以是具有一定意义的文字、字母、数字符号的组合、图形、图像、视频、音频等，也是客观事物的属性、数量、">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg">
<meta property="article:published_time" content="2022-11-12T21:31:16.000Z">
<meta property="article:modified_time" content="2023-06-10T00:00:00.000Z">
<meta property="article:author" content="Shiqing Huang">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="Hive">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg"><link rel="shortcut icon" href="/img/favicon01.png"><link rel="canonical" href="https://www.huangshiqing.website/2022/11/12/HadoopBase/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":100,"languages":{"author":"作者: Shiqing Huang","link":"链接: ","source":"来源: Ofra Serendipity","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop 生态',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-10 00:00:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "/img/loading.gif" data-lazy-src="/img/avatar002.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-heart-pulse"></i><span> Fun</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book-open"></i><span> Book</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fab fa-steam"></i><span> Game</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Reference</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/2023/02/26/acwing/"><i class="fa-fw fas fa-arrow-right"></i><span> Acwing</span></a></li><li><a class="site-page child" href="/2022/11/03/KeyboardShutcut/"><i class="fa-fw fas fa-arrow-right"></i><span> 实用快捷键</span></a></li><li><a class="site-page child" href="/2022/10/27/TheCharmOfMarkdown/"><i class="fa-fw fas fa-arrow-right"></i><span> 了不起的 Markdown</span></a></li><li><a class="site-page child" href="/2022/11/03/HexoTagPlugins/"><i class="fa-fw fas fa-arrow-right"></i><span> Hexo Built-in Tag Plugins</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.imgdb.cn/item/63766f6616f2c2beb1356f73.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ofra Serendipity</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-heart-pulse"></i><span> Fun</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book-open"></i><span> Book</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fab fa-steam"></i><span> Game</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-star"></i><span> Reference</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/2023/02/26/acwing/"><i class="fa-fw fas fa-arrow-right"></i><span> Acwing</span></a></li><li><a class="site-page child" href="/2022/11/03/KeyboardShutcut/"><i class="fa-fw fas fa-arrow-right"></i><span> 实用快捷键</span></a></li><li><a class="site-page child" href="/2022/10/27/TheCharmOfMarkdown/"><i class="fa-fw fas fa-arrow-right"></i><span> 了不起的 Markdown</span></a></li><li><a class="site-page child" href="/2022/11/03/HexoTagPlugins/"><i class="fa-fw fas fa-arrow-right"></i><span> Hexo Built-in Tag Plugins</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Hadoop 生态</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-12T21:31:16.000Z" title="发表于 2022-11-12 21:31:16">2022-11-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-10T00:00:00.000Z" title="更新于 2023-06-10 00:00:00">2023-06-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Hadoop/">Hadoop</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">30.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>106分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title="Hadoop 生态"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><div class="note default modern"><p><a href="#HDFS-shell命令行常用操作">HDFS shell 命令行常用操作</a></p>
</div>
<div class="note default modern"><p><a href="#Apache-Hive部署实战">Hive安装(v3.1.2)</a></p>
</div>
<div class="note default modern"><p><a href="#Apache-Hive-DML语句与函数使用">Hive使用</a></p>
</div>
<h2 id="大数据导论"><a href="#大数据导论" class="headerlink" title="大数据导论"></a>大数据导论</h2><h3 id="企业数据分析方向"><a href="#企业数据分析方向" class="headerlink" title="企业数据分析方向"></a>企业数据分析方向</h3><h4 id="数据是什么"><a href="#数据是什么" class="headerlink" title="数据是什么"></a>数据是什么</h4><ul>
<li>数据是指对<strong>客观事件进行记录并可以鉴别的符号</strong>，是对客观事物的性质、状态以及相互关系等进行记载的物理符号或这些物理符号的组合，它是可识别的、抽象的符号。</li>
<li>它不仅指狭义上的<strong>数字</strong>，还可以是具有一定意义的<strong>文字、字母、数字符号的组合、图形、图像、视频、音频</strong>等，也是客观事物的属性、数量、位置及其相互关系的抽象表示。例如，“0、1、2…”、“阴、雨、下降”、“学生的档案记录、货物的运输情况”等都是数据。<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-02-20.jpg" style="height:300px">
</li>
</ul>
<h4 id="数据如何产生"><a href="#数据如何产生" class="headerlink" title="数据如何产生"></a>数据如何产生</h4><p>对客观事物的<strong>计量和记录产生</strong>数据<br><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-07-17.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-07-30.png" style="height:200px"></p>
<h4 id="分析方向"><a href="#分析方向" class="headerlink" title="分析方向"></a>分析方向</h4><p><strong>把隐藏在数据背后的信息集中和提炼出来，总结出所研究对象的内在规律，帮助管理者进行有效的判断和决策</strong>。</p>
<p>数据分析在企业日常经营分析中主要有三大方向：<br><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-21-20.png" alt="企业数据分析方向"></p>
</blockquote></p>
<ul>
<li><strong>现状分析</strong>（分析<strong>当下</strong>的数据）：现阶段的整体情况，各个部分的构成占比、发展、变动；</li>
<li><strong>原因分析</strong>（分析<strong>过去</strong>的数据）：某一现状为什么发生，确定原因，做出调整优化；</li>
<li><strong>预测分析</strong>（结合数据预测<strong>未来</strong>）：结合已有数据预测未来发展趋势。</li>
</ul>
<div class="tabs" id="分析方向"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分析方向-1">现状分析</button></li><li class="tab"><button type="button" data-href="#分析方向-2">原因分析</button></li><li class="tab"><button type="button" data-href="#分析方向-3">预测分析</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分析方向-1"><p><strong>实时分析</strong>（Real Time Processing |<strong>Streaming</strong>）</p>
<p>面向当下，分析实时产生的数据；<br>所谓的实时是指从数据产生到数据分析到数据应用的时间间隔很短，可细分秒级、毫秒级。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-27-18.png" alt="实时分析"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分析方向-2"><p><strong>离线分析</strong>（<strong>Batch</strong> Processing）</p>
<p>面向过去，面向<strong>历史</strong>，分析已有的数据；<br>在时间维度明显成<strong>批次性变化</strong>。一周一分析(T+7)，一天一分析（T+1），所以也叫做<strong>批处理</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-29-41.png" alt="离线分析"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分析方向-3"><p><strong>机器学习</strong>（<strong>Machine Learning</strong>）</p>
<p>基于历史数据和当下产生的实时数据预测未来发生的事情；<br>侧重于<strong>数学算法</strong>的运用，如分类、聚类、关联、预测。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-30-55.png" alt="机器学习"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h3 id="数据分析基本步骤"><a href="#数据分析基本步骤" class="headerlink" title="数据分析基本步骤"></a>数据分析基本步骤</h3><p>数据分析步骤（流程）的重要性体现在：对<strong>如何开展数据分析提供了强有力的逻辑支撑</strong>;<br>张文霖在《数据分析六步曲》说，典型的数据分析应该包含以下几个步骤<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-33-05.png" alt="数据分析六步曲"></p>
<h4 id="Step1：明确分析目的和思路"><a href="#Step1：明确分析目的和思路" class="headerlink" title="Step1：明确分析目的和思路"></a>Step1：明确分析目的和思路</h4><ul>
<li>目的是整个分析流程的起点，为数据的收集、处理及分析提供清晰的指引方向；</li>
<li>思路是使<strong>分析框架体系化</strong>，比如先分析什么，后分析什么，使各分析点之间具有逻辑联系，保证分析维度的<strong>完整性</strong>，分析结果的<strong>有效性</strong>以及<strong>正确性</strong>，需要数据分析方法论进行支撑；</li>
<li>数据分析方法论是一些营销管理类相关理论，比如用户行为理论、<strong>PEST分析法</strong>、5W2H分析法等。</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-35-53.png" alt="PEST分析法"></p>
<h4 id="Step2：数据收集"><a href="#Step2：数据收集" class="headerlink" title="Step2：数据收集"></a>Step2：数据收集</h4><ul>
<li>数据<strong>从无到有</strong>的过程：比如传感器收集气象数据、埋点收集用户行为数据</li>
<li>数据<strong>传输搬运</strong>的过程：比如采集数据库数据到数据分析平台</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-37-13.png" alt="数据收集"></p>
<h4 id="Step3：数据处理"><a href="#Step3：数据处理" class="headerlink" title="Step3：数据处理"></a>Step3：数据处理</h4><ul>
<li>准确来说，应该称之为<strong>数据预处理</strong>。</li>
<li>数据预处理需要对收集到的数据进行加工整理，形成适合数据分析的样式，主要包括<strong>数据清洗、数据转化、数据提取、数据计算</strong>；</li>
<li>数据预处理可以保证数据的一致性和有效性，让数据变成干净规整的<strong>结构化数据</strong>。</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-38-59.png" style="height:300px">
<h4 id="Step4：数据分析"><a href="#Step4：数据分析" class="headerlink" title="Step4：数据分析"></a>Step4：数据分析</h4><ul>
<li>用适当的分析方法及分析工具，对处理过的数据进行分析，提取有价值的信息，形成有效结论的过程；</li>
<li>需要掌握各种<strong>数据分析方法</strong>，还要熟悉<strong>数据分析软件</strong>的操作；</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-42-20.png" style="height:300px">
<h4 id="Step5：数据展现"><a href="#Step5：数据展现" class="headerlink" title="Step5：数据展现"></a>Step5：数据展现</h4><ul>
<li>数据展现又称之为<strong>数据可视化</strong>，指的是<strong>分析结果图表展示</strong>，因为人类是视觉动物；</li>
<li>数据可视化（Data Visualization）属于数据应用的一种；</li>
<li>注意，<strong>数据分析的结果不是只有可视化展示</strong>，还可以继续数据挖掘（Data Mining）、即席查询（Ad Hoc）等。</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-42-09.png" style="height:300px">
<h4 id="Step6：报告撰写"><a href="#Step6：报告撰写" class="headerlink" title="Step6：报告撰写"></a>Step6：报告撰写</h4><ul>
<li>数据分析报告是对整个数据分析过程的一个总结与呈现</li>
<li>把数据分析的起因、过程、结果及建议完整地呈现出来，供决策者参考</li>
<li>需要有明确的结论，最好有建议或解决方案</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-44-29.png" style="height:400px">
<div class="note info modern"><p>数据分析</p>
<ul>
<li><strong>一切围绕着数据</strong></li>
<li>通俗描述：<strong>数据从哪里来、数据到哪里去</strong></li>
<li>核心步骤:采集、处理、分析、应用</li>
</ul>
</div>
<h3 id="大数据时代"><a href="#大数据时代" class="headerlink" title="大数据时代"></a>大数据时代</h3><div class="note info modern"><ul>
<li>解决海量数据<strong>储存</strong>问题</li>
<li>解决海量数据<strong>计算</strong>问题</li>
</ul>
</div>
<h4 id="大数据时代背景"><a href="#大数据时代背景" class="headerlink" title="大数据时代背景"></a>大数据时代背景</h4><ul>
<li>最早提出“<strong>大数据</strong>”<strong>时代</strong>到来的是全球知名咨询公司<strong>麦肯锡</strong>，其称：“数据，已经渗透到当今每一个行业和业务职能领域，成为重要的生产因素。人们对于海量数据的挖掘和运用，预示着新一波生产率增长和消费者盈余浪潮的到来。”</li>
<li>2019年，央视推出了国内首部大数据产业题材纪录片《大数据时代》，节目细致而生动地讲述了大数据技术在<strong>政府治理、民生服务、数据安全、工业转型、未来生活</strong>等方面给我们带来的改变和影响。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-29-35.png" alt="大数据时代"></li>
</ul>
<h4 id="大数据定义"><a href="#大数据定义" class="headerlink" title="大数据定义"></a>大数据定义</h4><ul>
<li><strong>大数据（big data）</strong>是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合；</li>
<li>是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-30-31.png" alt="big data"></li>
</ul>
<h4 id="大数据5V特征"><a href="#大数据5V特征" class="headerlink" title="大数据5V特征"></a>大数据5V特征</h4><p>5个V开头的单词，从5个方面准确、生动、形象的介绍了大数据特征。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-31-43.png" alt="大数据5V特征"></p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-32-24.png" alt="大数据5V特征"></p>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><ul>
<li><p><strong>电商领域</strong><br>精准广告位、个性化推荐、大数据杀熟</p>
</li>
<li><p><strong>传媒领域</strong><br>精准营销、猜你喜欢、交互推荐</p>
</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-36-52.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-37-00.png" style="height:200px">
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-41-40.png" style="height:200px">
<ul>
<li><p><strong>金融方面</strong><br>理财投资，通过对个人的信用评估，风险承担能力评估，集合众多理财产品、推荐响应的投资理财产品。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-46-35.png" alt="金融方面"></p>
</li>
<li><p><strong>交通领域</strong><br>拥堵预测、智能红绿灯、导航最优规划</p>
</li>
<li><p><strong>电信领域</strong><br>基站选址优化、舆情监控、客户用户画像</p>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-47-53.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-47-59.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-48-04.png" style="height:200px">
</li>
<li><p><strong>安防领域</strong><br>犯罪预防、天网监控</p>
</li>
<li><p><strong>医疗领域</strong><br>智慧医疗、疾病预防、病源追踪</p>
</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-50-27.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-50-32.png" style="height:200px">
<h3 id="分布式与集群"><a href="#分布式与集群" class="headerlink" title="分布式与集群"></a>分布式与集群</h3><p>分布式、集群是两个不同的概念，但口语中经常混淆二者。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-52-16.png" alt="分布式与集群"></p>
<ul>
<li>分布式、集群的共同点是：<strong>都是多台机器（服务器）组成的</strong>；</li>
<li>因此口语中混淆两者概念的时候都是：<strong>相对于单机来说的</strong>。</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-53-33.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-53-38.png" style="height:200px">
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>数据大爆炸，海量数据处理场景面临问题<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-57-22.png" alt="大数据背景下"></p>
<h2 id="Apache-Hadoop、HDFS"><a href="#Apache-Hadoop、HDFS" class="headerlink" title="Apache Hadoop、HDFS"></a>Apache Hadoop、HDFS</h2><h3 id="Apache-Hadoop概述"><a href="#Apache-Hadoop概述" class="headerlink" title="Apache Hadoop概述"></a>Apache Hadoop概述</h3><h4 id="Hadoop介绍、发展简史、现状"><a href="#Hadoop介绍、发展简史、现状" class="headerlink" title="Hadoop介绍、发展简史、现状"></a>Hadoop介绍、发展简史、现状</h4><h5 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h5><ul>
<li><mark class="hl-label blue">狭义上Hadoop指的是Apache软件基金会的一款开源软件。</mark> 
<p>用java语言实现，开源<br>允许用户使用<strong>简单的编程模型</strong>实现<strong>跨机器</strong>集群对海量数据进行<strong>分布式计算</strong>处理</p>
</li>
<li><mark class="hl-label blue">Hadoop核心组件</mark> 
<p>Hadoop HDFS（分布式文件<strong>存储</strong>系统）：解决海量数据存储<br>Hadoop YARN（集群<strong>资源管理</strong>和任务调度框架）：解决资源任务调度<br>Hadoop MapReduce（分布式<strong>计算</strong>框架）：解决海量数据计算</p>
</li>
<li><mark class="hl-label blue">官网</mark> 
<p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p>
</li>
<li><mark class="hl-label red">广义上Hadoop指的是围绕Hadoop打造的大数据生态圈。</mark> 
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-22-38-08.png" alt="广义Hadoop"></p>
</li>
</ul>
<h5 id="Hadoop发展简史"><a href="#Hadoop发展简史" class="headerlink" title="Hadoop发展简史"></a>Hadoop发展简史</h5><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-22-42-09.png" alt="Hadoop之父"></p>
</blockquote>
<ul>
<li><mark class="hl-label blue">Hadoop之父</mark> 
<p><strong>Doug Cutting</strong></p>
</li>
<li><mark class="hl-label blue">Hadoop起源于ApacheLucene子项目：Nutch</mark> 
<p>Nutch的设计目标是构建一个大型的全网搜索引擎。<br>遇到瓶颈：如何解决数十亿网页的存储和索引问题</p>
</li>
<li><mark class="hl-label red">Google三篇论文</mark> 
<p>《The Google file system》：谷歌分布式文件系统GFS<br>《MapReduce: Simplified Data Processing on Large Clusters》：谷歌分布式计算框架MapReduce<br>《Bigtable: A Distributed Storage System for Structured Data》：谷歌结构化数据存储系统</p>
</li>
</ul>
<h5 id="Hadoop现状"><a href="#Hadoop现状" class="headerlink" title="Hadoop现状"></a>Hadoop现状</h5><ul>
<li><strong>HDFS</strong>作为分布式文件存储系统，处在<strong>生态圈的底层与核心地位</strong>；</li>
<li><strong>YARN</strong>作为分布式通用的集群资源管理系统和任务调度平台，<strong>支撑各种计算引擎运行</strong>，保证了Hadoop地位；</li>
<li><strong>MapReduce</strong>作为大数据生态圈第一代分布式计算引擎，由于自身设计的模型所产生的弊端，导致企业一线<strong>几乎不再直接使用</strong>MapReduce进行编程处理，但是很多软件的底层依然在使用MapReduce引擎来处理数据。</li>
</ul>
<div class="note info modern"><ol>
<li>狭义上Hadoop指软件，广义上Hadoop指生态圈</li>
<li>Hadoop之父Doug Cutting</li>
<li>Hadoop起源于Nutch项目</li>
<li>受Google3篇论文启发</li>
<li>2008年开源给Apache软件基金会</li>
</ol>
</div>
<h4 id="Hadoop特性优点、国内外应用"><a href="#Hadoop特性优点、国内外应用" class="headerlink" title="Hadoop特性优点、国内外应用"></a>Hadoop特性优点、国内外应用</h4><h5 id="Hadoop特性优点"><a href="#Hadoop特性优点" class="headerlink" title="Hadoop特性优点"></a>Hadoop特性优点</h5><ol>
<li><p><strong>扩容能力</strong> scalability<br>Hadoop是在可用的计算机集群间分配数据并完成计算任务的，这些集群可方便灵活的方式扩展到数以千计的节点。</p>
</li>
<li><p><strong>成本低</strong> Economical<br>Hadoop集群允许通过部署普通廉价的机器组成集群来处理大数据，以至于成本很低。看重的是集群整体能力。</p>
</li>
<li><p><strong>效率高</strong> efficiency<br>通过<strong>并发数据</strong>，Hadoop可以在节点之间动态<strong>并行</strong>的移动数据，使得速度非常快。</p>
</li>
<li><p>可靠性 reliability<br>能自动维护数据的多份复制，并且在任务失败后能自动地重新部署（redeploy）计算任务。所以Hadoop的按位存储和处理数据的能力值得人们信赖。</p>
</li>
</ol>
<h5 id="Hadoop国外应用"><a href="#Hadoop国外应用" class="headerlink" title="Hadoop国外应用"></a>Hadoop国外应用</h5><ul>
<li><p>Yahoo<br>支持广告系统<br>用户行为分析<br>支持Web搜索<br>反垃圾邮件系统</p>
</li>
<li><p>Facebook<br>存储处理数据挖掘和日志统计<br>构建基于Hadoop数据仓库平台（Apache Hive来自FB）</p>
</li>
<li><p>IBM<br>蓝云基础设施构建<br>商业化Hadoop发行、解决方案支持</p>
</li>
</ul>
<h5 id="Hadoop国内应用"><a href="#Hadoop国内应用" class="headerlink" title="Hadoop国内应用"></a>Hadoop国内应用</h5><ul>
<li><p>百度<br>用户搜索表征的需求数据、阿拉丁爬虫数据存储<br>数据分析和挖掘竞价排名</p>
</li>
<li><p>阿里巴巴<br>为电子商务网络平台提供底层的基础计算和存储服务<br>交易数据、信用数据</p>
</li>
<li><p>腾讯<br>用户关系数据<br>基于Hadoop、Hive构建TDW（腾讯分布式数据仓库）</p>
</li>
<li><p>华为<br>对Hadoop的HA方案，以及HBase领域有深入研究</p>
</li>
</ul>
<div class="note info modern"><ul>
<li><p>Hadoop成功的魅力—<strong>通用性</strong><br>精准区分做什么和怎么做<br>做什么属于业务问题怎么做属于技术问题。<br>用户负责业务Hadoop负责技术</p>
</li>
<li><p>Hadoop成功的魅力—<strong>简单</strong><br><strong>一个东西你用起来比较简单，可不是你的能力！</strong></p>
</li>
</ul>
</div>
<h4 id="Hadoop发行版本、架构变迁"><a href="#Hadoop发行版本、架构变迁" class="headerlink" title="Hadoop发行版本、架构变迁"></a>Hadoop发行版本、架构变迁</h4><h5 id="Hadoop发行版本"><a href="#Hadoop发行版本" class="headerlink" title="Hadoop发行版本"></a>Hadoop发行版本</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-32-21.png" alt="Hadoop发行版本"></p>
<ul>
<li><p>Apache开源社区版本<br><a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p>
</li>
<li><p>商业发行版本<br>Cloudera: <a target="_blank" rel="noopener" href="https://www.cloudera.com/products/open-source/apache-hadoop.html">https://www.cloudera.com/products/open-source/apache-hadoop.html</a><br>Hortonworks: <a target="_blank" rel="noopener" href="https://www.cloudera.com/products/hdp.html">https://www.cloudera.com/products/hdp.html</a></p>
</li>
</ul>
<h5 id="Hadoop架构变迁（1-0-2-0变迁）"><a href="#Hadoop架构变迁（1-0-2-0变迁）" class="headerlink" title="Hadoop架构变迁（1.0-2.0变迁）"></a>Hadoop架构变迁（1.0-2.0变迁）</h5><ul>
<li><p>Hadoop 1.0<br>HDFS（分布式文件存储）<br>MapReduce（资源管理和分布式数据处理）</p>
</li>
<li><p>Hadoop 2.0<br>HDFS（分布式文件存储）<br>MapReduce（分布式数据处理）<br><strong>YARN</strong>（集群资源管理、任务调度）</p>
</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-38-22.png" alt="Hadoop架构变迁（1.0-2.0变迁）"></p>
<h5 id="Hadoop架构变迁（3-0新版本）"><a href="#Hadoop架构变迁（3-0新版本）" class="headerlink" title="Hadoop架构变迁（3.0新版本）"></a>Hadoop架构变迁（3.0新版本）</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-39-05.png" alt="Hadoop架构变迁（3.0新版本）"><br>Hadoop 3.0架构组件和Hadoop 2.0类似,<strong>3.0着重于性能优化</strong>。</p>
<ul>
<li><p>通用方面<br>精简内核、类路径隔离、shell脚本重构</p>
</li>
<li><p>Hadoop HDFS<br>EC纠删码、多NameNode支持</p>
</li>
<li><p>Hadoop MapReduce<br>任务本地化优化、内存参数自动推断</p>
</li>
<li><p>Hadoop YARN<br>Timeline Service V2、队列配置</p>
</li>
</ul>
<h3 id="Apache-Hadoop集群搭建"><a href="#Apache-Hadoop集群搭建" class="headerlink" title="Apache Hadoop集群搭建"></a>Apache Hadoop集群搭建</h3><h4 id="Hadoop集群简介"><a href="#Hadoop集群简介" class="headerlink" title="Hadoop集群简介"></a>Hadoop集群简介</h4><ul>
<li>Hadoop集群包括两个集群：HDFS集群、YARN集群</li>
<li>两个集群<strong>逻辑上分离、通常物理上在一起</strong>(可单独启动，部署于一台计算机)</li>
<li>两个集群都是标准的<strong>主从架构</strong>集群</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-46-13.png" alt="Hadoop集群"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-48-36.png" alt="Hadoop集群"></p>
<ul>
<li><p>逻辑上分离<br>两个集群<strong>互相之间没有依赖、互不影响</strong></p>
</li>
<li><p>物理上在一起<br>某些角色进程往往<strong>部署在同一台物理服务器上</strong></p>
</li>
<li><p>MapReduce集群呢？<br>MapReduce是计算框架、代码层面的组件没有集群之说</p>
</li>
</ul>
<h4 id="Hadoop集群模式-分布式-安装（Cluster-mode）"><a href="#Hadoop集群模式-分布式-安装（Cluster-mode）" class="headerlink" title="Hadoop集群模式(分布式)安装（Cluster mode）"></a>Hadoop集群模式(分布式)安装（Cluster mode）</h4><p>详细的集群搭建步骤可参考<a href="/2022/10/25/HadoopClusterBuilding3-3-4/" title="Hadoop 3.3.4 集群搭建">Hadoop 3.3.4 集群搭建</a></p>
<h5 id="Hadoop源码编译"><a href="#Hadoop源码编译" class="headerlink" title="Hadoop源码编译"></a>Hadoop源码编译</h5><ul>
<li>安装包、源码包下载地址<br><a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/">https://archive.apache.org/dist/hadoop/common/</a><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-00-08-55.png" alt="Hadoop版本"></li>
<li>为什么要重新编译Hadoop源码?<br>匹配不同<strong>操作系统本地库环境</strong>，Hadoop某些操作比如压缩、IO需要调用系统本地库（<em>.so|</em>.dll）<br><strong>修改源码、重构源码</strong></li>
<li>如何编译Hadoop<br>源码包根目录下文件：BUILDING.txt<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-00-09-25.png" alt="如何编译Hadoop"></li>
</ul>
<h5 id="Step1-集群角色规划"><a href="#Step1-集群角色规划" class="headerlink" title="Step1:集群角色规划"></a>Step1:集群角色规划</h5><ul>
<li><p>角色规划的准则<br>根据软件工作特性和服务器硬件资源情况合理分配<br>比如依赖内存工作的NameNode是不是部署在大内存机器上？</p>
</li>
<li><p>角色规划注意事项<br><strong>资源上有抢夺冲突的，尽量不要部署在一起</strong><br><strong>工作上需要互相配合的。尽量部署在一起</strong></p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>服务器</th>
<th>运行角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>node1.itcast.cn</td>
<td>namenode datanode resourcemanager nodemanager</td>
</tr>
<tr>
<td>node2.itcast.cn</td>
<td>secondarynamenode datanode nodemanager</td>
</tr>
<tr>
<td>node3.itcast.cn</td>
<td>datanode nodemanager</td>
</tr>
</tbody>
</table>
</div>
<h5 id="Step2-服务器基础环境准备"><a href="#Step2-服务器基础环境准备" class="headerlink" title="Step2:服务器基础环境准备"></a>Step2:服务器基础环境准备</h5><ul>
<li><p>主机名（3台机器）<br><code>vim /etc/hostname</code></p>
</li>
<li><p>Hosts映射（3台机器）<br><code>vim /etc/hosts</code></p>
</li>
<li><p>防火墙关闭（3台机器）<br><code>systemctl stop firewalld.service</code> #关闭防火墙<br><code>systemctl disable firewalld.service</code> #禁止防火墙开启自启</p>
</li>
<li><p>ssh免密登录<br><code>ssh-keygen</code>#4个回车生成公钥、私钥<br><code>ssh-copy-id node1</code>、<code>ssh-copy-id node2</code>、<code>ssh-copy-id node3</code></p>
</li>
<li><p>集群时间同步（3台机器）<br><code>yum -y install ntpdate</code><br><code>ntpdate ntp4.aliyun.com</code></p>
</li>
<li><p>创建统一工作目录（3台机器）</p>
</li>
</ul>
<h5 id="Step3-上传安装包、解压安装包"><a href="#Step3-上传安装包、解压安装包" class="headerlink" title="Step3:上传安装包、解压安装包"></a>Step3:上传安装包、解压安装包</h5><ul>
<li>JDK 1.8安装（3台机器）</li>
<li>上传、解压Hadoop安装包</li>
</ul>
<h5 id="Step4-Hadoop安装包目录结构"><a href="#Step4-Hadoop安装包目录结构" class="headerlink" title="Step4:Hadoop安装包目录结构"></a>Step4:Hadoop安装包目录结构</h5><div class="table-container">
<table>
<thead>
<tr>
<th>目录</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>bin</strong></td>
<td>Hadoop最基本的<strong>管理脚本</strong>和使用脚本的目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用Hadoop。</td>
</tr>
<tr>
<td><strong>etc</strong></td>
<td>Hadoop<strong>配置文件</strong>所在的目录</td>
</tr>
<tr>
<td>include</td>
<td>对外提供的编程库头文件（具体动态库和静态库在lib目录中），这些头文件均是用C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序。</td>
</tr>
<tr>
<td>lib</td>
<td>该目录包含了Hadoop对外提供的编程动态库和静态库，与include目录中的头文件结合使用。</td>
</tr>
<tr>
<td>libexec</td>
<td>各个服务对用的shell配置文件所在的目录，可用于配置日志输出、启动参数（比如JVM参数）等基本信息。</td>
</tr>
<tr>
<td><strong>sbin</strong></td>
<td>Hadoop管理脚本所在的目录，主要包含HDFS和YARN中各类服务的<strong>启动/关闭脚本</strong>。</td>
</tr>
<tr>
<td><strong>share</strong></td>
<td>Hadoop各个模块编译后的<strong>jar包</strong>所在的目录，<strong>官方自带示例</strong>。</td>
</tr>
</tbody>
</table>
</div>
<h5 id="配置文件概述"><a href="#配置文件概述" class="headerlink" title="配置文件概述"></a>配置文件概述</h5><ul>
<li><p>官网文档<br><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/">https://hadoop.apache.org/docs/</a></p>
</li>
<li><p>第一类1个: <strong>hadoop-env.sh</strong></p>
</li>
<li><p>第二类4个：xxxx-site.xml ,site表示的是用户定义的配置，会覆盖default中的默认配置。</p>
<ul>
<li><strong>core-site.xml</strong> 核心模块配置</li>
<li><strong>hdfs-site.xml</strong> hdfs文件系统模块配置</li>
<li><strong>mapred-site.xml</strong> MapReduce模块配置</li>
<li><strong>yarn-site.xml</strong> yarn模块配置</li>
</ul>
</li>
<li><p>第三类1个：<strong>workers</strong></p>
</li>
<li>上述的配置文件目录：$HADOOP_HOME/etc/hadoop</li>
</ul>
<h5 id="Step5-编辑Hadoop配置文件-hadoop-env-sh"><a href="#Step5-编辑Hadoop配置文件-hadoop-env-sh" class="headerlink" title="Step5:编辑Hadoop配置文件 hadoop-env.sh"></a>Step5:编辑Hadoop配置文件 hadoop-env.sh</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=jdk安装路径</span><br><span class="line"><span class="comment">#文件最后添加</span></span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件-core-site-xml"><a href="#Step5-编辑Hadoop配置文件-core-site-xml" class="headerlink" title="Step5:编辑Hadoop配置文件 core-site.xml"></a>Step5:编辑Hadoop配置文件 core-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--设置默认使用的文件系统Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 8020/9000/9820均可 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--设置Hadoop本地保存数据路径--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/mysoft/data/hadoop-3.3.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--设置HDFS web UI用户身份--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--整合hive 用户代理设置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--垃圾桶文件保存时间--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1440<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件hdfs-site-xml"><a href="#Step5-编辑Hadoop配置文件hdfs-site-xml" class="headerlink" title="Step5:编辑Hadoop配置文件hdfs-site.xml"></a>Step5:编辑Hadoop配置文件hdfs-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--设置SNN进程运行机器位置信息--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node2:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件-mapred-site-xml"><a href="#Step5-编辑Hadoop配置文件-mapred-site-xml" class="headerlink" title="Step5:编辑Hadoop配置文件 mapred-site.xml"></a>Step5:编辑Hadoop配置文件 mapred-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--设置MR程序默认运行模式：yarn集群模式local本地模式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--MR程序历史服务器端地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--历史服务器web端地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件-yarn-site-xml"><a href="#Step5-编辑Hadoop配置文件-yarn-site-xml" class="headerlink" title="Step5:编辑Hadoop配置文件 yarn-site.xml"></a>Step5:编辑Hadoop配置文件 yarn-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--设置YARN集群主角色运行机器位置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否将对容器实施物理内存限制--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否将对容器实施虚拟内存限制。--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--开启日志聚集--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--设置yarn历史服务器地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://node1:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--保存的时间7天--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件-workers"><a href="#Step5-编辑Hadoop配置文件-workers" class="headerlink" title="Step5:编辑Hadoop配置文件 workers"></a>Step5:编辑Hadoop配置文件 workers</h5><pre><code class="highlight plaintext">node1
node2
node3</code></pre>
<h5 id="Step6-分发同步安装包"><a href="#Step6-分发同步安装包" class="headerlink" title="Step6:分发同步安装包"></a>Step6:分发同步安装包</h5><ul>
<li>在node1机器上将Hadoop安装包scp同步到其他机器</li>
</ul>
<h5 id="Step7-配置Hadoop环境变量"><a href="#Step7-配置Hadoop环境变量" class="headerlink" title="Step7:配置Hadoop环境变量"></a>Step7:配置Hadoop环境变量</h5><ul>
<li><p>在node1上配置Hadoop环境变量<br><code>vim /etc/profile</code><br><code>export HADOOP_HOME=hadoop安装路径</code><br><code>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</code></p>
</li>
<li><p>将修改后的环境变量同步其他机器<br><code>scp /etc/profile root@node2:/etc/</code><br><code>scp /etc/profile root@node3:/etc/</code></p>
</li>
<li><p>重新加载环境变量验证是否生效（3台机器）<br><code>source /etc/profile</code><br><code>hadoop</code> #验证环境变量是否生效</p>
</li>
</ul>
<h5 id="Step8-NameNode-format（格式化操作）"><a href="#Step8-NameNode-format（格式化操作）" class="headerlink" title="Step8:NameNode format（格式化操作）"></a>Step8:NameNode format（格式化操作）</h5><ul>
<li>首次启动HDFS时，必须对其进行格式化操作</li>
<li>format本质上是<strong>初始化工作，进行HDFS清理和准备工作</strong></li>
<li>命令：<br><code>hdfs namenode -format</code></li>
</ul>
<div class="note info modern"><ol>
<li>首次启动之前需要format操作;</li>
<li>format只能进行一次后续不再需要;</li>
<li>如果多次format除了造成数据丢失外，还会导致hdfs集群主从角色之间互不识别。通过删除所有机器hadoop.tmp.dir目录重新format解决</li>
</ol>
</div>
<h4 id="Hadoop集群启停命令、Web-UI"><a href="#Hadoop集群启停命令、Web-UI" class="headerlink" title="Hadoop集群启停命令、Web UI"></a>Hadoop集群启停命令、Web UI</h4><h5 id="手动逐个进程启停"><a href="#手动逐个进程启停" class="headerlink" title="手动逐个进程启停"></a>手动逐个进程启停</h5><p>每台机器上每次手动启动关闭一个角色进程,可以精准控制每个进程启停，避免群起群停。</p>
<ul>
<li><p>HDFS集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop2.x版本命令</span></span><br><span class="line">hadoop-daemon.sh start|stop namenode|datanode|secondarynamenode</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop3.x版本命令</span></span><br><span class="line">hdfs --daemon start|stop namenode|datanode|secondarynamenode</span><br></pre></td></tr></table></figure>
</li>
<li><p>YARN集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop2.x版本命令</span></span><br><span class="line">yarn-daemon.sh start|stop resourcemanager|nodemanager</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop3.x版本命令</span></span><br><span class="line">yarn --daemon start|stop resourcemanager|nodemanager</span><br></pre></td></tr></table></figure>
</li>
<li><p>JobHistoryServer守护进程(19888端口号查看web界面)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop2.x版本命令</span></span><br><span class="line">mr-jobhistory-daemon.sh start|stop historyserver</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop3.x版本命令</span></span><br><span class="line">mapred --daemon start|stop historyserver</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="shell脚本一键启停"><a href="#shell脚本一键启停" class="headerlink" title="shell脚本一键启停"></a>shell脚本一键启停</h5><p>在node1上，使用软件自带的shell脚本一键启动。前提：<strong>配置好机器之间的SSH免密登录和workers文件</strong>。</p>
<ul>
<li><p>HDFS集群<br><code>start-dfs.sh</code><br><code>stop-dfs.sh</code></p>
</li>
<li><p>YARN集群<br><code>start-yarn.sh</code><br><code>stop-yarn.sh</code></p>
</li>
<li><p>Hadoop集群<br><code>start-all.sh</code><br><code>stop-all.sh</code></p>
</li>
</ul>
<h5 id="进程状态、日志查看"><a href="#进程状态、日志查看" class="headerlink" title="进程状态、日志查看"></a>进程状态、日志查看</h5><ul>
<li>启动完毕之后可以使用<strong>jps命令</strong>查看进程是否启动成功</li>
<li>Hadoop启动日志路径：$HADOOP_HOME/logs/</li>
</ul>
<h5 id="HDFS集群web界面"><a href="#HDFS集群web界面" class="headerlink" title="HDFS集群web界面"></a>HDFS集群web界面</h5><p>地址：<a target="_blank" rel="noopener" href="http://namenode_host:9870">http://namenode_host:9870</a></p>
<p>其中namenode_host是namenode运行所在机器的主机名或者ip<br>如果使用主机名访问，别忘了在Windows配置hosts<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-27-37.png" alt="HDFS集群web界面"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-27-57.png" alt="HDFS集群web界面"></p>
<h5 id="YARN集群web界面"><a href="#YARN集群web界面" class="headerlink" title="YARN集群web界面"></a>YARN集群web界面</h5><p>地址：<a target="_blank" rel="noopener" href="http://resourcemanager_host:8088">http://resourcemanager_host:8088</a></p>
<p>其中resourcemanager_host是resourcemanager运行所在机器的主机名或者ip<br>如果使用主机名访问，别忘了在Windows配置hosts<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-28-12.png" alt="YARN集群web界面"></p>
<h4 id="Hadoop初体验"><a href="#Hadoop初体验" class="headerlink" title="Hadoop初体验"></a>Hadoop初体验</h4><h5 id="HDFS-初体验"><a href="#HDFS-初体验" class="headerlink" title="HDFS 初体验"></a>HDFS 初体验</h5><h6 id="shell命令操作"><a href="#shell命令操作" class="headerlink" title="shell命令操作"></a>shell命令操作</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoopfs-mkdir /itcast</span><br><span class="line">hadoopfs-put zookeeper.out/itcast</span><br><span class="line">hadoopfs-ls/</span><br></pre></td></tr></table></figure>
<h6 id="Web-UI页面操作"><a href="#Web-UI页面操作" class="headerlink" title="Web UI页面操作"></a>Web UI页面操作</h6><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-34-40.png" alt="Web UI页面操作"></p>
<h5 id="MapReduce-YARN初体验"><a href="#MapReduce-YARN初体验" class="headerlink" title="MapReduce+YARN初体验"></a>MapReduce+YARN初体验</h5><p>执行Hadoop官方自带的MapReduce案例，评估圆周率π的值。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $HADOOP_HOME/share/hadoop/mapreduce/</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-3.3.0.jar pi 2 4</span><br></pre></td></tr></table></figure>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-42-07.png" alt="MapReduce+YARN初体验"></p>
<h3 id="HDFS分布式文件系统基础"><a href="#HDFS分布式文件系统基础" class="headerlink" title="HDFS分布式文件系统基础"></a>HDFS分布式文件系统基础</h3><h4 id="文件系统、分布式文件系统"><a href="#文件系统、分布式文件系统" class="headerlink" title="文件系统、分布式文件系统"></a>文件系统、分布式文件系统</h4><h5 id="文件系统定义"><a href="#文件系统定义" class="headerlink" title="文件系统定义"></a>文件系统定义</h5><ul>
<li>文件系统是一种<strong>存储</strong>和<strong>组织数据</strong>的方法，实现了数据的存储、分级组织、访问和获取等操作，使得用户对文件访问和查找变得容易；</li>
<li>文件系统使用<strong>树形目录</strong>的<strong>抽象逻辑</strong>概念代替了硬盘等物理设备使用数据块的概念，用户不必关心数据底层存在硬盘哪里，只需要记住这个文件的所属目录和文件名即可；</li>
<li>文件系统通常使用硬盘和光盘这样的存储设备，并<strong>维护文件在设备中的物理位置</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-56-16.png" alt="文件系统"></li>
</ul>
<h5 id="传统常见的文件系统"><a href="#传统常见的文件系统" class="headerlink" title="传统常见的文件系统"></a>传统常见的文件系统</h5><ul>
<li>所谓传统常见的文件系统更多指的的<strong>单机的文件系统</strong>，也就是<strong>底层不会横跨多台机器</strong>实现。比如windows操作系统上的文件系统、Linux上的文件系统、FTP文件系统等等。</li>
<li>这些文件系统的共同特征包括：<ol>
<li>带有<strong>抽象的目录树结构</strong>，树都是从<strong>\/根目录开始</strong>往下蔓延；</li>
<li>树中节点分为两类：<strong>目录</strong>和<strong>文件</strong>；</li>
<li>从根目录开始，节点<strong>路径具有唯一性</strong>。</li>
</ol>
</li>
</ul>
<h5 id="数据、元数据"><a href="#数据、元数据" class="headerlink" title="数据、元数据"></a>数据、元数据</h5><h6 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h6><p>指存储的内容本身，比如文件、视频、图片等，这些<strong>数据底层最终是存储在磁盘</strong>等存储介质上的，一般<strong>用户无需关心</strong>，只需要基于目录树进行增删改查即可，实际针对数据的操作由文件系统完成。</p>
<h6 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h6><p>元数据（metadata）又称之为解释性数据，记录数据的数据；<br>文件系统元数据一般指<strong>文件大小、最后修改时间、底层存储位置、属性、所属用户、权限等信息</strong>。</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-00-22.png" alt="元数据"></p>
<h5 id="海量数据存储遇到的问题"><a href="#海量数据存储遇到的问题" class="headerlink" title="海量数据存储遇到的问题"></a>海量数据存储遇到的问题</h5><ul>
<li><p><strong>成本高</strong><br>传统存储硬件通用性差，设备投资加上后期维护、<strong>升级扩容的成本非常高</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-12-07.png" alt="成本高"></p>
</li>
<li><p>如何支撑高效率的计算分析<br>传统存储方式意味着数据：存储是存储，计算是计算，当<strong>需要处理数据的时候把数据移动过来</strong>。<br>程序和数据存储是属于不同的技术厂商实现，无法有机统一整合在一起。</p>
</li>
<li><p><strong>性能低</strong><br><strong>单节点I/O性能瓶</strong>颈无法逾越，难以支撑海量数据的<strong>高并发高吞吐</strong>场景。</p>
</li>
<li><p><strong>可扩展性差</strong><br>无法实现快速部署和弹性扩展，动态扩容、缩容成本高，技术实现难度大。</p>
</li>
</ul>
<h4 id="分布式存储系统的核心属性及功能含义"><a href="#分布式存储系统的核心属性及功能含义" class="headerlink" title="分布式存储系统的核心属性及功能含义"></a>分布式存储系统的核心属性及功能含义</h4><p>分布式存储系统核心属性</p>
<ul>
<li>分布式存储</li>
<li>元数据记录</li>
<li>分块存储</li>
<li>副本机制</li>
</ul>
<h5 id="分布式存储的优点"><a href="#分布式存储的优点" class="headerlink" title="分布式存储的优点"></a>分布式存储的优点</h5><ul>
<li>问题：数据量大，单机存储遇到瓶颈</li>
<li>解决：<br>单机纵向扩展：磁盘不够加磁盘，有上限瓶颈限制<br><strong>多机横向扩展</strong>：机器不够加机器，理论上<strong>无限扩展</strong></li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-16-54.png" alt="分布式存储"></p>
<h5 id="元数据记录的功能"><a href="#元数据记录的功能" class="headerlink" title="元数据记录的功能"></a>元数据记录的功能</h5><ul>
<li>问题：文件分布在不同机器上不利于寻找</li>
<li>解决：元数据记录下文件及其存储位置信息，<strong>快速定位文件位置</strong></li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-18-30.png" alt="元数据记录的功能"></p>
<h5 id="分块存储好处"><a href="#分块存储好处" class="headerlink" title="分块存储好处"></a>分块存储好处</h5><ul>
<li>问题：文件过大导致单机存不下、上传下载效率低</li>
<li>解决：文件分块存储在不同机器，<strong>针对块并行操作提高效率</strong></li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-20-30.png" alt="分块存储好处"></p>
<h5 id="副本机制的作用"><a href="#副本机制的作用" class="headerlink" title="副本机制的作用"></a>副本机制的作用</h5><ul>
<li>问题：硬件故障难以避免，数据易丢失</li>
<li>解决：不同机器设置备份，<strong>冗余存储，保障数据安全</strong></li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-22-03.png" alt="副本机制的作用"></p>
<div class="note info modern"><ol>
<li><div class="hide-block"><button type="button" class="hide-button" style>分布式存储的优点是什么？
 </button><div class="hide-content"><p><strong>无限扩展</strong>支撑海量数据存储</p>
</div></div>
</li>
<li><div class="hide-block"><button type="button" class="hide-button" style>元数据记录的功能是什么？
 </button><div class="hide-content"><p>快速<strong>定位文件</strong>位置便于查找</p>
</div></div>
</li>
<li><div class="hide-block"><button type="button" class="hide-button" style>文件分块存储好处是什么？
 </button><div class="hide-content"><p>针对块<strong>并行操作</strong>提高效率</p>
</div></div>
</li>
<li><div class="hide-block"><button type="button" class="hide-button" style>设置副本备份的作用是什么？
 </button><div class="hide-content"><p>冗余存储保障<strong>数据安全</strong></p>
</div></div>
</li>
</ol>
</div>
<h4 id="HDFS简介"><a href="#HDFS简介" class="headerlink" title="HDFS简介"></a>HDFS简介</h4><ul>
<li><p>HDFS（Hadoop Distributed File System ），意为：<strong>Hadoop分布式文件系统</strong>。</p>
</li>
<li><p>是Apache Hadoop核心组件之一，作为<strong>大数据生态圈最底层</strong>的分布式存储服务而存在。也可以说大数据首先要解决的问题就是海量数据的存储问题。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-22-38-08.png" alt="广义Hadoop"></p>
</li>
<li><p>HDFS主要是<strong>解决大数据如何存储问题的</strong>。分布式意味着是HDFS是横跨在多台计算机上的存储系统。</p>
</li>
<li><p>HDFS是一种能够在普通硬件上运行的分布式文件系统，它是<strong>高度容错</strong>的，适应于具有大数据集的应用程序，它非常适于存储大型数据(比如TB 和PB)。</p>
</li>
<li><p>HDFS使用多台计算机存储文件, 并且提供<strong>统一的访问接口</strong>, 像是访问一个普通文件系统一样使用分布式文件系统。</p>
</li>
<li><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-38-51.png" alt="HDFS简介"></p>
</li>
</ul>
<h4 id="HDFS起源发展、设计目标"><a href="#HDFS起源发展、设计目标" class="headerlink" title="HDFS起源发展、设计目标"></a>HDFS起源发展、设计目标</h4><h5 id="HDFS起源发展"><a href="#HDFS起源发展" class="headerlink" title="HDFS起源发展"></a>HDFS起源发展</h5><ul>
<li><p><strong>Doug Cutting</strong> 领导<strong>Nutch项目</strong>研发，Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能。</p>
</li>
<li><p>随着爬虫抓取网页数量的增加，遇到了严重的可扩展性问题——<strong>如何解决数十亿网页的存储和索引问题</strong>。</p>
</li>
<li><p>2003年的时候, Google发表的论文为该问题提供了可行的解决方案。<br>《<strong>分布式文件系统（GFS）</strong>，可用于处理海量网页的存储》<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-19-16-18.png" alt="GFS"></p>
</li>
<li><p>Nutch的开发人员完成了相应的开源实现HDFS，并从Nutch中剥离和MapReduce成为独立项目HADOOP。</p>
</li>
</ul>
<h5 id="HDFS设计目标"><a href="#HDFS设计目标" class="headerlink" title="HDFS设计目标"></a>HDFS设计目标</h5><ul>
<li>硬件故障（Hardware Failure）是常态，HDFS可能有成百上千的服务器组成，每一个组件都有可能出现故障。因此<strong>故障检测和自动快速恢复</strong>是HDFS的核心架构目标。</li>
<li>HDFS上的应用主要是以流式读取数据（Streaming Data Access）。HDFS被设计成用于批处理，而不是用户交互式的。相较于数据访问的反应时间，更<strong>注重数据访问的高吞吐量</strong>。</li>
<li>典型的HDFS文件大小是GB到TB的级别。所以，HDFS被调整成<strong>支持大文件（Large Data Sets）</strong>。它应该提供很高的聚合数据带宽，一个集群中支持数百个节点，一个集群中还应该支持千万级别的文件。</li>
<li>大部分HDFS应用对文件要求的是<strong>write-one-read-many</strong>访问模型。一个文件一旦<strong>创建、写入、关闭之后就不需要修改</strong>了。这一假设简化了数据一致性问题，使高吞吐量的数据访问成为可能。</li>
<li><strong>移动计算的代价比之移动数据的代价低</strong>。一个应用请求的计算，离它操作的数据越近就越高效。将计算移动到数据附近，比之将数据移动到应用所在显然更好。</li>
<li>HDFS被设计为可从一个平台<strong>轻松移植</strong>到另一个平台。这有助于将HDFS广泛用作大量应用程序的首选平台。</li>
</ul>
<h4 id="HDFS应用场景"><a href="#HDFS应用场景" class="headerlink" title="HDFS应用场景"></a>HDFS应用场景</h4><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-20-17-58.png" alt="HDFS应用场景"></p>
<h4 id="HDFS重要特性"><a href="#HDFS重要特性" class="headerlink" title="HDFS重要特性"></a>HDFS重要特性</h4><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-20-19-47.png" alt="HDFS"></p>
</blockquote>
<h5 id="整体概述"><a href="#整体概述" class="headerlink" title="整体概述"></a>整体概述</h5><ul>
<li>主从架构</li>
<li>分块存储</li>
<li>副本机制</li>
<li>元数据记录</li>
<li>抽象统一的目录树结构（namespace）</li>
</ul>
<h5 id="（1）主从架构"><a href="#（1）主从架构" class="headerlink" title="（1）主从架构"></a>（1）主从架构</h5><ul>
<li>HDFS集群是标准的master/slave主从架构集群。</li>
<li>一般一个HDFS集群是有一个Namenode和一定数目的Datanode组成。</li>
<li><strong>Namenode是HDFS主节点，Datanode是HDFS从节点，两种角色各司其职，共同协调</strong>完成分布式的文件存储服务。</li>
<li>官方架构图中是<strong>一主五从</strong>模式，其中五个从角色位于两个机架（Rack）的不同服务器上。</li>
</ul>
<h5 id="（2）分块存储"><a href="#（2）分块存储" class="headerlink" title="（2）分块存储"></a>（2）分块存储</h5><ul>
<li>HDFS中的文件在<strong>物理上是分块存储（block）</strong>的，默认大小是128M（134217728），不足128M则本身就是一块。</li>
<li>块的大小可以通过配置参数来规定，参数位于hdfs-default.xml中：dfs.blocksize。</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-20-30-29.png" alt="分块存储"></p>
<h5 id="（3）副本机制"><a href="#（3）副本机制" class="headerlink" title="（3）副本机制"></a>（3）副本机制</h5><ul>
<li>文件的所有block都会有副本。副本系数可以在文件创建的时候指定，也可以在之后通过命令改变。</li>
<li>副本数由参数dfs.replication控制，<strong>默认值是3</strong>，也就是会<strong>额外再复制2份</strong>，连同本身总共3份副本。</li>
</ul>
<h5 id="（4）元数据管理"><a href="#（4）元数据管理" class="headerlink" title="（4）元数据管理"></a>（4）元数据管理</h5><p>在HDFS中，Namenode管理的元数据具有两种类型：</p>
<ul>
<li><p><strong>文件自身属性信息</strong><br>文件名称、权限，修改时间，文件大小，复制因子，数据块大小。</p>
</li>
<li><p><strong>文件块位置映射信息</strong><br>记录文件块和DataNode之间的映射信息，即哪个块位于哪个节点上。</p>
</li>
</ul>
<h5 id="（5）namespace"><a href="#（5）namespace" class="headerlink" title="（5）namespace"></a>（5）namespace</h5><ul>
<li>HDFS支持传统的<strong>层次型文件组织结构</strong>。用户可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。</li>
<li>Namenode负责维护文件系统的namespace名称空间，任何对文件系统名称空间或属性的修改都将被Namenode记录下来。</li>
<li>HDFS会给客户端提供一个<strong>统一的抽象目录树</strong>，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。</li>
</ul>
<h5 id="（6）数据块存储"><a href="#（6）数据块存储" class="headerlink" title="（6）数据块存储"></a>（6）数据块存储</h5><ul>
<li>文件的各个block的<strong>具体存储管理由DataNode节点承担</strong>。</li>
<li>每一个block都可以在多个DataNode上存储。</li>
</ul>
<h3 id="HDFS-shell操作"><a href="#HDFS-shell操作" class="headerlink" title="HDFS shell操作"></a>HDFS shell操作</h3><h4 id="HDFS-shell命令行解释说明"><a href="#HDFS-shell命令行解释说明" class="headerlink" title="HDFS shell命令行解释说明"></a>HDFS shell命令行解释说明</h4><p><strong>命令行界面</strong>（英语：command-line interface，缩写：CLI），是指用户通过键盘输入指令，计算机接收到指令后，予以执行一种人际交互方式。</p>
<p>Hadoop提供了文件系统的shell命令行客户端: <code>hadoop fs [generic options]</code></p>
<h5 id="文件系统协议"><a href="#文件系统协议" class="headerlink" title="文件系统协议"></a>文件系统协议</h5><ul>
<li>HDFS Shell CLI支持操作多种文件系统，包括本地文件系统（file:///）、分布式文件系统（hdfs://nn:8020）等</li>
<li>具体操作的是什么文件系统取决于命令中文件路径<strong>URL中的前缀协议</strong>。</li>
<li>如果没有指定前缀，则将会读取环境变量中的<code>fs.defaultFS</code>属性，以该属性值作为默认文件系统。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls file:/// #操作本地文件系统</span><br><span class="line">hadoop fs -ls hdfs://node1:8020/ #操作HDFS分布式文件系统</span><br><span class="line">hadoop fs -ls / #直接根目录，没有指定协议将加载读取fs.defaultFS值</span><br></pre></td></tr></table></figure>
<h5 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h5><ul>
<li>hadoop dfs 只能操作HDFS文件系统（包括与Local FS间的操作），不过已经Deprecated；</li>
<li>hdfs dfs 只能操作HDFS文件系统相关（包括与Local FS间的操作）,常用；</li>
<li><code>hadoop fs</code> 可操作任意文件系统，不仅仅是hdfs文件系统，使用范围更广；</li>
</ul>
<p>目前版本来看，官方最终推荐使用的是hadoop fs。当然hdfs dfs在市面上的使用也比较多。</p>
<h5 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h5><ul>
<li>HDFS文件系统的操作命令很多和Linux类似，因此学习成本相对较低。</li>
<li>可以通过<code>hadoop fs -help</code>命令来查看每个命令的详细用法。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">[-appendToFile&lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">……</span><br><span class="line">-appendToFile&lt;localsrc&gt; ... &lt;dst&gt; :</span><br><span class="line">Appends the contents of all the given local files to the given dst file. The dst</span><br><span class="line">file will be created if it does not exist. If &lt;localSrc&gt; is -, then the input is</span><br><span class="line">read from stdin.</span><br><span class="line">-cat [-ignoreCrc] &lt;src&gt; ... :</span><br><span class="line">Fetch all files that match the file pattern &lt;src&gt; and display their content on</span><br><span class="line">stdout.</span><br></pre></td></tr></table></figure>
<h4 id="HDFS-shell命令行常用操作"><a href="#HDFS-shell命令行常用操作" class="headerlink" title="HDFS shell命令行常用操作"></a>HDFS shell命令行常用操作</h4><h5 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir [-p] &lt;path&gt; ...</span><br></pre></td></tr></table></figure>
<ul>
<li><code>path</code> 为待创建的目</li>
<li><code>-p</code>选项的行为与Unix mkdir -p非常相似，它<strong>会创建路径中的各级父目录</strong>。</li>
</ul>
<h5 id="查看指定目录下内容"><a href="#查看指定目录下内容" class="headerlink" title="查看指定目录下内容"></a>查看指定目录下内容</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls [-h] [-R] [&lt;path&gt; ...]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>path</code> 指定目录路径</li>
<li><code>-h</code> 人性化显示文件size</li>
<li><code>-R</code> 递归查看指定目录及其子目录</li>
</ul>
<h5 id="上传文件到HDFS指定目录下"><a href="#上传文件到HDFS指定目录下" class="headerlink" title="上传文件到HDFS指定目录下"></a>上传文件到HDFS指定目录下</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>-f</code> 覆盖目标文件（已存在下）</li>
<li><code>-p</code> 保留访问和修改时间，所有权和权限。</li>
<li><code>localsrc</code> 本地文件系统（客户端所在机器）</li>
<li><code>dst</code> 目标文件系统（HDFS）</li>
</ul>
<h5 id="查看HDFS文件内容"><a href="#查看HDFS文件内容" class="headerlink" title="查看HDFS文件内容"></a>查看HDFS文件内容</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat &lt;src&gt; ...</span><br></pre></td></tr></table></figure>
<p>读取指定文件全部内容，显示在标准输出控制台。<br>注意：对于<strong>大文件内容读取，慎重</strong>。</p>
<h5 id="下载HDFS文件"><a href="#下载HDFS文件" class="headerlink" title="下载HDFS文件"></a>下载HDFS文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get [-f] [-p] &lt;src&gt; ... &lt;localdst&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>下载文件到本地文件系统指定目录，localdst必须是目录</li>
<li><code>-f</code> 覆盖目标文件（已存在下）</li>
<li><code>-p</code> 保留访问和修改时间，所有权和权限。</li>
</ul>
<h5 id="拷贝HDFS文件"><a href="#拷贝HDFS文件" class="headerlink" title="拷贝HDFS文件"></a>拷贝HDFS文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp [-f] &lt;src&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>-f</code> 覆盖目标文件（已存在下）</li>
</ul>
<h5 id="追加数据到HDFS文件中"><a href="#追加数据到HDFS文件中" class="headerlink" title="追加数据到HDFS文件中"></a>追加数据到HDFS文件中</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -appendToFile&lt;localsrc&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<p>将所有给定本地文件的内容追加到给定dst文件。<br>dst如果文件不存在，将创建该文件。<br>注意：<strong>appendToFile 是将当地文件内容追加的到 hadoop 上的文件（不能hadoop上的文件1 追加给 hadoop上的文件2）</strong></p>
<h5 id="HDFS数据移动操作"><a href="#HDFS数据移动操作" class="headerlink" title="HDFS数据移动操作"></a>HDFS数据移动操作</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv &lt;src&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<p>移动文件到指定文件夹下<br>可以使用该命令移动数据，重命名文件的名称</p>
<h5 id="HDFS-shell其他命令"><a href="#HDFS-shell其他命令" class="headerlink" title="HDFS shell其他命令"></a>HDFS shell其他命令</h5><p>命令官方指导文档<br><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/">https://hadoop.apache.org/docs/</a><br><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/FileSystemShell.html">https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/FileSystemShell.html</a></p>
<p>常见的操作自己最好能够记住，其他操作可以根据需要查询文档使用。<br>命令属于<strong>多用多会，孰能生巧，不用就忘</strong>。</p>
<h3 id="HDFS工作流程与机制"><a href="#HDFS工作流程与机制" class="headerlink" title="HDFS工作流程与机制"></a>HDFS工作流程与机制</h3><h4 id="HDFS集群角色与职责"><a href="#HDFS集群角色与职责" class="headerlink" title="HDFS集群角色与职责"></a>HDFS集群角色与职责</h4><h5 id="官方架构图"><a href="#官方架构图" class="headerlink" title="官方架构图"></a>官方架构图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-20-19-47.png" alt="HDFS"></p>
<h5 id="主角色：namenode"><a href="#主角色：namenode" class="headerlink" title="主角色：namenode"></a>主角色：namenode</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-10-53.png" alt="namenode"></p>
<ul>
<li><code>NameNode</code>是Hadoop分布式文件系统的核心，架构中的主角色。</li>
<li><strong>NameNode维护和管理文件系统元数据</strong>，包括名称空间目录树结构、文件和块的位置信息、访问权限等信息。</li>
<li>基于此，<strong>NameNode成为了访问HDFS的唯一入口</strong>。</li>
<li>NameNode内部通过<strong>内存</strong>和<strong>磁盘文件</strong>两种方式管理元数据。</li>
<li>其中磁盘上的元数据文件包括Fsimage内存元数据镜像文件和edits log（Journal）编辑日志。</li>
</ul>
<h5 id="从角色：datanode"><a href="#从角色：datanode" class="headerlink" title="从角色：datanode"></a>从角色：datanode</h5><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-16-51.png" alt="datanode"></p>
</blockquote>
<ul>
<li><code>DataNode</code>是Hadoop HDFS中的从角色，负责<strong>具体的数据块存储</strong>。</li>
<li>DataNode的数量决定了HDFS集群的整体数据存储能力。通过和NameNode配合维护着数据块。</li>
</ul>
<h5 id="主角色辅助角色：secondarynamenode"><a href="#主角色辅助角色：secondarynamenode" class="headerlink" title="主角色辅助角色：secondarynamenode"></a>主角色辅助角色：secondarynamenode</h5><ul>
<li>Secondary NameNode充当NameNode的辅助节点，但不能替代NameNode。</li>
<li>主要是帮助主角色进行元数据文件的合并动作。可以通俗的理解为主角色的“秘书”。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-18-47.png" alt="secondarynamenode"></li>
</ul>
<h5 id="namenode职责"><a href="#namenode职责" class="headerlink" title="namenode职责"></a>namenode职责</h5><ul>
<li>NameNode仅<strong>存储HDFS的元数据</strong>：文件系统中所有文件的目录树，并跟踪整个集群中的文件，不存储实际数据。</li>
<li>NameNode知道HDFS中任何<strong>给定文件的块列表及其位置</strong>。使用此信息NameNode知道如何从块中构建文件。</li>
<li>NameNode<strong>不持久化存储每个文件中各个块所在的datanode的位置信息</strong>，这些信息会在系统启动时从DataNode重建。</li>
<li>NameNode是Hadoop集群中的<strong>单点故障</strong>。</li>
<li>NameNode所在机器通常会配置有<strong>大量内存（RAM）</strong>。</li>
</ul>
<h5 id="datanode职责"><a href="#datanode职责" class="headerlink" title="datanode职责"></a>datanode职责</h5><ul>
<li>DataNode负责<strong>最终数据块block的存储</strong>。是集群的<strong>从角色</strong>，也称为Slave。</li>
<li>DataNode启动时，会将自己<strong>注册</strong>到NameNode并<strong>汇报</strong>自己负责持有的块列表。</li>
<li>当某个DataNode关闭时，不会影响数据的可用性。NameNode将安排由其他DataNode管理的块进行副本复制。</li>
<li>DataNode所在机器通常配置有大量的<strong>硬盘</strong>空间，因为实际数据存储在DataNode中。</li>
</ul>
<h4 id="HDFS写数据流程（上传文件）"><a href="#HDFS写数据流程（上传文件）" class="headerlink" title="HDFS写数据流程（上传文件）"></a>HDFS写数据流程（上传文件）</h4><h5 id="写数据完整流程图"><a href="#写数据完整流程图" class="headerlink" title="写数据完整流程图"></a>写数据完整流程图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-25-38.png" alt="写数据完整流程图"></p>
<h5 id="核心概念—Pipeline管道"><a href="#核心概念—Pipeline管道" class="headerlink" title="核心概念—Pipeline管道"></a>核心概念—Pipeline管道</h5><ul>
<li><code>Pipeline</code>，中文翻译为管道。这是HDFS在上传文件写数据过程中采用的一种数据传输方式。</li>
<li>客户端将数据块写入第一个数据节点，第一个数据节点保存数据之后再将块复制到第二个数据节点，后者保存后将其复制到第三个数据节点。</li>
<li>为什么datanode之间采用pipeline线性传输，而不是一次给三个datanode拓扑式传输呢？</li>
<li>因为数据以管道的方式，<strong>顺序的沿着一个方向传输，这样能够充分利用每个机器的带宽，避免网络瓶颈和高延迟时的连接，最小化推送所有数据的延时</strong>。</li>
<li>在线性推送模式下，每台机器所有的出口宽带都用于以最快的速度传输数据，而不是在多个接受者之间分配宽带。</li>
</ul>
<h5 id="核心概念—ACK应答响应"><a href="#核心概念—ACK应答响应" class="headerlink" title="核心概念—ACK应答响应"></a>核心概念—ACK应答响应</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-31-42.png" alt="核心概念"></p>
<ul>
<li>ACK (Acknowledge character）即是确认字符，在数据通信中，接收方发给发送方的一种传输类控制字符。表示发来的数据已确认接收无误。</li>
<li>在HDFS pipeline管道传输数据的过程中，传输的反方向会进行ACK校验，确保数据传输安全。</li>
</ul>
<h5 id="核心概念—默认3副本存储策略"><a href="#核心概念—默认3副本存储策略" class="headerlink" title="核心概念—默认3副本存储策略"></a>核心概念—默认3副本存储策略</h5><ul>
<li>默认副本存储策略是由BlockPlacementPolicyDefault指定。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-34-22.png" alt="默认3副本存储策略"></li>
<li>第一块副本：优先客户端本地，否则随机</li>
<li>第二块副本：不同于第一块副本的不同机架。</li>
<li>第三块副本：第二块副本相同机架不同机器。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-35-49.png" alt="默认3副本存储策略"></li>
</ul>
<h5 id="写数据完整流程图文字描述"><a href="#写数据完整流程图文字描述" class="headerlink" title="写数据完整流程图文字描述"></a>写数据完整流程图文字描述</h5><ol>
<li>HDFS客户端创建对象实例<code>DistributedFileSystem</code>，该对象中封装了与HDFS文件系统操作的相关方法。</li>
<li>调用DistributedFileSystem对象的create()方法，通过<code>RPC</code>(远程过程调用)请求NameNode创建文件。<br>NameNode执行各种检查判断：目标文件是否存在、父目录是否存在、客户端是否具有创建该文件的权限。检查通过，NameNode就会为本次请求记下一条记录，返回<code>FSDataOutputStream输出流</code>对象给客户端用于写数据。</li>
<li>客户端通过FSDataOutputStream输出流开始写入数据。</li>
<li>客户端写入数据时，将数据分成一个个数据包（<strong>packet 默认64k</strong>）,内部组件<code>DataStreamer</code>请求NameNode挑选出适合存储数据副本的一组DataNode地址，默认是3副本存储。<br>DataStreamer将数据包流式传输到<code>pipeline</code>的第一个DataNode,该DataNode存储数据包并将它发送到pipeline的第二个DataNode。同样，第二个DataNode存储数据包并且发送给第三个（也是最后一个）DataNode。</li>
<li>传输的反方向上，会通过<code>ACK机制</code>校验数据包传输是否成功；</li>
<li>客户端完成数据写入后，在FSDataOutputStream输出流上调用close()方法关闭。</li>
<li>DistributedFileSystem联系NameNode告知其文件写入完成，等待NameNode确认。<br>因为namenode已经知道文件由哪些块组成（DataStream请求分配数据块），因此仅需等待最小复制块即可成功返回。<br>最小复制是由参数dfs.namenode.replication.min指定，默认是1.</li>
</ol>
<h4 id="HDFS读数据流程（下载文件）"><a href="#HDFS读数据流程（下载文件）" class="headerlink" title="HDFS读数据流程（下载文件）"></a>HDFS读数据流程（下载文件）</h4><h5 id="读数据完整流程图"><a href="#读数据完整流程图" class="headerlink" title="读数据完整流程图"></a>读数据完整流程图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-46-15.png" alt="读数据完整流程图"></p>
<ol>
<li>HDFS客户端创建对象实例<code>DistributedFileSystem</code>，调用该对象的open()方法来打开希望读取的文件。</li>
<li>DistributedFileSystem使用RPC调用namenode来确定<strong>文件中前几个块的块位置（分批次读取）信息</strong>。<br>对于每个块，namenode返回具有该块所有副本的datanode位置地址列表，并且该地址列表是排序好的，与客户端的网络拓扑距离近的排序靠前。</li>
<li>DistributedFileSystem将FSDataInputStream输入流返回到客户端以供其读取数据。</li>
<li>客户端在FSDataInputStream输入流上调用read()方法。然后，已存储DataNode地址的InputStream连接到文件中第一个块的最近的DataNode。数据从DataNode流回客户端，结果客户端可以在流上重复调用read（）。</li>
<li>当该块结束时，FSDataInputStream将关闭与DataNode的连接，然后寻找下一个block块的最佳datanode位置。这些操作对用户来说是透明的。所以用户感觉起来它一直在读取一个连续的流。<br>客户端从流中读取数据时，也会根据需要询问NameNode来<strong>检索下一批数据块的DataNode位置信息</strong>。</li>
<li>一旦客户端完成读取，就对FSDataInputStream调用close()方法。</li>
</ol>
<h2 id="Hadoop-MapReduce与Hadoop-YARN"><a href="#Hadoop-MapReduce与Hadoop-YARN" class="headerlink" title="Hadoop MapReduce与Hadoop YARN"></a>Hadoop MapReduce与Hadoop YARN</h2><h3 id="Hadoop-MapReduce"><a href="#Hadoop-MapReduce" class="headerlink" title="Hadoop MapReduce"></a>Hadoop MapReduce</h3><h4 id="MapReduce思想"><a href="#MapReduce思想" class="headerlink" title="MapReduce思想"></a>MapReduce思想</h4><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-15-37.png" alt="分而治之"></p>
</blockquote>
<ul>
<li>MapReduce的思想核心是“<strong>先分再合，分而治之</strong>”。</li>
<li>所谓“分而治之”就是<strong>把一个复杂的问题，按照一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，分别找出各部分的结果，然后把各部分的结果组成整个问题的最终结果</strong>。</li>
<li>这种思想来源于日常生活与工作时的经验。即使是发布过论文实现分布式计算的谷歌也只是实现了这种思想，而不是自己原创。</li>
<li>Map表示第一阶段，负责“<strong>拆分</strong>”：即把复杂的任务<strong>分解为若干个“简单的子任务”来并行处理</strong>。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎<strong>没有依赖关系</strong>。</li>
<li>Reduce表示第二阶段，负责“<strong>合并</strong>”：即对map阶段的结果进行全局汇总。</li>
<li>这两个阶段合起来正是MapReduce思想的体现。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-17-49.png" alt="apReduce思想"></li>
<li>一个比较形象的例子解释MapReduce<br>要数停车场中的所有停放车的总数量。<br>你数第一列，我数第二列…这就是Map阶段，人越多，能够同时数车的人就越多，速度就越快。<br>数完之后，聚到一起，把所有人的统计数加在一起。这就是Reduce合并汇总阶段。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-19-06.png" alt="MapReduce思想"></li>
</ul>
<h4 id="Hadoop-MapReduce设计构思"><a href="#Hadoop-MapReduce设计构思" class="headerlink" title="Hadoop MapReduce设计构思"></a>Hadoop MapReduce设计构思</h4><h5 id="（1）如何对付大数据处理场景"><a href="#（1）如何对付大数据处理场景" class="headerlink" title="（1）如何对付大数据处理场景"></a>（1）如何对付大数据处理场景</h5><ul>
<li>对相互间不具有计算依赖关系的大数据计算任务，实现并行最自然的办法就是<strong>采取MapReduce分而治之</strong>的策略。</li>
<li>首先Map阶段进行拆分，把大数据拆分成若干份小数据，多个程序同时并行计算产生中间结果；然后是Reduce聚合阶段，通过程序对并行的结果进行最终的汇总计算，得出最终的结果。</li>
<li><strong>不可拆分的计算任务或相互间有依赖关系的数据无法进行并行计算</strong>！</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-24-30.png" alt="MapReduce思想"></li>
</ul>
<h5 id="（2）构建抽象编程模型"><a href="#（2）构建抽象编程模型" class="headerlink" title="（2）构建抽象编程模型"></a>（2）构建抽象编程模型</h5><ul>
<li><p>MapReduce借鉴了<strong>函数式</strong>语言中的思想，用<strong>Map</strong>和<strong>Reduce</strong>两个函数提供了高层的并行编程抽象模型。<br>map: 对一组数据元素进行某种重复式的处理；<br>reduce: 对Map的中间结果进行某种进一步的结果整理。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-29-25.png" alt="构建抽象编程模型"></p>
</li>
<li><p>MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现:<br>map: (k1; v1) → (k2; v2)<br>reduce: (k2; [v2]) → (k3; v3)</p>
</li>
<li><p>通过以上两个编程接口，大家可以看出MapReduce处理的数据类型是<code>&lt;key,value&gt;键值对</code>。</p>
</li>
</ul>
<h5 id="（3）统一架构、隐藏底层细节"><a href="#（3）统一架构、隐藏底层细节" class="headerlink" title="（3）统一架构、隐藏底层细节"></a>（3）统一架构、隐藏底层细节</h5><ul>
<li>如何提供统一的计算框架，如果没有统一封装底层细节，那么程序员则需要考虑诸如数据存储、划分、分发、结果收集、错误恢复等诸多细节；为此，MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。</li>
<li>MapReduce最大的亮点在于通过抽象模型和计算框架把需要<strong>做什么(what need to do)</strong>与具体<strong>怎么做(how to do)</strong>分开了，为程序员提供一个抽象和高层的编程接口和框架。</li>
<li><strong>程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的业务程序代码</strong>。</li>
<li>至于如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理：从分布代码的执行，到大到数千小到单个节点集群的自动调度使用。</li>
</ul>
<h4 id="Hadoop-MapReduce介绍"><a href="#Hadoop-MapReduce介绍" class="headerlink" title="Hadoop MapReduce介绍"></a>Hadoop MapReduce介绍</h4><h5 id="分布式计算概念"><a href="#分布式计算概念" class="headerlink" title="分布式计算概念"></a>分布式计算概念</h5><ul>
<li><strong>分布式计算</strong>是一种计算方法，和<strong>集中式计算</strong>是相对的。</li>
<li>随着计算技术的发展，有些应用需要非常巨大的计算能力才能完成，如果采用集中式计算，需要耗费相当长的时间来完成。</li>
<li>分布式计算<strong>将该应用分解成许多小的部分，分配给多台计算机进行处理</strong>。这样可以节约整体计算时间，大大提高计算效率。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-37-36.png" alt="分布式计算概念"></li>
</ul>
<h5 id="Hadoop-MapReduce概述"><a href="#Hadoop-MapReduce概述" class="headerlink" title="Hadoop MapReduce概述"></a>Hadoop MapReduce概述</h5><ul>
<li>Hadoop MapReduce是一个<strong>分布式计算框架</strong>，用于轻松编写分布式应用程序，这些应用程序以可靠，容错的方式并行处理大型硬件集群（数千个节点）上的大量数据（多TB数据集）。</li>
<li>MapReduce是一种面向海量数据处理的一种指导思想，也是一种用于对大规模数据进行分布式计算的编程模型。</li>
</ul>
<h5 id="MapReduce产生背景"><a href="#MapReduce产生背景" class="headerlink" title="MapReduce产生背景"></a>MapReduce产生背景</h5><ul>
<li>MapReduce最早由<strong>Google</strong>于2004年在一篇名为《MapReduce:SimplifiedData Processingon Large Clusters》的<strong>论文</strong>中提出。</li>
<li>论文中谷歌把分布式数据处理的过程拆分为Map和Reduce两个操作函数（受到函数式编程语言的启发），随后被Apache Hadoop参考并作为开源版本提供支持，叫做Hadoop MapReduce。</li>
<li>它的出现解决了人们在最初面临海量数据束手无策的问题，同时它还是<strong>易于使用和高度可扩展的</strong>，使得开发者无需关系分布式系统底层的复杂性即可很容易的编写分布式数据处理程序，并在成千上万台普通的商用服务器中运行。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-51-39.png" alt="MapReduce产生背景"></li>
</ul>
<h5 id="MapReduce特点"><a href="#MapReduce特点" class="headerlink" title="MapReduce特点"></a>MapReduce特点</h5><ul>
<li><p><strong>易于编程</strong><br>Mapreduce框架提供了用于二次开发的接口；简单地实现一些接口，就可以完成一个分布式程序。任务计算交给计算框架去处理，将分布式程序部署到hadoop集群上运行，集群节点可以扩展到成百上千个等。</p>
</li>
<li><p><strong>良好的扩展性</strong><br>当计算机资源不能得到满足的时候，可以通过增加机器来扩展它的计算能力。基于MapReduce的分布式计算得特点可以随节点数目增长保持近似于线性的增长，这个特点是MapReduce处理海量数据的关键，通过将计算节点增至几百或者几千可以很容易地处理数百TB甚至PB级别的离线数据。</p>
</li>
<li><p><strong>高容错性</strong><br>Hadoop集群是分布式搭建和部署得，任何单一机器节点宕机了，它可以把上面的计算任务转移到另一个节点上运行，不影响整个作业任务得完成，过程完全是由Hadoop内部完成的。</p>
</li>
<li><p><strong>适合海量数据的离线处理</strong><br>可以处理GB、TB和PB级别得数据量</p>
</li>
</ul>
<h5 id="MapReduce局限性"><a href="#MapReduce局限性" class="headerlink" title="MapReduce局限性"></a>MapReduce局限性</h5><p>MapReduce虽然有很多的优势，也有相对得局限性，局限性不代表不能做，而是在有些场景下实现的效果比较差，并不适合用MapReduce来处理，主要表现在以下结果方面：</p>
<ul>
<li><p><strong>实时计算性能差</strong><br>MapReduce主要应用于离线作业，无法作到秒级或者是亚秒级得数据响应。</p>
</li>
<li><p><strong>不能进行流式计算</strong><br>流式计算特点是数据是源源不断得计算，并且数据是动态的；而MapReduce作为一个离线计算框架，主要是针对静态数据集得，数据是不能动态变化得。</p>
</li>
</ul>
<h5 id="MapReduce实例进程"><a href="#MapReduce实例进程" class="headerlink" title="MapReduce实例进程"></a>MapReduce实例进程</h5><p>一个完整的MapReduce程序在分布式运行时有<strong>三类</strong></p>
<ul>
<li><code>MRAppMaster</code>：负责整个MR程序的过程调度及状态协调</li>
<li><code>MapTask</code>：负责map阶段的整个数据处理流程</li>
<li><code>ReduceTask</code>：负责reduce阶段的整个数据处理流程</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-56-12.png" alt="MapReduce实例进程"></li>
</ul>
<h5 id="MapReduce阶段组成"><a href="#MapReduce阶段组成" class="headerlink" title="MapReduce阶段组成"></a>MapReduce阶段组成</h5><ul>
<li>一个MapReduce编程模型中<strong>只能包含一个Map阶段和一个Reduce阶段，或者只有Map阶段</strong>；</li>
<li>不能有诸如多个map阶段、多个reduce阶段的情景出现；</li>
<li>如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序串行运行。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-57-30.png" alt="阶段组成"></li>
</ul>
<h5 id="MapReduce数据类型"><a href="#MapReduce数据类型" class="headerlink" title="MapReduce数据类型"></a>MapReduce数据类型</h5><ul>
<li>注意：整个MapReduce程序中，数据都是以<strong>kv键值对的形式流转</strong>的；</li>
<li>在实际编程解决各种业务问题中，需要考虑每个阶段的输入输出kv分别是什么；</li>
<li>MapReduce内置了很多默认属性，比如排序、分组等，都和数据的k有关，所以说kv的类型数据确定及其重要的</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-58-42.png" alt="MapReduce数据类型"></li>
</ul>
<h4 id="Hadoop-MapReduce官方示例"><a href="#Hadoop-MapReduce官方示例" class="headerlink" title="Hadoop MapReduce官方示例"></a>Hadoop MapReduce官方示例</h4><ul>
<li>一个最终完整版本的MR程序需要<strong>用户编写的代码</strong>和<strong>Hadoop自己实现的代码</strong>整合在一起才可以；</li>
<li>其中用户负责map、reduce两个阶段的业务问题，Hadoop负责底层所有的技术问题；</li>
<li>由于MapReduce计算引擎天生的弊端（慢），当下企业中直接使用率已经日薄西山了，所以在<strong>企业中工作很少涉及到MapReduce直接编程</strong>，但是某些软件的背后还依赖MapReduce引擎。</li>
<li>可以通过官方提供的示例来<strong>感受MapReduce及其内部执行流程</strong>，因为后续的新的计算引擎比如Spark，当中就有MapReduce深深的影子存在。</li>
</ul>
<h5 id="MapReduce示例说明"><a href="#MapReduce示例说明" class="headerlink" title="MapReduce示例说明"></a>MapReduce示例说明</h5><ul>
<li>示例程序路径：<code>$HADOOP_HOME/share/hadoop/mapreduce/</code></li>
<li>示例程序：hadoop-mapreduce-examples-3.3.0.jar</li>
<li>MapReduce程序提交命令：<code>[hadoop jar|yarn jar] hadoop-mapreduce-examples-3.3.0.jar args…</code></li>
<li>提交到哪里去？<strong>提交到YARN集群上分布式执行</strong>。</li>
</ul>
<h5 id="评估圆周率π（PI）的值"><a href="#评估圆周率π（PI）的值" class="headerlink" title="评估圆周率π（PI）的值"></a>评估圆周率π（PI）的值</h5><p>Hadoop MapReduce示例提供了Monte Carlo方法计算圆周率。</p>
<h6 id="Monte-Carlo方法"><a href="#Monte-Carlo方法" class="headerlink" title="Monte Carlo方法"></a>Monte Carlo方法</h6><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-22-42-30.png" alt="评估圆周率π（PI）的值"></p>
</blockquote>
<p>假设正方形边长为1，圆半径也为1，那么1/4圆的面积为：$\frac{1}{4} \pi r^2$</p>
<p>在正方形内随机撒点，分布于1/4圆内的数量假设为a ，分布于圆外的数量为b，N则是所产生的总数：$N=a+b$</p>
<p>那么数量a与N的比值应与1/4圆面积及正方形面积成正比，于是：$\frac{\pi}{4}:1=a:N$</p>
<script type="math/tex; mode=display">
\pi = \frac{4a}{N}</script><h6 id="评估圆周率π参数设置"><a href="#评估圆周率π参数设置" class="headerlink" title="评估圆周率π参数设置"></a>评估圆周率π参数设置</h6><p>运行MapReduce程序评估一下圆周率的值，执行中可以去YARN页面上观察程序的执行的情况。</p>
<ul>
<li>第一个参数：pi表示MapReduce程序执行圆周率计算任务；</li>
<li>第二个参数：用于指定map阶段运行的任务task次数，并发度，这里是10；</li>
<li>第三个参数：用于指定每个map任务取样的个数，这里是50。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-3.3.0.jar pi 10 50</span><br></pre></td></tr></table></figure>
<h5 id="wordcount单词词频统计"><a href="#wordcount单词词频统计" class="headerlink" title="wordcount单词词频统计"></a>wordcount单词词频统计</h5><details class="toggle"><summary class="toggle-button" style>1.txt</summary><div class="toggle-content"><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello tom hello allen hello</span><br><span class="line">allen tom mac apple</span><br><span class="line">hello allen apple</span><br><span class="line">hello apple spark allen hadoop spark</span><br></pre></td></tr></table></figure>
</div></details>
<p>WordCount中文叫做单词统计、词频统计；<br>指的是统计指定文件中，每个<strong>单词出现的总次数</strong>。</p>
<h6 id="WordCount概述"><a href="#WordCount概述" class="headerlink" title="WordCount概述"></a>WordCount概述</h6><p>WordCount算是大数据计算领域经典的入门案例，相当于Hello World。</p>
<p>虽然WordCount业务极其简单，但关键是能够通过案例<strong>感受背后MapReduce的执行流程和默认的行为机制</strong>。</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-22-49-46.png" alt="WordCount"></p>
<h6 id="WordCount编程实现思路"><a href="#WordCount编程实现思路" class="headerlink" title="WordCount编程实现思路"></a>WordCount编程实现思路</h6><ul>
<li>map阶段的核心：把输入的<strong>数据经过切割，全部标记1</strong>，因此输出就是&lt;单词，1&gt;。</li>
<li><strong>shuffle阶段核心：经过MR程序内部自带默认的排序分组等功能，把key相同的单词会作为一组数据构成新的kv对</strong>。</li>
<li>reduce阶段核心：处理shuffle完的一组数据，该组数据就是该单词所有的键值对。对所有的1进行累加求和，就是单词的总次数。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-22-51-54.png" alt="WordCount"></li>
</ul>
<h6 id="WordCount程序提交"><a href="#WordCount程序提交" class="headerlink" title="WordCount程序提交"></a>WordCount程序提交</h6><ul>
<li><p>上传文本文件1.txt(写入一些单词)到HDFS文件系统的/input目录下，如果没有这个目录，使用shell创建<br><code>hadoop fs -mkdir /input</code><br><code>hadoop fs -put 1.txt /input</code></p>
</li>
<li><p>执行官方MapReduce实例，对上述文件进行单词次数统计<br>第一个参数：wordcount表示执行单词统计任务；<br>第二个参数：指定输入文件的路径；<br>第三个参数：指定输出结果的路径（该路径不能已存在）；</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-3.3.0.jar wordcount /input /output</span><br></pre></td></tr></table></figure>
<h4 id="Map阶段执行流程"><a href="#Map阶段执行流程" class="headerlink" title="Map阶段执行流程"></a>Map阶段执行流程</h4><h5 id="WordCount执行流程图"><a href="#WordCount执行流程图" class="headerlink" title="WordCount执行流程图"></a>WordCount执行流程图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-22-51-54.png" alt="WordCount"></p>
<h5 id="MapReduce整体执行流程图"><a href="#MapReduce整体执行流程图" class="headerlink" title="MapReduce整体执行流程图"></a>MapReduce整体执行流程图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-04-23.png" alt="MapReduce整体执行流程图"></p>
<h5 id="Map阶段执行过程"><a href="#Map阶段执行过程" class="headerlink" title="Map阶段执行过程"></a>Map阶段执行过程</h5><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-05-01.png" alt="Map阶段执行过程"></p>
</blockquote>
<ul>
<li><p>第一阶段：把输入目录下文件按照一定的标准逐个进行<strong>逻辑切片</strong>，形成切片规划。<br>默认Split size = Block size（128M），每一个切片由一个MapTask处理。（getSplits）</p>
</li>
<li><p>第二阶段：对切片中的数据按照一定的规则读取解析返回<key,value>对。<br>默认是<strong>按行读取数据</strong>。key是每一行的起始位置偏移量，value是本行的文本内容。（TextInputFormat）</key,value></p>
</li>
<li><p>第三阶段：调用Mapper类中的<strong>map方法处理数据</strong>。<br>每读取解析出来的一个<key,value> ，调用一次map方法。</key,value></p>
</li>
<li><p>第四阶段：按照一定的规则对Map输出的键值对进行<strong>分区partition</strong>。默认不分区，因为只有一个reducetask。<br>分区的数量就是reducetask运行的数量。</p>
</li>
<li><p>第五阶段：Map输出数据写入<strong>内存缓冲区</strong>，达到比例溢出到磁盘上。<strong>溢出spill</strong>的时候根据key进行<strong>排序sort</strong>。<br>默认根据key字典序排序。</p>
</li>
<li><p>第六阶段：对所有溢出文件进行最终的<strong>merge合并</strong>，成为一个文件。</p>
</li>
</ul>
<h4 id="Reduce阶段执行流程"><a href="#Reduce阶段执行流程" class="headerlink" title="Reduce阶段执行流程"></a>Reduce阶段执行流程</h4><ul>
<li>第一阶段：ReduceTask会主动从MapTask<strong>复制拉取</strong>属于需要自己处理的数据。</li>
<li>第二阶段：把拉取来数据，全部进行<strong>合并merge</strong>，即把分散的数据合并成一个大的数据。再对合并后的数据<strong>排序</strong>。</li>
<li>第三阶段是对排序后的键值对<strong>调用reduce方法</strong>。<strong>键相等</strong>的键值对调用一次reduce方法。最后把这些输出的键值对写入到HDFS文件中。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-16-08.png" alt="Reduce阶段执行流程"></li>
</ul>
<h4 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h4><h5 id="shuffle概念"><a href="#shuffle概念" class="headerlink" title="shuffle概念"></a>shuffle概念</h5><ul>
<li><code>Shuffle</code>的本意是洗牌、混洗的意思，把一组有规则的数据尽量打乱成无规则的数据。</li>
<li>而在MapReduce中，Shuffle更像是洗牌的<strong>逆</strong>过程，指的是<strong>将map端的无规则输出按指定的规则“打乱”成具有一定规则的数据，以便reduce端接收处理</strong>。</li>
<li><strong>一般把从Map产生输出开始到Reduce取得数据作为输入之前的过程称作shuffle</strong>。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-56-07.png" alt="shuffle概念"></li>
</ul>
<h5 id="Map端Shuffle"><a href="#Map端Shuffle" class="headerlink" title="Map端Shuffle"></a>Map端Shuffle</h5><ul>
<li>Collect阶段：将MapTask的结果收集输出到默认大小为100M的环形缓冲区，保存之前会对key进行分区的计算，默认Hash分区。</li>
<li>Spill阶段：当内存中的数据量达到一定的阀值的时候，就会将数据写入本地磁盘，在将数据写入磁盘之前需要对数据进行一次排序的操作，如果配置了combiner，还会将有相同分区号和key的数据进行排序。</li>
<li>Merge阶段：把所有溢出的临时文件进行一次合并操作，以确保一个MapTask最终只产生一个中间数据文件。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-57-34.png" alt="Map端Shuffle"></li>
</ul>
<h5 id="Reducer端shuffle"><a href="#Reducer端shuffle" class="headerlink" title="Reducer端shuffle"></a>Reducer端shuffle</h5><ul>
<li>Copy阶段：ReduceTask启动Fetcher线程到已经完成MapTask的节点上复制一份属于自己的数据。</li>
<li>Merge阶段：在ReduceTask远程复制数据的同时，会在后台开启两个线程对内存到本地的数据文件进行合并操作。</li>
<li>Sort阶段：在对数据进行合并的同时，会进行排序操作，由于MapTask阶段已经对数据进行了局部的排序，ReduceTask只需保证Copy的数据的最终整体有效性即可。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-58-30.png" alt="Reducer端shuffle"></li>
</ul>
<h5 id="shuffle机制弊端"><a href="#shuffle机制弊端" class="headerlink" title="shuffle机制弊端"></a>shuffle机制弊端</h5><ul>
<li>Shuffle是MapReduce程序的核心与精髓，是MapReduce的灵魂所在。</li>
<li>Shuffle也是MapReduce被诟病最多的地方所在。MapReduce相比较于Spark、Flink计算引擎慢的原因，跟Shuffle机制有很大的关系。</li>
<li>Shuffle中<strong>频繁涉及到数据在内存、磁盘之间的多次往复</strong>。</li>
</ul>
<h3 id="Hadoop-YARN"><a href="#Hadoop-YARN" class="headerlink" title="Hadoop YARN"></a>Hadoop YARN</h3><h4 id="Hadoop-YARN介绍"><a href="#Hadoop-YARN介绍" class="headerlink" title="Hadoop YARN介绍"></a>Hadoop YARN介绍</h4><h5 id="YARN简介"><a href="#YARN简介" class="headerlink" title="YARN简介"></a>YARN简介</h5><ul>
<li>Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的Hadoop资源管理器。</li>
<li>YARN是一个<code>通用</code><strong>资源管理系统</strong>和<strong>调度平台</strong>，可为上层应用提供统一的资源管理和调度。</li>
<li>它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-02-20.png" alt="YARN简介"></li>
</ul>
<h5 id="YARN功能说明"><a href="#YARN功能说明" class="headerlink" title="YARN功能说明"></a>YARN功能说明</h5><ul>
<li><strong>资源管理系统</strong>：集群的硬件资源，和程序运行相关，比如内存、CPU等。</li>
<li><strong>调度平台</strong>：多个程序同时申请计算资源如何分配，调度的规则（算法）。</li>
<li><strong>通用</strong>：不仅仅支持MapReduce程序，理论上<strong>支持各种计算程序</strong>。YARN不关心你干什么，只关心你要资源，在有的情况下给你，用完之后还我。</li>
</ul>
<h5 id="YARN概述"><a href="#YARN概述" class="headerlink" title="YARN概述"></a>YARN概述</h5><ul>
<li>可以把Hadoop YARN理解为相当于一个分布式的操作系统平台，而MapReduce等计算程序则相当于运行于操作系统之上的应用程序，<strong>YARN为这些程序提供运算所需的资源</strong>（内存、CPU等）。</li>
<li>Hadoop能有今天这个地位，YARN可以说是功不可没。因为有了YARN ，更多计算框架可以接入到HDFS中，而不单单是MapReduce，<strong>正是因为YARN的包容，使得其他计算框架能专注于计算性能的提升</strong>。</li>
<li>HDFS可能不是最优秀的大数据存储系统，但却是应用最广泛的大数据存储系统，YARN功不可没。</li>
</ul>
<h4 id="Hadoop-YARN架构、组件"><a href="#Hadoop-YARN架构、组件" class="headerlink" title="Hadoop YARN架构、组件"></a>Hadoop YARN架构、组件</h4><h5 id="YARN官方架构图"><a href="#YARN官方架构图" class="headerlink" title="YARN官方架构图"></a>YARN官方架构图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-05-28.png" alt="YARN官方架构图"></p>
<h5 id="官方架构图中出现的概念"><a href="#官方架构图中出现的概念" class="headerlink" title="官方架构图中出现的概念"></a>官方架构图中出现的概念</h5><ul>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-06-43.png" alt="官方架构图中出现的概念"></li>
<li>Client</li>
<li>Container容器（资源的抽象）</li>
</ul>
<h5 id="YARN3大组件"><a href="#YARN3大组件" class="headerlink" title="YARN3大组件"></a>YARN3大组件</h5><ul>
<li><p><strong>ResourceManager（RM）</strong><br>YARN集群中的主角色，决定系统中所有应用程序之间<strong>资源分配的最终权限，即最终仲裁者</strong>。<br>接收用户的作业提交，并通过NM分配、管理各个机器上的计算资源。</p>
</li>
<li><p><strong>NodeManager（NM）</strong><br>YARN中的从角色，一台机器上一个，负责<strong>管理本机器上的计算资源</strong>。<br>根据RM命令，启动Container容器、监视容器的资源使用情况。并且向RM主角色汇报资源使用情况。</p>
</li>
<li><p><strong>ApplicationMaster（AM）</strong><br>用户提交的每个应用程序均包含一个AM。<br><strong>应用程序内的“老大”</strong>，负责程序内部各阶段的资源申请，监督程序的执行情况。</p>
</li>
</ul>
<h4 id="程序提交YARN交互流程"><a href="#程序提交YARN交互流程" class="headerlink" title="程序提交YARN交互流程"></a>程序提交YARN交互流程</h4><h5 id="核心交互流程"><a href="#核心交互流程" class="headerlink" title="核心交互流程"></a>核心交互流程</h5><ul>
<li>MR作业提交Client—&gt;RM</li>
<li>资源的申请MrAppMaster—&gt;RM</li>
<li>MR作业状态汇报Container（Map|Reduce Task）—&gt;Container（MrAppMaster）</li>
<li>节点的状态汇报NM—&gt;RM</li>
</ul>
<h5 id="交互流程概述"><a href="#交互流程概述" class="headerlink" title="交互流程概述"></a>交互流程概述</h5><p>当用户向YARN 中提交一个应用程序后，YARN将分两个阶段运行该应用程序。</p>
<ul>
<li>第一个阶段是<strong>客户端申请资源启动运行本次程序的ApplicationMaster</strong>；</li>
<li>第二个阶段是由<strong>ApplicationMaster根据本次程序内部具体情况，为它申请资源，并监控它的整个运行过程</strong>，直到运行完成。</li>
</ul>
<h5 id="MR提交YARN交互流程"><a href="#MR提交YARN交互流程" class="headerlink" title="MR提交YARN交互流程"></a>MR提交YARN交互流程</h5><ul>
<li>第1步、用户通过客户端向YARN中ResourceManager提交应用程序（比如hadoop jar提交MR程序）。</li>
<li>第2步、ResourceManager为该应用程序分配第一个Container（容器），并与对应的NodeManager通信，要求它在这个Container中启动这个应用程序的ApplicationMaster。</li>
<li>第3步、ApplicationMaster启动成功之后，首先向ResourceManager注册并保持通信，这样用户可以直接通过ResourceManage查看应用程序的运行状态（处理了百分之几）。</li>
<li>第4步、AM为本次程序内部的各个Task任务向RM申请资源，并监控它的运行状态。</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-15-11.png" alt="YARN核心交互流程"></p>
<ul>
<li>第5步、一旦ApplicationMaster 申请到资源后，便与对应的NodeManager 通信，要求它启动任务。</li>
<li>第6步、NodeManager 为任务设置好运行环境后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。</li>
<li>第7步、各个任务通过某个RPC 协议向ApplicationMaster 汇报自己的状态和进度，以让ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC 向ApplicationMaster 查询应用程序的当前运行状态。</li>
<li>第8步、应用程序运行完成后，ApplicationMaster 向ResourceManager 注销并关闭自己。</li>
</ul>
<h4 id="YARN资源调度器Scheduler"><a href="#YARN资源调度器Scheduler" class="headerlink" title="YARN资源调度器Scheduler"></a>YARN资源调度器Scheduler</h4><h5 id="MR程序提交YARN交互流程"><a href="#MR程序提交YARN交互流程" class="headerlink" title="MR程序提交YARN交互流程"></a>MR程序提交YARN交互流程</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-24-33.png" alt="YARN资源调度器Scheduler"></p>
<h5 id="如何理解资源调度"><a href="#如何理解资源调度" class="headerlink" title="如何理解资源调度"></a>如何理解资源调度</h5><ul>
<li>在理想情况下，应用程序提出的请求将立即得到YARN批准。但是实际中，<strong>资源是有限的</strong>，并且在<strong>繁忙的群集上</strong>，应用程序通常将需要等待其某些请求得到满足。YARN调度程序的工作是<strong>根据一些定义的策略为应用程序分配资源</strong>。</li>
<li>在YARN中，负责给应用分配资源的就是<code>Scheduler</code>，它是ResourceManager的核心组件之一。Scheduler完全专用于调度作业，它无法跟踪应用程序的状态。</li>
<li>一般而言，调度是一个难题，并且没有一个“最佳”策略，为此，YARN提供了多种调度器和可配置的策略供选择。</li>
</ul>
<h5 id="调度器策略"><a href="#调度器策略" class="headerlink" title="调度器策略"></a>调度器策略</h5><ul>
<li><p>三种调度器<br><code>FIFO Scheduler</code>（先进先出调度器）、<code>Capacity Scheduler</code>（容量调度器）、<code>Fair Scheduler</code>（公平调度器）。</p>
</li>
<li><p>Apache版本YARN默认使用<code>Capacity Scheduler</code>。</p>
</li>
<li>如果需要使用其他的调度器，可以在yarn-site.xml中的yarn.resourcemanager.scheduler.class进行配置。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-27-16.png" alt="三种调度器"></li>
</ul>
<div class="tabs" id="diaoduqi"><ul class="nav-tabs"><li class="tab"><button type="button" data-href="#diaoduqi-1"><b>FIFO Scheduler</b></button></li><li class="tab"><button type="button" data-href="#diaoduqi-2"><b>Capacity Scheduler</b></button></li><li class="tab"><button type="button" data-href="#diaoduqi-3"><b>Fair Scheduler</b></button></li></ul><div class="tab-contents"><div class="tab-item-content" id="diaoduqi-1"><p><mark class="hl-label green">FIFO&nbsp;Scheduler概述</mark> </p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-28-28.png" alt="FIFO Schedule"></p>
<ul>
<li><code>FIFO Scheduler</code>是Hadoop1.x中JobTracker原有的调度器实现，此调度器在YARN中保留了下来。</li>
<li>FIFO Scheduler是一个<strong>先进先出</strong>的思想，即<strong>先提交的应用先运行</strong>。调度工作不考虑优先级和范围，适用于负载较低的小规模集群。当使用大型共享集群时，它的效率较低且会导致一些问题。</li>
<li>FIFO Scheduler拥有一个控制全局的队列queue，默认queue名称为default，该调度器会获取当前集群上所有的资源信息作用于这个全局的queue。</li>
</ul>
<p><mark class="hl-label green">FIFO&nbsp;Scheduler优势、坏处</mark> </p>
<ul>
<li><p>优势：<br>无需配置、先到先得、易于执行</p>
</li>
<li><p>坏处：<br>任务的优先级不会变高，因此高优先级的作业需要等待<br>不适合共享集群</p>
</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="diaoduqi-2"><p><mark class="hl-label green">Capacity&nbsp;Scheduler概述</mark> </p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-34-07.png" alt="Capacity Scheduler"></p>
<ul>
<li>Capacity Scheduler容量调度是<strong>Apache Hadoop3.x默认调度策略</strong>。该策略允许<strong>多个组织共享整个集群资源</strong>，每个组织可以获得集群的一部分计算能力。<strong>通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源</strong>，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。</li>
<li>Capacity可以理解成一个个的资源队列，这个资源队列是用户自己去分配的。队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略。</li>
</ul>
<p><mark class="hl-label green">Capacity&nbsp;Scheduler资源队列划分</mark> </p>
<p>Capacity Scheduler调度器以队列为单位划分资源。简单通俗点来说，就是一个个队列有独立的资源，队列的结构和资源是可以进行配置的。</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-35-41.png" alt="Capacity Scheduler资源队列划分"></p>
<p><mark class="hl-label green">Capacity&nbsp;Scheduler特性优势</mark> </p>
<ul>
<li><p><strong>层次化的队列设计</strong>（Hierarchical Queues）<br>层次化的管理，可以更容易、更合理分配和限制资源的使用。</p>
</li>
<li><p><strong>容量保证</strong>（Capacity Guarantees）<br>每个队列上都可以设置一个资源的占比，保证每个队列都不会占用整个集群的资源。</p>
</li>
<li><p><strong>安全</strong>（Security）<br>每个队列有严格的访问控制。用户只能向自己的队列里面提交任务，而且不能修改或者访问其他队列的任务。</p>
</li>
<li><p><strong>弹性分配</strong>（Elasticity）<br>空闲的资源可以被分配给任何队列。<br>当多个队列出现争用的时候，则会按照权重比例进行平衡。</p>
</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="diaoduqi-3"><p><mark class="hl-label green">Fair&nbsp;Scheduler概述</mark> </p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-38-13.png" alt="Fair Scheduler概述"></p>
<ul>
<li><strong>Fair Scheduler叫做公平调度</strong>，提供了YARN应用程序<strong>公平地共享大型集群中资源</strong>的另一种方式。使所有应用在平均情况下随着时间的流逝可以获得相等的资源份额。</li>
<li>Fair Scheduler设计目标是为所有的应用分配公平的资源（对公平的定义通过参数来设置）。</li>
<li>公平调度可以在多个队列间工作，允许资源共享和抢占。</li>
</ul>
<p><mark class="hl-label green">如何理解公平共享</mark> </p>
<ul>
<li>有两个用户A和B，每个用户都有自己的队列。</li>
<li>A启动一个作业，由于没有B的需求，它分配了集群所有可用的资源。</li>
<li>然后B在A的作业仍在运行时启动了一个作业，经过一段时间，A,B各自作业都使用了一半的资源。</li>
<li>现在，如果B用户在其他作业仍在运行时开始第二个作业，它将与B的另一个作业共享其资源，因此B的每个作业将拥有资源的四分之一，而A的继续将拥有一半的资源。结果是资源在用户之间公平地共享。</li>
</ul>
<p><mark class="hl-label green">Fair&nbsp;特性优势</mark> </p>
<ul>
<li><strong>分层队列</strong>：队列可以按层次结构排列以划分资源，并可以配置权重以按特定比例共享集群。</li>
<li><strong>基于用户或组的队列映射</strong>：可以根据提交任务的用户名或组来分配队列。如果任务指定了一个队列,则在该队列中提交任务。</li>
<li><strong>资源抢占</strong>：根据应用的配置，抢占和分配资源可以是友好的或是强制的。默认不启用资源抢占。</li>
<li><strong>保证最小配额</strong>：可以设置队列最小资源，允许将保证的最小份额分配给队列，保证用户可以启动任务。当队列不能满足最小资源时,可以从其它队列抢占。当队列资源使用不完时,可以给其它队列使用。这对于确保某些用户、组或生产应用始终获得足够的资源。</li>
<li><strong>允许资源共享</strong>：即当一个应用运行时,如果其它队列没有任务执行,则可以使用其它队列,当其它队列有应用需要资源时再将占用的队列释放出来。所有的应用都从资源队列中分配资源。</li>
<li><strong>默认不限制每个队列和用户可以同时运行应用的数量</strong>。可以配置来限制队列和用户并行执行的应用数量。限制并行执行应用数量不会导致任务提交失败,超出的应用会在队列中等待。</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h2 id="数据仓库基础与Apache-Hive入门"><a href="#数据仓库基础与Apache-Hive入门" class="headerlink" title="数据仓库基础与Apache Hive入门"></a>数据仓库基础与Apache Hive入门</h2><h3 id="数据仓库基本概念"><a href="#数据仓库基本概念" class="headerlink" title="数据仓库基本概念"></a>数据仓库基本概念</h3><h4 id="数据仓库概念"><a href="#数据仓库概念" class="headerlink" title="数据仓库概念"></a>数据仓库概念</h4><h5 id="数仓概念"><a href="#数仓概念" class="headerlink" title="数仓概念"></a>数仓概念</h5><ul>
<li>数据仓库（Data Warehouse，简称<strong>数仓、DW</strong>）,是一个<strong>用于存储、分析、报告的数据系统</strong>。</li>
<li>数据仓库的目的是构建<strong>面向分析</strong>的集成化数据环境，分析结果为企业提供决策支持（Decision Support）。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-23-10.png" alt="数仓"></li>
</ul>
<div class="note red icon-padding modern"><i class="note-icon fas fa-question"></i><p>1.假如你现在手里有2000w,当下的时间点去投资口罩生产，你做不做？能不能赚钱？<br>2.假如你是公司营销总监，是否愿意招聘女主播进行短视频带货直播销售？</p>
</div>
<h5 id="数仓专注分析"><a href="#数仓专注分析" class="headerlink" title="数仓专注分析"></a>数仓专注分析</h5><ul>
<li>数据仓库<strong>本身并不“生产”任何数据</strong>，其数据来源于不同外部系统；</li>
<li>同时数据仓库自身<strong>也不需要“消费”任何的数据</strong>，其结果开放给各个外部应用使用；</li>
<li>这也是为什么叫“仓库”，而不叫“工厂”的原因。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-31-34.png" alt="数仓专注分析"></li>
</ul>
<h4 id="场景案例：数据仓库为何而来"><a href="#场景案例：数据仓库为何而来" class="headerlink" title="场景案例：数据仓库为何而来"></a>场景案例：数据仓库为何而来</h4><div class="note info modern"><p>数仓<strong>为了分析数据而来</strong>，分析结果给企业决策提供支撑。</p>
</div>
<p>下面以中国人寿保险公司（chinalife）发展为例，阐述数据仓库为何而来。<br><strong>1.业务数据存储问题</strong></p>
<ul>
<li>中国人寿保险（集团）公司下辖多条业务线，包括：人寿险、财险、车险，养老险等。各业务线的业务正常运营需要记录维护包括客户、保单、收付费、核保、理赔等信息。这么多<strong>业务数据存储在哪里呢</strong>？</li>
<li><mark class="hl-label red">联机事务处理系统（OLTP）</mark> 正好可以满足上述业务需求开展, 其主要任务是执行联机事务处理。其基本特征是<strong>前台接收的用户数据可以立即传送到后台进行处理，并在很短的时间内给出处理结果</strong>。</li>
<li><mark class="hl-label red">关系型数据库（RDBMS）是OLTP典型应用</mark> ，比如：Oracle、MySQL、SQL Server等。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-36-36.png" alt="人寿例子"></li>
</ul>
<p><strong>2.分析型决策的制定</strong><br>随着集团业务的持续运营，<strong>业务数据将会越来越多</strong>。由此也产生出许多运营相关的困惑：</p>
<ul>
<li>能够确定哪些险种正在恶化或已成为不良险种？</li>
<li>能够用有效的方式制定新增和续保的政策吗？</li>
<li>理赔过程有欺诈的可能吗？</li>
<li>现在得到的报表是否只是某条业务线的？集团整体层面数据如何？</li>
</ul>
<p>为了能够正确认识这些问题，制定相关的解决措施，瞎拍桌子是肯定不行的。<br>最稳妥办法就是：<strong>基于业务数据开展数据分析，基于分析的结果给决策提供支撑</strong>。也就是所谓的数据驱动决策的制定。</p>
<h5 id="OLTP环境开展分析可行吗？"><a href="#OLTP环境开展分析可行吗？" class="headerlink" title="OLTP环境开展分析可行吗？"></a>OLTP环境开展分析可行吗？</h5><p><strong>可以，但是没必要</strong>!</p>
<p>OLTP系统的核心是面向业务，支持业务，支持事务。所有的业务操作可以分为读、写两种操作，一般来说<strong>读的压力明显大于写的压力</strong>。如果在OLTP环境直接开展各种分析，有以下问题需要考虑：</p>
<ul>
<li>数据分析也是对数据进行读取操作，<strong>会让读取压力倍增</strong>；</li>
<li>OLTP<strong>仅存储数周或数月的数据</strong>；</li>
<li><strong>数据分散</strong>在不同系统不同表中，字段类型属性不统一；</li>
</ul>
<h5 id="数据仓库面世"><a href="#数据仓库面世" class="headerlink" title="数据仓库面世"></a>数据仓库面世</h5><ul>
<li>当分析所涉及数据规模较小的时候，在业务低峰期时可以在OLTP系统上开展直接分析。</li>
<li>但<strong>为了更好的进行各种规模的数据分析，同时也不影响OLTP系统运行，此时需要构建一个集成统一的数据分析平台</strong>。该平台的目的很简单：<strong>面向分析，支持分析</strong>，并且和OLTP系统解耦合。</li>
<li>基于这种需求，数据仓库的雏形开始在企业中出现了。</li>
</ul>
<h5 id="数据仓库的构建"><a href="#数据仓库的构建" class="headerlink" title="数据仓库的构建"></a>数据仓库的构建</h5><ul>
<li>就如数仓定义所说,<strong>数仓是一个用于存储、分析、报告的数据系统</strong>，目的是<strong>构建面向分析的集成化数据环境</strong>。我们把这种<strong>面向分析、支持分析的系统</strong>称之为<mark class="hl-label red">OLAP（联机分析处理）系统</mark> 。当然，数据仓库是OLAP系统的一种实现。</li>
<li>中国人寿保险公司就可以基于分析决策需求，构建数仓平台。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-46-08.png" alt="数仓构建"></li>
</ul>
<h4 id="数据仓库主要特征"><a href="#数据仓库主要特征" class="headerlink" title="数据仓库主要特征"></a>数据仓库主要特征</h4><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-47-19.png" alt="数据仓库主要特征"></p>
<p>1.<strong>面向主题性(Subject-Oriented)</strong></p>
<ul>
<li>主题是一个抽象的概念，是较高层次上企业信息系统中的<strong>数据综合、归类</strong>并进行分析利用的抽象。在逻辑意义上，它是对应企业中某一宏观分析领域所涉及的分析对象。</li>
<li>传统OLTP系统对数据的划分并不适用于决策分析。而基于主题组织的数据则不同，它们被划分为各自独立的领域，每个领域有各自的逻辑内涵但互不交叉，在<strong>抽象层次上对数据进行完整、一致和准确的描述</strong>。</li>
</ul>
<p>2.<strong>集成性(Integrated)</strong></p>
<ul>
<li>主题相关的<strong>数据通常会分布在多个操作型系统中，彼此分散、独立、异构</strong>。</li>
<li>因此在数据进入数据仓库之前，必然要经过<strong>统一与综合，对数据进行抽取、清理、转换和汇总</strong>，这一步是数据仓库建设中最关键、最复杂的一步，需要完成的工作有：<ul>
<li>要<strong>统一源数据中所有矛盾之处</strong>；<br>如字段的同名异义、异名同义、单位不统一、字长不一致等等。</li>
<li>进行<strong>数据综合和计算</strong>。<br>数据仓库中的数据综合工作可以在从原有数据库抽取数据时生成，但许多是在数据仓库内部生成的，即进入数据仓库以后进行综合生成的。</li>
</ul>
</li>
<li>下图说明了保险公司综合数据的简单处理过程，其中数据仓库中与“承保”主题有关的数据来自于多个不同的操作型系统。</li>
<li>这些系统内部数据的命名可能不同，数据格式也可能不同。把不同来源的数据存储到数据仓库之前，需要去除这些不一致。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-55-35.png" alt="集成性说明"></li>
</ul>
<p>3.<strong>非易失性、非异变性(Non-Volatile)</strong></p>
<ul>
<li><strong>数据仓库是分析数据的平台，而不是创造数据的平台</strong>。我们是通过数仓去分析数据中的规律，而不是去创造修改其中的规律。因此数据进入数据仓库后，它便稳定且不会改变。</li>
<li><strong>数据仓库的数据反映的是一段相当长的时间内历史数据的内容</strong>，数据仓库的用户对数据的操作大多是数据查询或比较复杂的挖掘，一旦数据进入数据仓库以后，一般情况下被较长时间保留。</li>
<li>数据仓库中一般有<strong>大量的查询操作</strong>，<strong>但修改和删除操作很少</strong>。</li>
</ul>
<p>4.<strong>时变性(Time-Variant)</strong></p>
<ul>
<li>数据仓库包含各种粒度的<strong>历史数据</strong>，数据可能与某个特定日期、星期、月份、季度或者年份有关。</li>
<li>当业务变化后会失去时效性。因此数据仓库的<strong>数据需要随着时间更新，以适应决策的需要</strong>。</li>
<li>从这个角度讲，数据仓库建设是一个项目，更是一个过程。</li>
</ul>
<h4 id="数据仓库主流开发语言—SQL"><a href="#数据仓库主流开发语言—SQL" class="headerlink" title="数据仓库主流开发语言—SQL"></a>数据仓库主流开发语言—SQL</h4><h5 id="数仓开发语言概述"><a href="#数仓开发语言概述" class="headerlink" title="数仓开发语言概述"></a>数仓开发语言概述</h5><ul>
<li>数仓作为面向分析的数据平台，其主职工作就是对存储在其中的数据开展分析，那么如何读取数据分析呢？</li>
<li>理论上来说，<strong>任何一款编程语言只要具备读写数据、处理数据的能力，都可以用于数仓的开发</strong>。比如C、java、Python等；</li>
<li><strong>关键在于编程语言是否易学、好用、功能是否强大</strong>。遗憾的是上面所列出的C、Python等编程语言都需要一定的时间进行语法的学习，并且学习语法之后还需要结合分析的业务场景进行编码，跑通业务逻辑。</li>
<li>不管从学习成本还是开发效率来说，上述所说的编程语言都不是十分友好的。</li>
<li>在数据分析领域，不得不提的就是<strong>SQL编程语言，应该称之为分析领域主流开发语言</strong>。</li>
</ul>
<h5 id="SQL语言介绍"><a href="#SQL语言介绍" class="headerlink" title="SQL语言介绍"></a>SQL语言介绍</h5><ul>
<li><strong>结构化查询语言</strong>(Structured Query Language) 简称<code>SQL</code>，是一种数据库查询和程序设计语言，用于<strong>存取</strong>数据以及<strong>查询</strong>、<strong>更新</strong>和<strong>管理</strong>数据。</li>
<li>SQL语言使我们有能力访问数据库，并且SQL是一种ANSI（美国国家标准化组织）的<strong>标准计算机语言</strong>，各大数据库厂商在生产数据库软件的时候，几乎都会去支持SQL的语法，以使得用户在使用软件时更加容易上手，以及在不同厂商软件之间进行切换时更加适应，因为大家的SQL语法都差不多。</li>
<li>SQL语言<strong>功能很强</strong>，十分简洁，核心功能只用了9个动词。语法接近英语口语，所以，很容易学习和使用。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-03-00.png" alt="SQL语言功能很强"></li>
</ul>
<h5 id="数仓与SQL"><a href="#数仓与SQL" class="headerlink" title="数仓与SQL"></a>数仓与SQL</h5><ul>
<li>虽然SQL语言本身是针对数据库软件设计的，但是在<strong>数据仓库领域</strong>，尤其是大数据数仓领域，很多数仓软件<strong>都会去支持SQL语法</strong>；</li>
<li>原因在于一是用户<strong>学习SQL成本低</strong>，二是SQL语言对于<strong>数据分析真的十分友好，爱不释手</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-04-48.png" alt="SQL语言"></li>
</ul>
<div class="note red icon-padding modern"><i class="note-icon fas fa-question"></i><p>1.SQL全称叫做结构化查询语言，结构化是什么意思？<br>2.有没有非结构化之说？</p>
</div>
<h5 id="结构化数据"><a href="#结构化数据" class="headerlink" title="结构化数据"></a>结构化数据</h5><ul>
<li><strong>结构化数据</strong>也称作行数据，是由<strong>二维表结构来逻辑表达和实现的数据</strong>，严格地遵循数据格式与长度规范，主要通过关系型数据库进行存储和管理。</li>
<li>与结构化数据相对的是不适于由数据库二维表来表现的<strong>非结构化数据</strong>，包括所有格式的办公文档、XML、HTML、各类报表、图片和音频、视频信息等。</li>
<li>通俗来说，结构化数据会有严格的行列对齐，便于解读与理解。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-07-07.png" alt="结构化数据与非结构化数据"></li>
</ul>
<h5 id="二维表结构"><a href="#二维表结构" class="headerlink" title="二维表结构"></a>二维表结构</h5><ul>
<li>表由一个名字标识（例如“客户”或者“订单”），叫做表名。表包含带有数据的记录（行）。</li>
<li>下面的例子是一个名为“Persons” 的表，包含三条记录（每一条对应一个人）和五个列（Id、姓、名、地址和城市）。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-08-09.png" alt="二维表结构"></li>
</ul>
<h5 id="SQL语法分类"><a href="#SQL语法分类" class="headerlink" title="SQL语法分类"></a>SQL语法分类</h5><div class="note info modern"><p>SQL主要语法分为两个部分：<strong>数据定义语言(DDL)</strong>和<strong>数据操纵语言(DML)</strong> 。</p>
</div>
<ul>
<li><p>DDL语法使我们有能力<strong>创建或删除表</strong>，以及数据库、索引等各种对象，但是不涉及表中具体数据操作：<br><mark class="hl-label red">CREATE</mark> DATABASE -创建新数据库<br>CREATE TABLE -创建新表</p>
</li>
<li><p>DML语法是我们有能力针对<strong>表中的数据进行插入、更新、删除、查询</strong>操作：<br><mark class="hl-label red">SELECT</mark>  -从数据库表中获取数据<br><mark class="hl-label red">UPDATE</mark>  -更新数据库表中的数据<br><mark class="hl-label red">DELETE</mark>  -从数据库表中删除数据<br><mark class="hl-label red">INSERT</mark>  -向数据库表中插入数据</p>
</li>
</ul>
<h3 id="Apache-Hive入门"><a href="#Apache-Hive入门" class="headerlink" title="Apache Hive入门"></a>Apache Hive入门</h3><h4 id="Apache-Hive概述"><a href="#Apache-Hive概述" class="headerlink" title="Apache Hive概述"></a>Apache Hive概述</h4><h5 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h5><ul>
<li>Apache Hive是一款建立在Hadoop之上的开源<strong>数据仓库</strong>系统，可以将存储在Hadoop文件中的<strong>结构化、半结构化数据文件映射为一张数据库表</strong>，基于表提供了一种类似SQL的查询模型，称为<strong>Hive查询语言（HQL）</strong>，用于访问和分析存储在Hadoop文件中的大型数据集。</li>
<li>Hive核心是将<strong>HQL转换为MapReduce程序</strong>，然后将程序提交到Hadoop群集执行。</li>
<li>Hive由Facebook实现并开源。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-15-37.png" alt="Hive"></li>
</ul>
<h5 id="为什么使用Hive"><a href="#为什么使用Hive" class="headerlink" title="为什么使用Hive"></a>为什么使用Hive</h5><ul>
<li><p>使用Hadoop MapReduce直接处理数据所面临的问题<br>人员学习成本太高需要掌握java语言<br>MapReduce实现复杂查询逻辑开发难度太大</p>
</li>
<li><p>使用Hive处理数据的好处<br>操作接口采用<strong>类SQL语法</strong>，提供快速开发的能力（简单、容易上手）<br>避免直接写MapReduce，减少开发人员的学习成本<br>支持自定义函数，功能扩展很方便<br>背靠Hadoop，<strong>擅长存储分析海量数据集</strong></p>
</li>
</ul>
<h5 id="Hive和Hadoop关系"><a href="#Hive和Hadoop关系" class="headerlink" title="Hive和Hadoop关系"></a>Hive和Hadoop关系</h5><ul>
<li><p>从功能来说，数据仓库软件，至少需要具备下述两种能力：<br>存储数据的能力、分析数据的能力</p>
</li>
<li><p>Apache Hive作为一款大数据时代的数据仓库软件，当然也具备上述两种能力。只不过Hive并不是自己实现了上述两种能力，而是借助Hadoop。<br><strong>Hive利用HDFS存储数据，利用MapReduce查询分析数据</strong>。</p>
</li>
<li><p>这样突然发现Hive没啥用，不过是套壳Hadoop罢了。其实不然，Hive的最大的魅力在于<strong>用户专注于编写HQL，Hive帮您转换成为MapReduce程序完成对数据的分析</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-18-50.png" alt="Hive和Hadoop关系"></p>
</li>
</ul>
<h4 id="场景设计：如何模拟实现Hive功能"><a href="#场景设计：如何模拟实现Hive功能" class="headerlink" title="场景设计：如何模拟实现Hive功能"></a>场景设计：如何模拟实现Hive功能</h4><div class="note red icon-padding modern"><i class="note-icon fas fa-question"></i><p>如果我们来设计Hive这款软件，要求能够实现用户只编写sql语句，Hive自动将<strong>sql转换MapReduce程序</strong>，处理位于HDFS上的结构化数据。如何实现？</p>
</div>
<mark class="hl-label green">案例：如何模拟实现Apache&nbsp;Hive的功能</mark> 
<p>在HDFS文件系统上有一个文件，路径为/data/china_user.txt；<br>需求：统计来自于上海年龄大于25岁的用户有多少个？<br><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-33-08.png" style="height:150px"></p>
<div class="note icon-padding modern"><i class="note-icon fas fa-question"></i><p>Hive能将数据文件映射成为一张表，这个<strong>映射</strong>是指什么？<br>Hive软件本身到底承担了什么<strong>功能职责</strong>？</p>
</div>
<h5 id="映射信息记录"><a href="#映射信息记录" class="headerlink" title="映射信息记录"></a>映射信息记录</h5><ul>
<li><strong>映射</strong>在数学上称之为一种<strong>对应关系</strong>，比如y=x+1，对于每一个x的值都有与之对应的y的值。</li>
<li>在hive中<strong>能够写sql处理的前提是针对表，而不是针对文件</strong>，因此需要将<strong>文件和表之间的对应关系</strong>描述记录清楚。映射信息专业的叫法称之为<strong>元数据信息</strong>（元数据是指用来描述数据的数据metadata）。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-36-11.png" alt="映射信息记录"></li>
<li>具体来看，要记录的元数据信息包括：<ul>
<li>表对应着哪个文件（位置信息）</li>
<li>表的列对应着文件哪一个字段（顺序信息）</li>
<li>文件字段之间的分隔符是什么</li>
</ul>
</li>
</ul>
<h5 id="SQL语法解析、编译"><a href="#SQL语法解析、编译" class="headerlink" title="SQL语法解析、编译"></a>SQL语法解析、编译</h5><ul>
<li>用户写完sql之后，hive需要针对sql进行语法校验，并且根据记录的元数据信息解读sql背后的含义，制定执行计划。</li>
<li>并且把执行计划转换成MapReduce程序来具体执行，把执行的结果封装返回给用户。</li>
</ul>
<h5 id="对Hive的理解"><a href="#对Hive的理解" class="headerlink" title="对Hive的理解"></a>对Hive的理解</h5><ul>
<li><p>Hive能将数据文件映射成为一张表，这个<strong>映射</strong>是指什么？<br><strong>文件和表之间的对应关系</strong></p>
</li>
<li><p>Hive软件本身到底承担了什么<strong>功能职责</strong>？<br><strong>SQL语法解析编译成为MapReduce</strong></p>
</li>
<li><p>基于上述分析，最终要想模拟实现的Hive的功能，大致需要下图所示组件参与其中。</p>
</li>
<li>从中可以感受一下Hive承担了什么职责，当然，也可以把这个理解为Hive的架构图。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-41-04.png" alt="对Hive的理解"></li>
</ul>
<h4 id="Apache-Hive架构、组件"><a href="#Apache-Hive架构、组件" class="headerlink" title="Apache Hive架构、组件"></a>Apache Hive架构、组件</h4><h5 id="Hive架构图"><a href="#Hive架构图" class="headerlink" title="Hive架构图"></a>Hive架构图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-04-11.png" alt="Hive架构图"></p>
<h5 id="Hive组件"><a href="#Hive组件" class="headerlink" title="Hive组件"></a>Hive组件</h5><ul>
<li><p><strong>用户接口</strong><br>包括CLI、JDBC/ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；Hive中的Thrift服务器允许外部客户端通过网络与Hive进行交互，类似于JDBC或ODBC协议。WebGUI是通过浏览器访问Hive。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-07-09.png" alt="用户接口"></p>
</li>
<li><p><strong>元数据存储</strong><br>通常是存储在关系数据库如mysql/derby中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-08-07.png" alt="元数据存储"></p>
</li>
<li><p><strong>Driver驱动程序，包括语法解析器、计划编译器、优化器、执行器</strong><br>完成HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有执行引擎调用执行。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-08-20.png" alt="Driver驱动程序"></p>
</li>
<li><p><strong>执行引擎</strong><br>Hive本身并不直接处理数据文件。而是通过执行引擎处理。当下Hive支持MapReduce、Tez、Spark3种执行引擎。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-08-37.png" alt="执行引擎"></p>
</li>
</ul>
<h3 id="Apache-Hive安装部署"><a href="#Apache-Hive安装部署" class="headerlink" title="Apache Hive安装部署"></a>Apache Hive安装部署</h3><h4 id="Apache-Hive元数据"><a href="#Apache-Hive元数据" class="headerlink" title="Apache Hive元数据"></a>Apache Hive元数据</h4><p><strong>元数据</strong>（Metadata），又称中介数据、中继数据，为<strong>描述数据的数据</strong>（data about data），主要是描述数据属性（property）的信息，用来支持如指示存储位置、历史数据、资源查找、文件记录等功能。</p>
<h5 id="Hive-Metadata"><a href="#Hive-Metadata" class="headerlink" title="Hive Metadata"></a>Hive Metadata</h5><ul>
<li><strong>Hive Metadata即Hive的元数据</strong>。</li>
<li>包含用Hive创建的database、table、表的位置、类型、属性，字段顺序类型等元信息。</li>
<li><strong>元数据存储在关系型数据库中</strong>。如hive内置的Derby、或者第三方如MySQL等。</li>
<li>Metastore即<strong>元数据服务</strong>。Metastore服务的作用是<strong>管理metadata元数据</strong>，对外暴露服务地址，让各种客户端通过连接metastore服务，由metastore再去连接MySQL数据库来存取元数据。</li>
<li>有了metastore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySQL数据库的用户名和密码，只需要连接metastore 服务即可。某种程度上也保证了hive元数据的安全。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-15-15.png" alt="Hive Metadata"></li>
</ul>
<h5 id="metastore配置方式"><a href="#metastore配置方式" class="headerlink" title="metastore配置方式"></a>metastore配置方式</h5><ul>
<li>metastore服务配置有3种模式：内嵌模式、本地模式、<strong>远程模式</strong>。</li>
<li>区分3种配置方式的关键是弄清楚两个问题：<ul>
<li>Metastore服务是否需要单独配置、单独启动？</li>
<li>Metadata是存储在内置的derby中，还是第三方RDBMS,比如MySQL。</li>
</ul>
</li>
<li>企业推荐模式—远程模式部署。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">内嵌模式</th>
<th style="text-align:center">本地模式</th>
<th style="text-align:center">远程模式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Metastore单独配置、启动</td>
<td style="text-align:center">否</td>
<td style="text-align:center">否</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">Metastore存储介质</td>
<td style="text-align:center">Derby</td>
<td style="text-align:center">Mysql</td>
<td style="text-align:center">Mysql</td>
</tr>
</tbody>
</table>
</div>
<h5 id="metastore远程模式"><a href="#metastore远程模式" class="headerlink" title="metastore远程模式"></a>metastore远程模式</h5><p>在生产环境中，建议用远程模式来配置Hive Metastore。在这种情况下，其他依赖hive的软件都可以通过Metastore访问hive。由于还可以完全屏蔽数据库层，因此这也带来了更好的可管理性/安全性。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-24-04.png" alt="metastore远程模式"></p>
<h4 id="Apache-Hive部署实战"><a href="#Apache-Hive部署实战" class="headerlink" title="Apache Hive部署实战"></a>Apache Hive部署实战</h4><mark class="hl-label pink">安装前准备</mark> 
<ul>
<li>由于Apache Hive是一款基于Hadoop的数据仓库软件，通常部署运行在Linux系统之上。因此不管使用何种方式配置Hive Metastore，必须要先保证服务器的基础环境正常，Hadoop集群健康可用。</li>
<li><strong>服务器基础环境</strong><br>集群时间同步、防火墙关闭、主机Host映射、免密登录、JDK安装</li>
<li><strong>Hadoop集群健康可用</strong><br><strong><em>启动Hive之前必须先启动Hadoop集群</em></strong>。特别要注意，需等待HDFS安全模式关闭之后再启动运行Hive。<br>Hive不是分布式安装运行的软件，其分布式的特性主要借由Hadoop完成。包括分布式存储、分布式计算。</li>
</ul>
<mark class="hl-label pink">Hadoop与Hive整合</mark> 
<ul>
<li>因为Hive需要把数据存储在HDFS上，并且通过MapReduce作为执行引擎处理数据；</li>
<li>因此需要在Hadoop中添加相关配置属性，以满足Hive在Hadoop上运行。</li>
<li>修改Hadoop中<code>core-site.xml</code>，并且Hadoop集群同步配置文件，重启生效。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--整合hive --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step1：MySQL安装</mark> 
<div class="note warning modern"><p>MySQL只需要在一台机器安装并且需要授权远程访问</p>
</div>
<figure class="highlight bash"><figcaption><span>安装mysql</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@hsq01 ~]<span class="comment"># mkdir /mysoft/mysql</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#上传 mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar 到上述文件夹下  解压</span></span><br><span class="line"><span class="built_in">cd</span> mysoft/mysql</span><br><span class="line">tar xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行安装</span></span><br><span class="line">yum -y install libaio</span><br><span class="line">yum localinstall *</span><br><span class="line">yum install libncurses*（遇到问题使用）</span><br><span class="line"></span><br><span class="line">rpm -ivh mysql-community-common-5.7.29-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-libs-5.7.29-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-client-5.7.29-1.el7.x86_64.rpm --force --nodeps</span><br><span class="line">rpm -ivh mysql-community-server-5.7.29-1.el7.x86_64.rpm --force --nodeps</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><figcaption><span>mysql初始化设置</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化</span></span><br><span class="line">mysqld --initialize</span><br><span class="line"></span><br><span class="line"><span class="comment">#更改所属组</span></span><br><span class="line"><span class="built_in">chown</span> mysql:mysql /var/lib/mysql -R</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动mysql</span></span><br><span class="line">systemctl start mysqld.service</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看生成的临时root密码</span></span><br><span class="line"><span class="built_in">cat</span>  /var/log/mysqld.log</span><br><span class="line"></span><br><span class="line">[Note] A temporary password is generated <span class="keyword">for</span> root@localhost: o+TU+KDOm004</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><figcaption><span>修改root密码 授权远程访问 设置开机自启动</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[root@hsq01 ~]<span class="comment"># mysql -u root -p</span></span><br><span class="line">Enter password:     <span class="comment">#这里输入在日志中生成的临时密码</span></span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection <span class="built_in">id</span> is 3</span><br><span class="line">Server version: 5.7.29</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> or <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> <span class="built_in">help</span>. Type <span class="string">&#x27;\c&#x27;</span> to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment">#更新root密码  设置为hadoop</span></span><br><span class="line">mysql&gt; alter user user() identified by <span class="string">&quot;hadoop&quot;</span>;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="comment">#授权</span></span><br><span class="line">mysql&gt; use mysql;</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED BY <span class="string">&#x27;hadoop&#x27;</span> WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">mysql&gt; FLUSH PRIVILEGES; </span><br><span class="line"></span><br><span class="line"><span class="comment">#mysql的启动和关闭 状态查看 （这几个命令必须记住）</span></span><br><span class="line">systemctl stop mysqld</span><br><span class="line">systemctl status mysqld</span><br><span class="line">systemctl start mysqld</span><br><span class="line"></span><br><span class="line"><span class="comment">#建议设置为开机自启动服务</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># systemctl enable  mysqld</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/mysqld.service to /usr/lib/systemd/system/mysqld.service.</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看是否已经设置自启动成功</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># systemctl list-unit-files | grep mysqld</span></span><br><span class="line">mysqld.service                                enabled</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><figcaption><span>出现问题可以干净卸载mysql 5.7重来</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#关闭mysql服务</span></span><br><span class="line">systemctl stop mysqld.service</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找安装mysql的rpm包</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># rpm -qa | grep -i mysql</span></span><br><span class="line">mysql-community-libs-5.7.29-1.el7.x86_64</span><br><span class="line">mysql-community-common-5.7.29-1.el7.x86_64</span><br><span class="line">mysql-community-client-5.7.29-1.el7.x86_64</span><br><span class="line">mysql-community-server-5.7.29-1.el7.x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment">#卸载</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># yum remove mysql-community-libs-5.7.29-1.el7.x86_64 mysql-community-common-5.7.29-1.el7.x86_64 mysql-community-client-5.7.29-1.el7.x86_64 mysql-community-server-5.7.29-1.el7.x86_64</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看是否卸载干净</span></span><br><span class="line">rpm -qa | grep -i mysql</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找mysql相关目录 删除</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># find / -name mysql</span></span><br><span class="line">/var/lib/mysql</span><br><span class="line">/var/lib/mysql/mysql</span><br><span class="line">/usr/share/mysql</span><br><span class="line"></span><br><span class="line">[root@hsq01 ~]<span class="comment"># rm -rf /var/lib/mysql</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># rm -rf /var/lib/mysql/mysql</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># rm -rf /usr/share/mysql</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#删除默认配置 日志</span></span><br><span class="line"><span class="built_in">rm</span> -rf /etc/my.cnf</span><br><span class="line"><span class="built_in">rm</span> -rf /var/log/mysqld.log</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step2：上传解压Hive安装包（node1安装即可）</mark> 
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /mysoft</span><br><span class="line"></span><br><span class="line"><span class="comment">#上传 apache-hive-3.1.2-bin.tar.gz 到上述文件夹下  解压</span></span><br><span class="line">tar zxvf apache-hive-3.1.2-bin.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">#解决Hive与Hadoop之间guava版本差异</span></span><br><span class="line"><span class="built_in">cd</span> /mysoft/apache-hive-3.1.2-bin/</span><br><span class="line"><span class="built_in">rm</span> -rf lib/guava-19.0.jar</span><br><span class="line"><span class="built_in">cp</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/lib/guava-27.0-jre.jar ./lib/</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step3：配置Hive系统环境变量</mark> 
<figure class="highlight sh"><figcaption><span>vi /etc/profile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hive environment variables</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/mysoft/apache-hive-3.1.2-bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step4：修改hive-env.sh</mark> 
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /mysoft/apache-hive-3.1.2-bin/conf</span><br><span class="line"><span class="built_in">mv</span> hive-env.sh.template hive-env.sh</span><br><span class="line"></span><br><span class="line">vi hive-env.sh</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/local/hadoop-3.3.4</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/mysoft/apache-hive-3.1.2-bin/conf</span><br><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=/mysoft/apache-hive-3.1.2-bin/lib</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step5：新增hive-site.xml</mark> 
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 存储元数据mysql相关配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hsq01:3306/hive3?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=UTF-8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- H2S运行绑定host --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hsq01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 远程模式部署metastore metastore地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hsq01:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭元数据存储授权  --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step6：添加驱动、初始化</mark> 
<ul>
<li>上传MySQL JDBC驱动到Hive安装包lib路径下<code>mysql-connector-java-5.1.32.jar</code></li>
<li>初始化Hive的元数据<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /mysoft/apache-hive-3.1.2-bin/</span><br><span class="line"></span><br><span class="line">bin/schematool -initSchema -dbType mysql -verbos</span><br><span class="line"><span class="comment">#初始化成功会在mysql中创建74张表</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="metastore服务启动方式"><a href="#metastore服务启动方式" class="headerlink" title="metastore服务启动方式"></a>metastore服务启动方式</h5><mark class="hl-label green">前台启动</mark> 
<p><strong>前台启动</strong>，进程会一直占据终端，<code>ctrl + c</code>结束进程，服务关闭。<br>可以根据需求添加参数开启debug日志，获取详细日志信息，便于排错。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#前台启动  关闭ctrl+c</span></span><br><span class="line">/mysoft/apache-hive-3.1.2-bin/bin/hive --service metastore</span><br><span class="line"></span><br><span class="line"><span class="comment">#前台启动开启debug日志</span></span><br><span class="line">/mysoft/apache-hive-3.1.2-bin/bin/hive --service metastore --hiveconf hive.root.logger=DEBUG,console</span><br><span class="line"></span><br><span class="line"><span class="comment">#前台启动关闭方式ctrl+c结束进程</span></span><br></pre></td></tr></table></figure>
<mark class="hl-label green">后台启动</mark> 
<p><strong>后台启动</strong>，输出日志信息在/root目录下nohup.out</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /mysoft/apache-hive-3.1.2-bin/bin/hive --service metastore &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment">#后台挂起启动结束进程使用jps查看进程 使用kill -9 杀死进程</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#nohup 命令，在默认情况下（非重定向时），会输出一个名叫nohup.out 的文件到当前目录下</span></span><br></pre></td></tr></table></figure>
<h4 id="Apache-Hive客户端使用"><a href="#Apache-Hive客户端使用" class="headerlink" title="Apache Hive客户端使用"></a>Apache Hive客户端使用</h4><h5 id="（1）Hive自带客户端"><a href="#（1）Hive自带客户端" class="headerlink" title="（1）Hive自带客户端"></a>（1）Hive自带客户端</h5><p><code>bin/hive</code>、<code>bin/beeline</code></p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-13-11-26-19.png" alt="Remote Metastore远程模式"></p>
<ul>
<li>Hive发展至今，总共历经了两代客户端工具。</li>
<li>第一代客户端（deprecated不推荐使用）：<code>$HIVE_HOME/bin/hive</code>, 是一个shellUtil。主要功能：一是可用于以交互或批处理模式运行Hive查询；二是用于Hive相关服务的启动，比如metastore服务。</li>
<li>第二代客户端（recommended 推荐使用）：<code>$HIVE_HOME/bin/beeline</code>，是一个JDBC客户端，是<strong>官方强烈推荐使用</strong>的Hive命令行工具，和第一代客户端相比，性能加强安全性提高。</li>
</ul>
<h5 id="HiveServer2服务介绍"><a href="#HiveServer2服务介绍" class="headerlink" title="HiveServer2服务介绍"></a>HiveServer2服务介绍</h5><ul>
<li><strong>远程模式下beeline通过Thrift 连接到单独的HiveServer2服务上</strong>，这也是官方推荐在生产环境中使用的模式。</li>
<li>HiveServer2支持多客户端的并发和身份认证，旨在为开放API客户端如JDBC、ODBC提供更好的支持。</li>
</ul>
<h5 id="Hive客户端和服务的关系"><a href="#Hive客户端和服务的关系" class="headerlink" title="Hive客户端和服务的关系"></a>Hive客户端和服务的关系</h5><ul>
<li><p>HiveServer2通过Metastore服务读写元数据。所以在远程模式下，<strong>启动HiveServer2之前必须先首先启动metastore服务</strong>。</p>
</li>
<li><p>特别注意：远程模式下，<code>Beeline</code>客户端只能通过HiveServer2服务访问Hive。而<code>bin/hive</code>是通过Metastore服务访问的。具体关系如下：<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-13-11-33-50.png" alt="Hive客户端和服务的关系"></p>
</li>
</ul>
<h5 id="bin-beeline客户端使用"><a href="#bin-beeline客户端使用" class="headerlink" title="bin/beeline客户端使用"></a><code>bin/beeline</code>客户端使用</h5><ul>
<li><p>在hive安装的服务器上，<strong>首先启动metastore服务，然后启动hiveserver2服务</strong>。</p>
<figure class="highlight bash"><figcaption><span>先启动metastore服务然后启动hiveserver2服务</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /mysoft/apache-hive-3.1.2-bin/bin/hive --service metastore &amp;</span><br><span class="line"><span class="built_in">nohup</span> /mysoft/apache-hive-3.1.2-bin/bin/hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure>
<div class="note warning modern"><p>启动hiveserver2需要一定的时间  不要启动之后立即beeline连接 可能连接不上</p>
</div>
</li>
<li><p>在node3上使用beeline客户端进行连接访问。需要注意<strong>hiveserver2服务启动之后需要稍等一会才可以对外提供服务</strong>。</p>
<figure class="highlight bash"><figcaption><span>拷贝node1安装包到beeline客户端机器上（node3）</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /mysoft/apache-hive-3.1.2-bin/ hsq03:/mysoft/</span><br></pre></td></tr></table></figure>
<div class="note info modern"><p>node3中同样需要配置hadoop集群<code>core-site.xml</code>文件，<a href="#Apache-Hive部署实战">同上</a></p>
</div>
</li>
<li><p>Beeline是JDBC的客户端，通过JDBC协议和Hiveserver2服务进行通信，协议的地址是：jdbc:hive2://node1:10000</p>
<figure class="highlight bash"><figcaption><span>连接访问</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/mysoft/apache-hive-3.1.2-bin/bin/beeline</span><br><span class="line"></span><br><span class="line">beeline&gt; ! connect jdbc:hive2://node1:10000</span><br><span class="line">beeline&gt; root</span><br><span class="line">beeline&gt; 直接回车</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="2-Hive可视化工具客户端"><a href="#2-Hive可视化工具客户端" class="headerlink" title="(2)Hive可视化工具客户端"></a>(2)Hive可视化工具客户端</h5><h6 id="Hive可视化工具"><a href="#Hive可视化工具" class="headerlink" title="Hive可视化工具"></a>Hive可视化工具</h6><p>Hive可视化工具有DataGrip、Dbeaver、SQuirrel SQL Client等</p>
<ul>
<li>可以在Windows、MAC平台中<strong>通过JDBC连接HiveServer2的图形界面工具</strong>；</li>
<li>这类工具往往专门针对SQL类软件进行开发优化、<strong>页面美观大方</strong>，<strong>操作简洁</strong>，更重要的是<strong>SQL编辑环境优雅</strong>；</li>
<li><strong>SQL语法智能提示补全、关键字高亮、查询结果智能显示、按钮操作大于命令操作</strong>；</li>
</ul>
<h6 id="DataGrip"><a href="#DataGrip" class="headerlink" title="DataGrip"></a>DataGrip</h6><p><strong>DataGrip</strong>是由JetBrains公司推出的数据库管理软件，DataGrip支持几乎所有主流的关系数据库产品，如DB2、Derby、MySQL、Oracle、SQL Server等，也支持几乎所有主流的大数据生态圈SQL软件，并且提供了简单易用的界面，开发者上手几乎不会遇到任何困难。</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-03-29-09-43-28.png" alt="DataGrip支持的数据源"></p>
<h5 id="使用DataGrip软件远程连接Hiveserver2服务"><a href="#使用DataGrip软件远程连接Hiveserver2服务" class="headerlink" title="使用DataGrip软件远程连接Hiveserver2服务"></a>使用DataGrip软件远程连接Hiveserver2服务</h5><mark class="hl-label pink">step1:新建项目</mark> 
<p>新建项目…</p>
<mark class="hl-label pink">step2:新建数据源</mark> 
<p>数据源选择<code>Apache Hive</code></p>
<mark class="hl-label pink">step3:配置HiveJDBC连接驱动</mark> 
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-03-29-10-00-42.png" alt="驱动程序"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-03-29-10-01-51.png" alt="选择驱动程序jar包"></p>
<mark class="hl-label pink">step4:左上角箭头(←)返回，配置Hiveserver2服务连接信息</mark> 
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-03-29-10-02-33.png" alt="配置"></p>
<mark class="hl-label pink">step5:连接测试</mark> 
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-03-29-10-07-24.png" alt="连接测试"></p>
<mark class="hl-label pink">step6:新建HQL文件</mark> 
<p>新建HQL文件，开始编写HQL语句</p>
<h3 id="Hive-SQL语言：DDL建库、建表"><a href="#Hive-SQL语言：DDL建库、建表" class="headerlink" title="Hive SQL语言：DDL建库、建表"></a>Hive SQL语言：DDL建库、建表</h3><h4 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h4><h5 id="Hive数据模型总览"><a href="#Hive数据模型总览" class="headerlink" title="Hive数据模型总览"></a>Hive数据模型总览</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-15-15-53.png" alt="Hive数据模型总览"></p>
<h5 id="SQL中DDL语法的作用"><a href="#SQL中DDL语法的作用" class="headerlink" title="SQL中DDL语法的作用"></a>SQL中DDL语法的作用</h5><ul>
<li><strong>数据定义语言</strong>(Data Definition Language,<code>DDL</code>)，是SQL语言集中对数据库内部的<strong>对象结构进行创建，删除，修改</strong>等的操作语言，这些数据库对象包括database、table等。</li>
<li>DDL核心语法由CREATE、ALTER与DROP三个所组成。<strong>DDL并不涉及表内部数据的操作</strong>。</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-15-17-51.png" alt="DDL语法"></p>
<h5 id="Hive中DDL语法的使用"><a href="#Hive中DDL语法的使用" class="headerlink" title="Hive中DDL语法的使用"></a>Hive中DDL语法的使用</h5><ul>
<li>Hive SQL（HQL）与标准SQL的语法大同小异，基本相通；</li>
<li>基于Hive的设计、使用特点，<strong>HQL中create语法（尤其create table）是Hive DDL语法的重中之重</strong>。</li>
<li>建表是否成功直接影响数据文件是否映射成功，进而影响后续是否可以基于SQL分析数据。通俗点说，没有表，表没有数据，你用Hive分析什么呢？</li>
<li><strong>选择正确的方向,往往比盲目努力重要</strong>。</li>
</ul>
<h4 id="Hive-SQL之数据库与建库"><a href="#Hive-SQL之数据库与建库" class="headerlink" title="Hive SQL之数据库与建库"></a>Hive SQL之数据库与建库</h4><div class="tabs" id="database"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#database-1">数据库database</button></li><li class="tab"><button type="button" data-href="#database-2"><b>create database</b></button></li><li class="tab"><button type="button" data-href="#database-3"><b>use database</b></button></li><li class="tab"><button type="button" data-href="#database-4"><b>drop database</b></button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="database-1"><ul>
<li>在Hive中，默认的数据库叫做<code>default</code>，存储数据位置位于HDFS的<code>/user/hive/warehouse</code>目录下</li>
<li>用户自己创建的数据库存储位置是<code>/user/hive/warehouse/database_name.db</code>目录下</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="database-2"><p><code>create database</code><strong>用于创建新的数据库</strong></p>
<figure class="highlight sql"><figcaption><span>语法</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> (DATABASE<span class="operator">|</span>SCHEMA) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name<span class="operator">=</span>property_value, ...)];</span><br></pre></td></tr></table></figure>

<ul>
<li>COMMENT：数据库的注释说明语句</li>
<li>LOCATION：指定数据库在HDFS存储位置，默认<code>/user/hive/warehouse/dbname.db</code>，<strong>如果需要使用location指定路径的时候，最好指向的是一个新创建的空文件夹</strong>。</li>
<li>WITH DBPROPERTIES：用于指定一些数据库的属性配置</li>
</ul>
<figure class="highlight sql"><figcaption><span>for example</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> test</span><br><span class="line">comment &quot;this is my first db&quot;</span><br><span class="line"><span class="keyword">with</span> dbproperties (<span class="string">&#x27;createdBy&#x27;</span><span class="operator">=</span><span class="string">&#x27;Allen&#x27;</span>);</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="database-3"><p><code>use database</code><strong>用于选择特定的数据库进行操作</strong></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="database-4"><p><code>drop database</code><strong>用于删除数据库</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> (DATABASE<span class="operator">|</span>SCHEMA) [IF <span class="keyword">EXISTS</span>] database_name [RESTRICT<span class="operator">|</span>CASCADE];</span><br></pre></td></tr></table></figure>

<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-15-49-37.png" alt="注意"><br>默认行为是<code>RESTRICT</code>，这意味着仅在数据库为空时才删除它。<br>要删除带有表的数据库（不为空的数据库），我们可以使用<code>CASCADE</code>。</p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h4 id="Hive-SQL之表与建表"><a href="#Hive-SQL之表与建表" class="headerlink" title="Hive SQL之表与建表"></a>Hive SQL之表与建表</h4><div class="tabs" id="table"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#table-1">表Table</button></li><li class="tab"><button type="button" data-href="#table-2">建表语法</button></li><li class="tab"><button type="button" data-href="#table-3">数据类型</button></li><li class="tab"><button type="button" data-href="#table-4">分隔符指定语法</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="table-1"><ul>
<li>一个数据库通常包含一个或多个表。每个表由一个名字标识（例如“客户”或者“订单”）</li>
<li>表包含带有数据的记录（行）</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Id</th>
<th>LastName</th>
<th>FirstName</th>
<th>City</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Adams</td>
<td>John</td>
<td>London</td>
</tr>
<tr>
<td>2</td>
<td>Bush</td>
<td>George</td>
<td>New York</td>
</tr>
</tbody>
</table>
</div><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="table-2"><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span>[IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]table_name</span><br><span class="line">(col_name data_type [COMMENT col_comment], ... )</span><br><span class="line">[COMMENT table_comment]</span><br><span class="line">[<span class="type">ROW</span> FORMAT DELIMITED …];</span><br></pre></td></tr></table></figure>

<ul>
<li><code>CREATE TABLE</code>和<code>ROW FORMAT DELIMITED</code>是建表语法的关键字，用于指定某些功能</li>
<li><code>[]</code>中括号的语法表示可选。</li>
<li><strong>建表语句中的语法顺序要和语法树中顺序保持一致</strong>。</li>
<li>最低限度必须包括的语法为：<code>CREATE TABLE table_name (col_name data_type);</code></li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="table-3"><ul>
<li>Hive数据类型指的是表中列的字段类型；</li>
<li>整体分为两类：<strong>原生数据类型</strong>（primitive data type）和<strong>复杂数据类型</strong>（complex data type）</li>
<li>最常用的数据类型是<code>字符串String</code>和<code>数字类型Int</code></li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-16-07-46.png" alt="数据类型"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="table-4"><p><code>ROW FORMATDELIMITED</code>语法用于指定字段之间等相关的分隔符，这样Hive才能正确的读取解析数据，或者说只有分隔符指定正确，解析数据成功，我们才能在表中看到数据。</p>
<p>LazySimpleSerDe是Hive默认的，包含4种子语法，分别用于指定<strong>字段之间</strong>、集合元素之间、map映射kv之间、换行的分隔符号。</p>
<p>在建表的时候可以根据<strong>数据的特点</strong>灵活搭配使用。</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-16-12-12.png" alt="LazySimpleSerDe分隔符指定"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> &quot;\001&quot;</span><br><span class="line">collection items terminated <span class="keyword">by</span> &quot;&quot;</span><br><span class="line">map keys terminated <span class="keyword">by</span> &quot;&quot;</span><br><span class="line">lines terminated <span class="keyword">by</span> &quot;&quot;;</span><br></pre></td></tr></table></figure>

<ul>
<li>Hive建表时如果没有row format语法指定分隔符，则采用<strong>默认分隔符</strong>；</li>
<li><strong>默认的分割符是’\001’</strong>，是一种特殊的字符，使用的是ASCII编码的值，键盘是打不出来的。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-16-34-20.png" alt="默认分隔符"></li>
<li>在<strong>vim编辑器</strong>中，连续按下<code>Ctrl+v/Ctrl+a</code>即可输入<code>&#39;\001&#39;</code> ，显示<code>^A</code><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-16-35-58.png" alt="在vim 编辑器中"></li>
<li>在<strong>vs code编辑器</strong>中以<code>SOH</code>的形式显示<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-16-37-29.png" alt="在vs code编辑器中"></li>
<li>在<strong>记事本</strong>中会乱码显示<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-16-41-55.png" alt="在记事本中"></li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h4 id="Demo——Hive建表基础语法练习"><a href="#Demo——Hive建表基础语法练习" class="headerlink" title="Demo——Hive建表基础语法练习"></a>Demo——Hive建表基础语法练习</h4><div class="tabs" id="demo"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#demo-1">需求</button></li><li class="tab"><button type="button" data-href="#demo-2">建表语句</button></li><li class="tab"><button type="button" data-href="#demo-3">验证结果</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="demo-1"><p>文件<code>archer.txt</code>中记录了手游《王者荣耀》射手的相关信息，包括生命、物防、物攻等属性信息，其中字段之间分隔符为制表符<code>\t</code>,要求在Hive中建表映射成功该文件。</p>
<p><strong>数据格式</strong></p>
<figure class="highlight txt"><figcaption><span>archer.txt</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1    后羿    5986    1784    396    336    remotely    archer</span><br><span class="line">2    马可波罗    5584    200    362    344    remotely    archer</span><br><span class="line">3    鲁班七号    5989    1756    400    323    remotely    archer</span><br><span class="line">4    李元芳    5725    1770    396    340    remotely    archer</span><br></pre></td></tr></table></figure>

<ul>
<li>字段含义：id、name（英雄名称）、hp_max（最大生命）、mp_max（最大法力）、attack_max（最高物攻）、defense_max（最大物防）、attack_range（攻击范围）、role_main（主要定位）、role_assist（次要定位）</li>
<li>分析可知：字段都是基本类型，字段的顺序需要注意</li>
<li>字段之间的分隔符是制表符，需要使用<code>row format</code>语法进行指定</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="demo-2"><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建数据库并切换使用</span></span><br><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> test;</span><br><span class="line">use test;</span><br><span class="line"><span class="comment">--ddl create table</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_archer(</span><br><span class="line">    id <span class="type">int</span> comment &quot;ID&quot;,</span><br><span class="line">    name string comment &quot;英雄名称&quot;,</span><br><span class="line">    hp_max <span class="type">int</span> comment &quot;最大生命&quot;,</span><br><span class="line">    mp_max <span class="type">int</span> comment &quot;最大法力&quot;,</span><br><span class="line">    attack_max <span class="type">int</span> comment &quot;最高物攻&quot;,</span><br><span class="line">    defense_max <span class="type">int</span> comment &quot;最大物防&quot;,</span><br><span class="line">    attack_range string comment &quot;攻击范围&quot;,</span><br><span class="line">    role_main string comment &quot;主要定位&quot;,</span><br><span class="line">    role_assist string comment &quot;次要定位&quot;</span><br><span class="line">) comment &quot;王者荣耀射手信息&quot;</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure>

<ul>
<li>建表成功之后，在Hive的默认存储路径下就生成了表对应的文件夹</li>
<li>把<code>archer.txt</code>文件上传到对应的表文件夹下<figure class="highlight shell"><figcaption><span>node1上操作</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line">mkdir hivedata</span><br><span class="line">cd hivedata/</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">把文件从课程资料中首先上传到node1 linux系统上</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">执行命令把文件上传到HDFS表所对应的目录下</span></span><br><span class="line">hadoop fs -put archer.txt /user/hive/warehouse/test.db/t_archer</span><br></pre></td></tr></table></figure>
</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="demo-3"><p>执行查询操作，可以看出数据已经映射成功。</p>
<p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">use test;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_archer;</span><br></pre></td></tr></table></figure><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-04-03-17-01-32.png" alt="验证结果"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h3 id="Hive-Show语法"><a href="#Hive-Show语法" class="headerlink" title="Hive Show语法"></a>Hive Show语法</h3><h4 id="show语法功能"><a href="#show语法功能" class="headerlink" title="show语法功能"></a>show语法功能</h4><p>Show相关的语句可以帮助用户查询相关信息。<br>比如我们最常使用的查询当前数据库下有哪些表<code>show tables</code>。</p>
<h4 id="常用show语句"><a href="#常用show语句" class="headerlink" title="常用show语句"></a>常用show语句</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--1、显示所有数据库SCHEMAS和DATABASES的用法功能一样</span></span><br><span class="line"><span class="keyword">show</span> databases;</span><br><span class="line"><span class="keyword">show</span> schemas;</span><br><span class="line"></span><br><span class="line"><span class="comment">--2、显示当前数据库所有表</span></span><br><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"><span class="keyword">SHOW</span> TABLES [<span class="keyword">IN</span> database_name]; <span class="comment">--指定某个数据库</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--3、查询显示一张表的元数据信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">desc</span> formatted table_name;</span><br></pre></td></tr></table></figure>
<h3 id="注释comment中文乱码解决"><a href="#注释comment中文乱码解决" class="headerlink" title="注释comment中文乱码解决"></a>注释comment中文乱码解决</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--注意 下面sql语句是需要在MySQL中执行  修改Hive存储的元数据信息（metadata）</span></span><br><span class="line">use hive3;</span><br><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hive3.COLUMNS_V2 modify <span class="keyword">column</span> COMMENT <span class="type">varchar</span>(<span class="number">256</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hive3.TABLE_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hive3.PARTITION_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hive3.PARTITION_KEYS modify <span class="keyword">column</span> PKEY_COMMENT <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hive3.INDEX_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure>
<h2 id="Apache-Hive-DML语句与函数使用"><a href="#Apache-Hive-DML语句与函数使用" class="headerlink" title="Apache Hive DML语句与函数使用"></a>Apache Hive DML语句与函数使用</h2><div class="note info modern"><p><code>DML</code>是<strong>数据库操作语言</strong>(Data Manipulation Language)的缩写，指的是能够在数据库对数据内容进行操作的语言。</p>
</div>
<h3 id="Hive-SQL-DML语法之加载数据"><a href="#Hive-SQL-DML语法之加载数据" class="headerlink" title="Hive SQL DML语法之加载数据"></a>Hive SQL DML语法之加载数据</h3><ul>
<li>在Hive中建表成功之后，就会在HDFS上创建一个与之对应的文件夹，且<strong>文件夹名字就是表名</strong></li>
<li>文件夹父路径是由参数hive.metastore.warehouse.dir控制，默认值是<code>/user/hive/warehouse</code></li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-23-00-26-42.png" alt="Hive建表路径"></li>
<li>不管路径在哪里，只有把数据文件移动到对应的表文件夹下面，Hive才能映射解析成功</li>
<li>最原始暴力的方式就是使用<code>hadoop fs –put|-mv</code>等方式直接将数据移动到表文件夹下</li>
<li>但是，<strong>Hive官方推荐使用<code>Load</code>命令将数据加载到表中</strong></li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-23-00-27-49.png" alt="Hive映射"></li>
</ul>
<h4 id="Hive-SQL-DML-Load加载数据"><a href="#Hive-SQL-DML-Load加载数据" class="headerlink" title="Hive SQL-DML-Load加载数据"></a>Hive SQL-DML-Load加载数据</h4><mark class="hl-label pink">Load语法功能</mark> 
<ul>
<li>Load英文单词的含义为：<strong>加载、装载</strong></li>
<li>所谓加载是指：<strong>将数据文件移动到与Hive表对应的位置，移动时是纯复制、移动操作</strong></li>
<li><strong>纯复制、移动</strong>指在数据load加载到表中时，Hive不会对表中的数据内容进行任何转换，任何操作</li>
</ul>
<mark class="hl-label pink">Load语法功能</mark> 
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data [<span class="keyword">local</span>] inpath <span class="string">&#x27;filepath&#x27;</span> [overwrite] <span class="keyword">into</span> <span class="keyword">table</span> tablename;</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">语法规则之filepath</mark> 
<ul>
<li><code>filepath</code>表示<strong>待移动数据的路径</strong>。可以指向文件（在这种情况下，Hive将文件移动到表中），也可以指向目录（在这种情况下，Hive将把该目录中的所有文件移动到表中）。</li>
<li>filepath文件路径支持下面三种形式，要结合local关键字一起考虑：<ol>
<li>相对路径，例如：project/data1</li>
<li>绝对路径，例如：/user/hive/project/data1</li>
<li>具有<code>schema</code>的完整URI，例如：hdfs://namenode:9000/user/hive/project/data1</li>
</ol>
</li>
</ul>
<mark class="hl-label pink">语法规则之LOCAL</mark> 
<p><strong>指定LOCAL</strong>，将在本地文件系统中查找文件路径。</p>
<ul>
<li>若指定相对路径，将相对于用户的当前工作目录进行解释；</li>
<li>用户也可以为本地文件指定完整的URI-例如：file:///user/hive/project/data1。</li>
</ul>
<p><strong>没有指定LOCAL关键字</strong>。</p>
<ul>
<li>如果filepath指向的是一个完整的URI，会直接使用这个URI；</li>
<li>如果没有指定schema，Hive会使用在hadoop配置文件中参数fs.default.name指定的（不出意外，都是HDFS）</li>
</ul>
<div class="note pink icon-padding modern"><i class="note-icon fa-solid fa-question"></i><p>LOCAL本地是哪里？</p>
</div>
<div class="note blue icon-padding modern"><i class="note-icon fa-solid fa-comment"></i><p>如果对HiveServer2服务运行此命令<br><strong>本地文件系统</strong>指的是<strong>Hiveserver2服务所在机器的本地Linux文件系统</strong>，不是Hive客户端所在的本地文件系统。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-23-01-20-14.png" alt="LOCAL本地是哪里？"></p>
</div>
<mark class="hl-label pink">语法规则之OVERWRITE</mark> 
<p>指定OVERWRITE代表将原表中的数据进行覆盖，不加表示追加。</p>
<blockquote><p>demo</p>
</blockquote>
<div class="tabs" id="load"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#load-1">数据集</button></li><li class="tab"><button type="button" data-href="#load-2">建表</button></li><li class="tab"><button type="button" data-href="#load-3">load加载数据</button></li><li class="tab"><button type="button" data-href="#load-4">加载数据日志信息</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="load-1"><p>现有数据集<code>students.txt</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">95001,李勇,男,20,CS</span><br><span class="line">95002,刘晨,女,19,IS</span><br><span class="line">95003,王敏,女,22,MA</span><br><span class="line">95004,张立,男,19,IS</span><br><span class="line">95005,刘刚,男,18,MA</span><br><span class="line">95006,孙庆,男,23,CS</span><br><span class="line">95007,易思玲,女,19,MA</span><br><span class="line">95008,李娜,女,18,CS</span><br><span class="line">95009,梦圆圆,女,18,MA</span><br><span class="line">95010,孔小涛,男,19,CS</span><br><span class="line">95011,包小柏,男,18,MA</span><br><span class="line">95012,孙花,女,20,CS</span><br><span class="line">95013,冯伟,男,21,CS</span><br><span class="line">95014,王小丽,女,19,CS</span><br><span class="line">95015,王君,男,18,MA</span><br><span class="line">95016,钱国,男,21,MA</span><br><span class="line">95017,王风娟,女,18,IS</span><br><span class="line">95018,王一,女,19,IS</span><br><span class="line">95019,邢小丽,女,19,IS</span><br><span class="line">95020,赵钱,男,21,IS</span><br><span class="line">95021,周二,男,17,MA</span><br><span class="line">95022,郑明,男,20,MA</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="load-2"><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> test;</span><br><span class="line"></span><br><span class="line">use test;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--建表student_local 用于演示从本地加载数据</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_local</span><br><span class="line">(</span><br><span class="line">    num <span class="type">int</span>,</span><br><span class="line">    name string,</span><br><span class="line">    sex string,</span><br><span class="line">    age <span class="type">int</span>,</span><br><span class="line">    dept string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--建表student_HDFS 用于演示从HDFS加载数据</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> student_HDFS</span><br><span class="line">(</span><br><span class="line">    num <span class="type">int</span>,</span><br><span class="line">    name string,</span><br><span class="line">    sex string,</span><br><span class="line">    age <span class="type">int</span>,</span><br><span class="line">    dept string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="load-3"><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--建议使用beeline客户端可以显示出加载过程日志信息</span></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--从本地加载数据数据位于（node1）本地文件系统本质是hadoop fs -put上传操作</span></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/root/data/students.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> student_local;</span><br><span class="line"></span><br><span class="line"><span class="comment">--从HDFS加载数据数据位于HDFS文件系统根目录下本质是hadoop fs -mv 移动操作</span></span><br><span class="line"><span class="comment">--先把数据上传到HDFS上 hadoop fs -put /root/data/students.txt /</span></span><br><span class="line">load data inpath <span class="string">&#x27;/students.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> student_HDFS;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="load-4"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-23-01-49-04.png" alt="从本地文件系统加载"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-23-01-49-32.png" alt="从HDFS上加载"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h4 id="Hive-SQL-DML-Insert插入数据"><a href="#Hive-SQL-DML-Insert插入数据" class="headerlink" title="Hive SQL-DML-Insert插入数据"></a>Hive SQL-DML-Insert插入数据</h4><mark class="hl-label green">Insert语法功能</mark> 
<ul>
<li>Hive官方推荐加载数据的方式：<strong>清洗数据成为结构化文件，再使用<code>Load</code>语法加载数据到表中</strong>。这样的效率更高。</li>
<li>也可以使用<code>insert</code>语法把数据插入到指定的表中，最常用的配合是<strong>把查询返回的结果插入到另一张表中</strong>。</li>
</ul>
<h5 id="insert-select"><a href="#insert-select" class="headerlink" title="insert+select"></a>insert+select</h5><p><code>insert+select</code>表示：<strong>将后面查询返回的结果作为内容插入到指定表中</strong></p>
<ol>
<li>需要保证查询结果<strong>列的数目</strong>和需要插入数据表格的列数目<strong>一致</strong></li>
<li>如果查询出来的<strong>数据类型</strong>和插入表格对应的列数据类型不一致，将会进行转换，但是不能保证转换一定成功，转换失败的数据将会为NULL</li>
</ol>
<figure class="highlight sql"><figcaption><span>基本语法</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> tablename <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> other_tablename;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><figcaption><span>使用刚刚创建的表</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">use test;</span><br><span class="line"><span class="comment">--创建一张目标表只有两个字段</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_from_insert(sno <span class="type">int</span>,sname string);</span><br><span class="line"><span class="comment">--使用insert+select插入数据到新表中</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student_from_insert <span class="keyword">select</span> num,name <span class="keyword">from</span> student_local;</span><br><span class="line"><span class="comment">--验证</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span><span class="keyword">from</span> student_from_insert;</span><br></pre></td></tr></table></figure>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-23-02-13-31.png" alt="实际上是转化为mapreduce作业"></p>
<h3 id="Hive-SQL-DML语法之查询数据"><a href="#Hive-SQL-DML语法之查询数据" class="headerlink" title="Hive SQL DML语法之查询数据"></a>Hive SQL DML语法之查询数据</h3><h4 id="Select语法树"><a href="#Select语法树" class="headerlink" title="Select语法树"></a>Select语法树</h4><ul>
<li>从哪里查询取决于<code>FROM</code>关键字后面的table_reference，这是我们写查询SQL的首先要确定的事即你查询谁？</li>
<li>表名和列名不区分大小写。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> <span class="operator">|</span> <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[LIMIT [<span class="keyword">offset</span>,] <span class="keyword">rows</span>];</span><br></pre></td></tr></table></figure>
<h4 id="Demo——美国Covid-19新冠数据之select查询"><a href="#Demo——美国Covid-19新冠数据之select查询" class="headerlink" title="Demo——美国Covid-19新冠数据之select查询"></a>Demo——美国Covid-19新冠数据之select查询</h4><p>准备select语法测试环境，现有一份数据文件<code>《us-covid19-counties.dat》</code>，里面记录了2021-01-28美国各个县累计新冠确诊病例数和累计死亡病例数。</p>
<figure class="highlight plaintext"><figcaption><span>us-covid19-counties.dat</span><a href="/downloads/code/data/hive-test/us-covid19-counties.dat">view raw</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2021-01-28,Autauga,Alabama,01001,5554,69</span><br><span class="line">2021-01-28,Baldwin,Alabama,01003,17779,225</span><br><span class="line">2021-01-28,Barbour,Alabama,01005,1920,40</span><br><span class="line">2021-01-28,Bibb,Alabama,01007,2271,51</span><br><span class="line">2021-01-28,Blount,Alabama,01009,5612,98</span><br><span class="line">2021-01-28,Bullock,Alabama,01011,1079,29</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">建表加载数据</mark> 
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建表t_usa_covid19</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> t_usa_covid19;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t_usa_covid19(</span><br><span class="line">    count_date string comment &quot;日期&quot;,</span><br><span class="line">    county string comment &quot;郡&quot;,</span><br><span class="line">    state string comment &quot;洲&quot;,</span><br><span class="line">    fips <span class="type">int</span> comment &quot;编码&quot;,</span><br><span class="line">    cases <span class="type">int</span> comment &quot;病例&quot;,</span><br><span class="line">    deaths <span class="type">int</span> comment &quot;死亡病例&quot;</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;,&quot;;</span><br><span class="line"></span><br><span class="line"><span class="comment">--将源数据load加载到t_usa_covid19表对应的路径下</span></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/root/hivedata/us-covid19-counties.dat&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> t_usa_covid19;</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">（1）select_expr</mark> 
<p>select_expr表示检索查询返回的列，必须至少有一个select_expr。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--1、select_expr</span></span><br><span class="line"><span class="comment">--查询所有字段或者指定字段</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19;</span><br><span class="line"><span class="keyword">select</span> county, cases, deaths <span class="keyword">from</span> t_usa_covid19;</span><br><span class="line"><span class="comment">--查询当前数据库</span></span><br><span class="line"><span class="keyword">select</span> current_database(); <span class="comment">--省去from关键字</span></span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">（2）ALL、DISTINCT</mark> 
<p>用于指定查询返回结果中重复的行<strong>如何处理</strong>。</p>
<ol>
<li>如果没有给出这些选项，则默认值为<code>ALL</code>（返回所有匹配的行）。</li>
<li><strong>DISTINCT指定从结果集中删除重复的行</strong>。</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--2、ALL DISTINCT</span></span><br><span class="line"><span class="comment">--返回所有匹配的行</span></span><br><span class="line"><span class="keyword">select</span> state <span class="keyword">from</span> t_usa_covid19;</span><br><span class="line"><span class="comment">--相当于</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">all</span> state <span class="keyword">from</span> t_usa_covid19;</span><br><span class="line"><span class="comment">--返回所有匹配的行去除重复的结果</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span> state <span class="keyword">from</span> t_usa_covid19;</span><br><span class="line"><span class="comment">--多个字段distinct 整体（字段都一样）去重</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">distinct</span> county,state <span class="keyword">from</span> t_usa_covid19;</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">（3）WHERE</mark> 
<ul>
<li>WHERE后面是一个布尔表达式（结果要么为true，要么为false），用于<strong>查询过滤</strong>，当<strong>布尔表达式为true时，返回select后面expr表达式的结果</strong>，否则返回空。</li>
<li>在WHERE表达式中，可以使用Hive支持的任何函数和运算符，但聚合函数除外。</li>
<li>使用比较运算、逻辑运算<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-26-01-36-24.png" alt="使用比较运算、逻辑运算"></li>
<li>特殊条件（空值判断、<code>between</code>、<code>in</code>）<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-26-01-37-11.png" alt="特殊条件"></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--3、WHERE</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> <span class="number">1</span> <span class="operator">&gt;</span> <span class="number">2</span>; <span class="comment">--1 &gt; 2 返回false</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> <span class="number">1</span> <span class="operator">=</span> <span class="number">1</span>; <span class="comment">--1 = 1 返回true</span></span><br><span class="line"><span class="comment">--找出来自于California州的疫情数据</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> state <span class="operator">=</span> &quot;California&quot;;</span><br><span class="line"><span class="comment">--where条件中使用函数找出州名字母长度超过10位的有哪些</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> length(state) <span class="operator">&gt;</span><span class="number">10</span> ;</span><br><span class="line"><span class="comment">--注意：where条件中不能使用聚合函数</span></span><br><span class="line"><span class="comment">--报错SemanticException:Not yet supported place for UDAF ‘sum&#x27;</span></span><br><span class="line"><span class="comment">--聚合函数要使用它的前提是结果集已经确定。</span></span><br><span class="line"><span class="comment">--而where子句还处于“确定”结果集的过程中，因而不能使用聚合函数。</span></span><br><span class="line"><span class="comment">--select state,sum(deaths) from t_usa_covid19 where sum(deaths) &gt;100 group by state; --报错</span></span><br><span class="line"><span class="comment">--可以使用Having实现</span></span><br><span class="line"><span class="keyword">select</span> state,<span class="built_in">sum</span>(deaths) <span class="keyword">from</span> t_usa_covid19 <span class="keyword">group</span> <span class="keyword">by</span> state <span class="keyword">having</span> <span class="built_in">sum</span>(deaths) <span class="operator">&gt;</span> <span class="number">100</span>;</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">（4）聚合操作</mark> 
<ul>
<li>SQL中拥有很多可用于计数和计算的内建函数，其使用的语法是：<code>SELECT function(列) FROM 表</code>。</li>
<li>聚合（Aggregate）操作函数，如：Count、Sum、Max、Min、Avg等函数。</li>
<li>聚合函数的最大特点是<strong>不管原始数据有多少行记录，经过聚合操作只返回一条数据，这一条数据就是聚合的结果</strong>。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-26-23-38-42.png" alt="聚合"></li>
<li>常见的聚合操作函数如下</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>聚合函数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>AVG(column)</code></td>
<td>返回某列的平均值</td>
</tr>
<tr>
<td><code>COUNT(column)</code></td>
<td>返回某列的行数（不包括NULL值）</td>
</tr>
<tr>
<td><code>COUNT(*)</code></td>
<td>返回被选行数</td>
</tr>
<tr>
<td><code>MAX(column)</code></td>
<td>返回某列的最高值</td>
</tr>
<tr>
<td><code>MIN(column)</code></td>
<td>返回某列的最低值</td>
</tr>
<tr>
<td><code>SUM(column)</code></td>
<td>返回某列的总和</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--4、聚合操作</span></span><br><span class="line"><span class="comment">--统计美国总共有多少个县county</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(county) <span class="keyword">from</span> t_usa_covid19;</span><br><span class="line"><span class="comment">--统计美国加州有多少个县</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(county) <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> state <span class="operator">=</span> &quot;California&quot;;</span><br><span class="line"><span class="comment">--统计德州总死亡病例数</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">sum</span>(deaths) <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> state <span class="operator">=</span> &quot;Texas&quot;;</span><br><span class="line"><span class="comment">--统计出美国最高确诊病例数是哪个县</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">max</span>(cases) <span class="keyword">from</span> t_usa_covid19;</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">（5）GROUP BY</mark> 
<ul>
<li><code>GROUP BY</code>语句用于结合聚合函数，<strong>根据一个或多个列对结果集进行分组</strong>；</li>
<li>如果没有<code>group by</code>语法，则表中的所有行数据当成一组。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-00-09-13.png" alt="group by"></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--5、GROUP BY</span></span><br><span class="line"><span class="comment">--根据state州进行分组统计每个州有多少个县county</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(county) <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">group</span> <span class="keyword">by</span> state;</span><br><span class="line"><span class="comment">--想看一下统计的结果是属于哪一个州的</span></span><br><span class="line"><span class="keyword">select</span> state,<span class="built_in">count</span>(county) <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">group</span> <span class="keyword">by</span> state;</span><br><span class="line"><span class="comment">--再想看一下每个县的死亡病例数，我们猜想很简单呀把deaths字段加上返回真实情况如何呢？</span></span><br><span class="line"><span class="keyword">select</span> state,<span class="built_in">count</span>(county),deaths <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">group</span> <span class="keyword">by</span> state;</span><br><span class="line"><span class="comment">--很尴尬sql报错了org.apache.hadoop.hive.ql.parse.SemanticException:Line 1:27 Expression not in GROUP BY key &#x27;deaths&#x27;--为什么会报错？？group by的语法限制</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">group</span> <span class="keyword">by</span> county; <span class="comment">--报错</span></span><br><span class="line"><span class="comment">--结论：出现在GROUP BY中select_expr的字段：要么是GROUP BY分组的字段；要么是被聚合函数应用的字段。</span></span><br><span class="line"><span class="comment">--deaths不是分组字段报错</span></span><br><span class="line"><span class="comment">--state是分组字段可以直接出现在select_expr中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--被聚合函数应用</span></span><br><span class="line"><span class="keyword">select</span> state,<span class="built_in">count</span>(county),<span class="built_in">sum</span>(deaths) <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">group</span> <span class="keyword">by</span> state;</span><br></pre></td></tr></table></figure>
<mark class="hl-label red">GROUP BY语法限制</mark> 
<p>出现在GROUP BY中select_expr的字段：<strong>要么是GROUP BY分组的字段；要么是被聚合函数应用的字段</strong>。<br>原因：<strong>避免出现一个字段多个值的歧义</strong>。</p>
<ol>
<li>分组字段出现select_expr中，一定没有歧义，因为就是基于该字段分组的，同一组中必相同；</li>
<li>被聚合函数应用的字段，也没歧义，因为聚合函数的本质就是多进一出，最终返回一个结果。</li>
<li>基于category进行分组，相同颜色的分在同一组中。在select_expr中，如果出现category字段，则没有问题，因为同一组中category值一样，但是返回day就有问题了，day的结果不一样。</li>
</ol>
<mark class="hl-label pink">（6）HAVING</mark> 
<ul>
<li>在SQL中增加HAVING子句原因是，<strong>WHERE关键字无法与聚合函数一起使用</strong>。</li>
<li><strong>HAVING子句可以让我们筛选分组后的各组数据,并且可以在Having中使用聚合函数，因为此时where，group by已经执行结束，结果集已经确定</strong>。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--6、having</span></span><br><span class="line"><span class="comment">--统计2021-01-28死亡病例数大于10000的州</span></span><br><span class="line"><span class="keyword">select</span> state,<span class="built_in">sum</span>(deaths) <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">and</span> <span class="built_in">sum</span>(deaths) <span class="operator">&gt;</span><span class="number">10000</span> <span class="keyword">group</span> <span class="keyword">by</span> state; <span class="comment">--报错</span></span><br><span class="line"><span class="comment">--where语句中不能使用聚合函数语法报错</span></span><br><span class="line"><span class="comment">--先where分组前过滤，再进行group by分组，分组后每个分组结果集确定再使用having过滤</span></span><br><span class="line"><span class="keyword">select</span> state,<span class="built_in">sum</span>(deaths) <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">group</span> <span class="keyword">by</span> state <span class="keyword">having</span> <span class="built_in">sum</span>(deaths) <span class="operator">&gt;</span> <span class="number">10000</span>;</span><br><span class="line"><span class="comment">--这样写更好即在group by的时候聚合函数已经作用得出结果having直接引用结果过滤不需要再单独计算一次了</span></span><br><span class="line"><span class="keyword">select</span> state,<span class="built_in">sum</span>(deaths) <span class="keyword">as</span> cnts <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">group</span> <span class="keyword">by</span> state <span class="keyword">having</span> cnts<span class="operator">&gt;</span> <span class="number">10000</span>;</span><br></pre></td></tr></table></figure>
<mark class="hl-label red">HAVING与WHERE区别</mark> 
<ul>
<li><code>having</code>是在分组后对数据进行过滤</li>
<li><code>where</code>是在分组前对数据进行过滤</li>
<li><code>having</code>后面可以使用聚合函数</li>
<li><code>where</code>后面不可以使用聚合函数</li>
</ul>
<mark class="hl-label pink">（7）ORDER BY</mark> 
<ul>
<li>ORDER BY 语句用于<strong>根据指定的列对结果集进行排序</strong>。</li>
<li>ORDER BY 语句<strong>默认按照升序</strong>（<code>ASC</code>）对记录进行排序。如果您希望按照降序对记录进行排序，可以使用<code>DESC</code>关键字</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--7、order by</span></span><br><span class="line"><span class="comment">--根据确诊病例数升序排序查询返回结果</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">order</span> <span class="keyword">by</span> cases;</span><br><span class="line"><span class="comment">--不写排序规则默认就是asc升序</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">order</span> <span class="keyword">by</span> cases <span class="keyword">asc</span>;</span><br><span class="line"><span class="comment">--根据死亡病例数倒序排序查询返回加州每个县的结果</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> state <span class="operator">=</span> &quot;California&quot; <span class="keyword">order</span> <span class="keyword">by</span> cases <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">（8）LIMIT</mark> 
<ul>
<li>LIMIT用<strong>于限制SELECT语句返回的行数</strong>。</li>
<li>LIMIT接受一个或两个数字参数，这两个参数都必须是<strong>非负整数</strong>常量。</li>
<li>第一个参数指定要返回的第一行的偏移量（从Hive 2.0.0开始），第二个参数指定要返回的最大行数。当给出单个参数时，它代表最大行数，并且偏移量默认为0。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--8、limit</span></span><br><span class="line"><span class="comment">--没有限制返回2021.1.28 加州的所有记录</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">and</span> state <span class="operator">=</span>&quot;California&quot;;</span><br><span class="line"><span class="comment">--返回结果集的前5条</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">and</span> state <span class="operator">=</span>&quot;California&quot; limit <span class="number">5</span>;</span><br><span class="line"><span class="comment">--返回结果集从第3行开始共3行</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">and</span> state <span class="operator">=</span>&quot;California&quot; limit <span class="number">2</span>,<span class="number">3</span>;</span><br><span class="line"><span class="comment">--注意第一个参数偏移量是从0开始的</span></span><br></pre></td></tr></table></figure>
<mark class="hl-label red">执行顺序</mark> 
<p>在查询过程中执行顺序：<code>from &gt; where &gt; group（含聚合）&gt; having &gt;order &gt; select</code>；</p>
<ol>
<li>聚合语句(sum,min,max,avg,count)要比having子句优先执行</li>
<li>where子句在查询过程中执行优先级别优先于聚合语句(sum,min,max,avg,count)</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--执行顺序</span></span><br><span class="line"><span class="keyword">select</span> state,<span class="built_in">sum</span>(deaths) <span class="keyword">as</span> cnts <span class="keyword">from</span> t_usa_covid19 <span class="keyword">where</span> count_date <span class="operator">=</span> &quot;2021-01-28&quot; <span class="keyword">group</span> <span class="keyword">by</span> state <span class="keyword">having</span> cnts<span class="operator">&gt;</span> <span class="number">10000</span> limit <span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<h3 id="Hive-SQL-Join关联查询"><a href="#Hive-SQL-Join关联查询" class="headerlink" title="Hive SQL Join关联查询"></a>Hive SQL Join关联查询</h3><h4 id="Hive-Join语法规则"><a href="#Hive-Join语法规则" class="headerlink" title="Hive Join语法规则"></a>Hive Join语法规则</h4><ul>
<li>根据数据库的<strong>三范式设计</strong>要求和日常工作习惯来说，我们通常不会设计一张大表把所有类型的数据都放在一起，而是<strong>不同类型的数据设计不同的表</strong>存储。</li>
<li>比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-01-11-52.png" alt="订单表与客户表"></li>
<li>在这种情况下，有时需要基于多张表查询才能得到最终完整的结果；</li>
<li><code>join</code>语法的出现是用于<strong>根据两个或多个表中的列之间的关系，从这些表中共同组合查询数据</strong>。</li>
</ul>
<h4 id="Hive主要的Join方式"><a href="#Hive主要的Join方式" class="headerlink" title="Hive主要的Join方式"></a>Hive主要的Join方式</h4><p>在Hive中，使用最多，最重要的两种join分别是：</p>
<ul>
<li><code>inner join</code>（内连接）</li>
<li><code>left join</code>（左连接）</li>
</ul>
<mark class="hl-label pink">join语法</mark> 
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">join_table:table_reference [<span class="keyword">INNER</span>] <span class="keyword">JOIN</span> table_factor [join_condition] <span class="operator">|</span> table_reference &#123;<span class="keyword">LEFT</span>&#125; [<span class="keyword">OUTER</span>] <span class="keyword">JOIN</span> table_reference join_conditionjoin_condition:<span class="keyword">ON</span> expression</span><br></pre></td></tr></table></figure>
<ul>
<li>reference：是join查询中使用的表名。</li>
<li>table_factor：与table_reference相同,是联接查询中使用的表名。</li>
<li>join_condition：join查询关联的条件，如果在两个以上的表上需要连接，则使用<code>AND</code>关键字。</li>
</ul>
<h4 id="Demo——join的使用"><a href="#Demo——join的使用" class="headerlink" title="Demo——join的使用"></a>Demo——join的使用</h4><mark class="hl-label pink">join查询数据环境准备</mark> 
<p>现有<code>employee.txt</code> <code>employee_address.txt</code> <code>employee_connection.txt</code>三份数据<br><div class="tabs" id="employee"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#employee-1">employee.txt</button></li><li class="tab"><button type="button" data-href="#employee-2">employee_address.txt</button></li><li class="tab"><button type="button" data-href="#employee-3">employee_connection.txt</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="employee-1"><p>员工表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1201,gopal,manager,50000,TP</span><br><span class="line">1202,manisha,cto,50000,TP</span><br><span class="line">1203,khalil,dev,30000,AC</span><br><span class="line">1204,prasanth,dev,30000,AC</span><br><span class="line">1206,kranthi,admin,20000,TP</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="employee-2"><p>员工住址信息表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1201,288A,vgiri,jublee</span><br><span class="line">1202,108I,aoc,ny</span><br><span class="line">1204,144Z,pgutta,hyd</span><br><span class="line">1206,78B,old city,la</span><br><span class="line">1207,720X,hitec,ny</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="employee-3"><p>员工联系方式表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1201,2356742,gopal@tp.com</span><br><span class="line">1203,1661663,manisha@tp.com</span><br><span class="line">1204,8887776,khalil@ac.com</span><br><span class="line">1205,9988774,prasanth@ac.com</span><br><span class="line">1206,1231231,kranthi@tp.com</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div></p>
<mark class="hl-label pink">建表</mark> 
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--table1: 员工表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employee</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> comment &quot;员工编号&quot;,</span><br><span class="line">    name string comment &quot;名字&quot;,</span><br><span class="line">    deg string comment &quot;职位&quot;,</span><br><span class="line">    salary <span class="type">int</span> comment &quot;薪水&quot;,</span><br><span class="line">    dept string comment &quot;部门&quot;</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"><span class="comment">--table2:员工家庭住址信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employee_address</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> comment &quot;员工编号&quot;,</span><br><span class="line">    hno string comment &quot;地址编号&quot;,</span><br><span class="line">    street string comment &quot;街道&quot;,</span><br><span class="line">    city string comment &quot;城市&quot;</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"><span class="comment">--table3:员工联系方式信息表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employee_connection</span><br><span class="line">(</span><br><span class="line">    id <span class="type">int</span> comment &quot;员工编号&quot;,</span><br><span class="line">    phno string comment &quot;手机号&quot;,</span><br><span class="line">    email string comment &quot;电子邮件&quot;</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">数据加载</mark> 
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--加载数据到表中</span></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/root/hivedata/employee.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> employee;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/root/hivedata/employee_address.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> employee_address;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/root/hivedata/employee_connection.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> employee_connection;</span><br></pre></td></tr></table></figure>
<h5 id="inner-join内连接"><a href="#inner-join内连接" class="headerlink" title="inner join内连接"></a>inner join内连接</h5><ul>
<li><strong>内连接</strong>是最常见的一种连接，它也被称为普通连接，其中inner可以省略：<code>inner join == join</code> ；</li>
<li>只有进行连接的两个表中都存在与连接条件相匹配的数据才会被留下来。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-11-30-54.png" alt="inner join内连接"></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--1、inner join</span></span><br><span class="line"><span class="keyword">select</span> e.id,e.name,e_a.city,e_a.street</span><br><span class="line"><span class="keyword">from</span> employee e <span class="keyword">inner</span> <span class="keyword">join</span> employee_address e_a</span><br><span class="line"><span class="keyword">on</span> e.id <span class="operator">=</span>e_a.id;</span><br><span class="line"><span class="comment">--等价于inner join=join</span></span><br><span class="line"><span class="keyword">select</span> e.id,e.name,e_a.city,e_a.street</span><br><span class="line"><span class="keyword">from</span> employee e <span class="keyword">join</span> employee_address e_a</span><br><span class="line"><span class="keyword">on</span> e.id <span class="operator">=</span>e_a.id;</span><br><span class="line"></span><br><span class="line"><span class="comment">--等价于隐式连接表示法</span></span><br><span class="line"><span class="keyword">select</span> e.id,e.name,e_a.city,e_a.street <span class="keyword">from</span> employee e , employee_address e_a <span class="keyword">where</span> e.id <span class="operator">=</span>e_a.id;</span><br></pre></td></tr></table></figure>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-11-35-44.png" alt="inner join内连接"></p>
<h5 id="left-join左连接"><a href="#left-join左连接" class="headerlink" title="left join左连接"></a>left join左连接</h5><ul>
<li>left join中文叫做是<strong>左外连接</strong>(Left Outer Join)或者左连接，其中outer可以省略，left outer join是早期的写法。</li>
<li>left join的核心就在于left左。左指的是join关键字左边的表，简称左表。</li>
<li>通俗解释：<strong>join时以左表的全部数据为准，右边与之关联；左表数据全部返回，右表关联上的显示返回，关联不上的显示null返回</strong>。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-11-37-32.png" alt="left join左连接"></li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--2、left join</span></span><br><span class="line"><span class="keyword">select</span> e.id,e.name,e_conn.phno,e_conn.email</span><br><span class="line"><span class="keyword">from</span> employee e <span class="keyword">left</span> <span class="keyword">join</span> employee_connection e_conn</span><br><span class="line"><span class="keyword">on</span> e.id <span class="operator">=</span>e_conn.id;</span><br><span class="line"><span class="comment">--等价于left outer join</span></span><br><span class="line"><span class="keyword">select</span> e.id,e.name,e_conn.phno,e_conn.email</span><br><span class="line"><span class="keyword">from</span> employee e <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> employee_connection e_conn</span><br><span class="line"><span class="keyword">on</span> e.id <span class="operator">=</span>e_conn.id;</span><br></pre></td></tr></table></figure>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-11-40-09.png" alt="left join左连接"></p>
<h3 id="Hive-SQL中的函数使用"><a href="#Hive-SQL中的函数使用" class="headerlink" title="Hive SQL中的函数使用"></a>Hive SQL中的函数使用</h3><h4 id="Hive函数概述及分类标准"><a href="#Hive函数概述及分类标准" class="headerlink" title="Hive函数概述及分类标准"></a>Hive函数概述及分类标准</h4><mark class="hl-label pink">概述</mark> 
<p>Hive内建了不少函数，用于满足用户不同使用需求，提高SQL编写效率：</p>
<ol>
<li>使用<code>show functions</code>查看当下可用的所有函数；</li>
<li>通过<code>describe function extended funcname</code>来查看函数的使用方式。</li>
</ol>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-11-45-50.png" alt="show functions的使用"></p>
<mark class="hl-label pink">分类标准</mark> 
<p>Hive的函数分为两大类：<strong>内置函数</strong>（Built-in Functions）、<strong>用户定义函数UDF</strong>（User-Defined Functions）：</p>
<ul>
<li>内置函数可分为：数值类型函数、日期类型函数、字符串类型函数、集合函数、条件函数等；</li>
<li>用户定义函数根据输入输出的行数可分为3类：<code>UDF</code>、<code>UDAF</code>、<code>UDTF</code>。</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-11-49-21.png" alt="分类"></p>
<mark class="hl-label pink">用户定义函数UDF分类标准</mark> 
<p>根据函数输入输出的行数：</p>
<ul>
<li><code>UDF</code>（User-Defined-Function）普通函数，一进一出</li>
<li><code>UDAF</code>（User-Defined Aggregation Function）聚合函数，多进一出</li>
<li><code>UDTF</code>（User-Defined Table-Generating Functions）表生成函数，一进多出</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-11-51-08.png" alt="用户定义函数UDF分类标准"></p>
<mark class="hl-label pink">UDF分类标准扩大化</mark> 
<ul>
<li>UDF分类标准本来针对的是用户自己编写开发实现的函数。<strong>UDF分类标准可以扩大到Hive的所有函数中：包括内置函数和用户自定义函数</strong>。</li>
<li>因为不管是什么类型的函数，一定满足于输入输出的要求，那么从输入几行和输出几行上来划分没有任何问题。</li>
<li>千万不要被UD（User-Defined）这两个字母所迷惑，照成视野的狭隘。</li>
<li>比如Hive官方文档中，针对聚合函数的标准就是内置的UDAF类型。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-11-52-47.png" alt="Hive官方文档"></li>
</ul>
<h4 id="Hive常用的内置函数"><a href="#Hive常用的内置函数" class="headerlink" title="Hive常用的内置函数"></a>Hive常用的内置函数</h4><p><strong>内置函数</strong>（build-in）指的是Hive开发实现好，直接可以使用的函数,也叫做内建函数。<br>官方文档地址：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</a><br>内置函数根据应用归类整体可以分为8大种类型。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-05-27-13-05-41.png" alt="8大类型"></p>
<div class="tabs" id="function"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#function-1">String Functions字符串函数</button></li><li class="tab"><button type="button" data-href="#function-2">Date Functions日期函数</button></li><li class="tab"><button type="button" data-href="#function-3">Mathematical Functions数学函数</button></li><li class="tab"><button type="button" data-href="#function-4">Conditional Functions条件函数</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="function-1"><ul>
<li>字符串长度函数：length</li>
<li>字符串反转函数：reverse</li>
<li>字符串连接函数：concat</li>
<li>带分隔符字符串连接函数：concat_ws</li>
<li>字符串截取函数：substr,substring</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">------------String Functions 字符串函数------------</span></span><br><span class="line"><span class="keyword">select</span> length(&quot;hsq&quot;);</span><br><span class="line"><span class="keyword">select</span> reverse(&quot;hsq&quot;);</span><br><span class="line"><span class="keyword">select</span> concat(&quot;angela&quot;,&quot;baby&quot;);</span><br><span class="line"><span class="comment">--带分隔符字符串连接函数：concat_ws(separator, [string | array(string)]+)</span></span><br><span class="line"><span class="keyword">select</span> concat_ws(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;www&#x27;</span>, <span class="keyword">array</span>(<span class="string">&#x27;huangshiqing&#x27;</span>, <span class="string">&#x27;website&#x27;</span>));</span><br><span class="line"><span class="comment">--字符串截取函数：substr(str, pos[, len]) 或者substring(str, pos[, len])</span></span><br><span class="line"><span class="keyword">select</span> substr(&quot;angelababy&quot;,<span class="number">-2</span>);</span><br><span class="line"><span class="comment">--pos是从1开始的索引，如果为负数则倒着数</span></span><br><span class="line"><span class="keyword">select</span> substr(&quot;angelababy&quot;,<span class="number">2</span>,<span class="number">2</span>);</span><br><span class="line"><span class="comment">--分割字符串函数: split(str, regex)</span></span><br><span class="line"><span class="keyword">select</span> split(<span class="string">&#x27;apache hive&#x27;</span>, <span class="string">&#x27; &#x27;</span>);</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="function-2"><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-----------Date Functions 日期函数-----------------</span></span><br><span class="line"><span class="comment">-- --获取当前日期: current_date</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">current_date</span>();</span><br><span class="line"><span class="comment">--获取当前UNIX时间戳函数: unix_timestamp</span></span><br><span class="line"><span class="keyword">select</span> unix_timestamp();</span><br><span class="line"><span class="comment">--日期转UNIX时间戳函数: unix_timestamp</span></span><br><span class="line"><span class="keyword">select</span> unix_timestamp(&quot;2011-12-07 13:01:03&quot;);</span><br><span class="line"><span class="comment">--指定格式日期转UNIX时间戳函数: unix_timestamp</span></span><br><span class="line"><span class="keyword">select</span> unix_timestamp(<span class="string">&#x27;20111207 13:01:03&#x27;</span>,<span class="string">&#x27;yyyyMMdd HH:mm:ss&#x27;</span>);</span><br><span class="line"><span class="comment">--UNIX时间戳转日期函数: from_unixtime</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1618238391</span>);</span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">0</span>, <span class="string">&#x27;yyyy-MM-dd HH:mm:ss&#x27;</span>);</span><br><span class="line"><span class="comment">--日期比较函数: datediff 日期格式要求&#x27;yyyy-MM-dd HH:mm:ss&#x27; or &#x27;yyyy-MM-dd&#x27;</span></span><br><span class="line"><span class="keyword">select</span> datediff(<span class="string">&#x27;2012-12-08&#x27;</span>,<span class="string">&#x27;2012-05-09&#x27;</span>);</span><br><span class="line"><span class="comment">--日期增加函数: date_add</span></span><br><span class="line"><span class="keyword">select</span> date_add(<span class="string">&#x27;2012-02-28&#x27;</span>,<span class="number">10</span>);</span><br><span class="line"><span class="comment">--日期减少函数: date_sub</span></span><br><span class="line"><span class="keyword">select</span> date_sub(<span class="string">&#x27;2012-01-1&#x27;</span>,<span class="number">10</span>);</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="function-3"><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">----Mathematical Functions 数学函数---------------</span></span><br><span class="line"><span class="comment">-- 取整函数: round 返回double类型的整数值部分（遵循四舍五入）</span></span><br><span class="line"><span class="keyword">select</span> round(<span class="number">3.1415926</span>);</span><br><span class="line"><span class="comment">--指定精度取整函数: round(double a, int d) 返回指定精度d的double类型</span></span><br><span class="line"><span class="keyword">select</span> round(<span class="number">3.1415926</span>,<span class="number">4</span>);</span><br><span class="line"><span class="comment">--取随机数函数: rand 每次执行都不一样返回一个0到1范围内的随机数</span></span><br><span class="line"><span class="keyword">select</span> rand();</span><br><span class="line"><span class="comment">--指定种子取随机数函数: rand(int seed) 得到一个稳定的随机数序列</span></span><br><span class="line"><span class="keyword">select</span> rand(<span class="number">3</span>);</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="function-4"><p>主要用于<strong>条件判断</strong>、<strong>逻辑判断</strong>转换这样的场合<br><code>student.txt</code>学生表见<a href="#Hive-SQL-DML-Load加载数据">Load语法Demo</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">use test;</span><br><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"><span class="comment">-----Conditional Functions 条件函数------------------</span></span><br><span class="line"><span class="comment">--使用之前课程创建好的student表数据</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student_local limit <span class="number">3</span>;</span><br><span class="line"><span class="comment">--if条件判断: if(boolean testCondition, T valueTrue, T valueFalseOrNull)</span></span><br><span class="line"><span class="keyword">select</span> if(<span class="number">1</span><span class="operator">=</span><span class="number">2</span>,<span class="number">100</span>,<span class="number">200</span>);</span><br><span class="line"><span class="keyword">select</span> if(sex <span class="operator">=</span><span class="string">&#x27;男&#x27;</span>,<span class="string">&#x27;M&#x27;</span>,<span class="string">&#x27;W&#x27;</span>) <span class="keyword">from</span> student_local limit <span class="number">3</span>;</span><br><span class="line"><span class="comment">--空值转换函数: nvl(T value, T default_value).为空转换</span></span><br><span class="line"><span class="keyword">select</span> nvl(&quot;allen&quot;,&quot;hsq&quot;);</span><br><span class="line"><span class="keyword">select</span> nvl(<span class="keyword">null</span>,&quot;hsq&quot;);</span><br><span class="line"><span class="comment">--条件转换函数: CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">case</span> <span class="number">100</span> <span class="keyword">when</span> <span class="number">50</span> <span class="keyword">then</span> <span class="string">&#x27;tom&#x27;</span> <span class="keyword">when</span> <span class="number">100</span> <span class="keyword">then</span> <span class="string">&#x27;mary&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;tim&#x27;</span> <span class="keyword">end</span>;</span><br><span class="line"><span class="keyword">select</span> name,<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;男&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;male&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;female&#x27;</span> <span class="keyword">end</span> <span class="keyword">from</span> student_local limit <span class="number">3</span>;</span><br></pre></td></tr></table></figure><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h2 id="Hadoop生态综合案例"><a href="#Hadoop生态综合案例" class="headerlink" title="Hadoop生态综合案例"></a>Hadoop生态综合案例</h2><a href="/2023/05/27/HadoopDemo/" title="Hadoop生态综合案例">Hadoop生态综合案例</a>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://www.huangshiqing.website">Shiqing Huang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.huangshiqing.website/2022/11/12/HadoopBase/">https://www.huangshiqing.website/2022/11/12/HadoopBase/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.huangshiqing.website" target="_blank">Ofra Serendipity</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/Hive/">Hive</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src= "/img/loading.gif" data-lazy-src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "/img/loading.gif" data-lazy-src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/11/07/TypeScript/"><img class="prev-cover" src= "/img/loading.gif" data-lazy-src="/2022/11/07/TypeScript/TypeScript.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">TypeScript</div></div></a></div><div class="next-post pull-right"><a href="/2022/11/20/AboutNPM/"><img class="next-cover" src= "/img/loading.gif" data-lazy-src="/2022/11/20/AboutNPM/npm.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">npm——Node 包管理器</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/05/27/HadoopDemo/" title="Hadoop生态综合案例"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-27</div><div class="title">Hadoop生态综合案例</div></div></a></div><div><a href="/2022/10/25/HadoopClusterBuilding3-3-4/" title="Hadoop 3.3.4 集群搭建"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-25</div><div class="title">Hadoop 3.3.4 集群搭建</div></div></a></div><div><a href="/2023/05/08/kafka3-4-0/" title="Kafka"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://pic2.imgdb.cn/item/645a5d440d2dde57777f32d1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-08</div><div class="title">Kafka</div></div></a></div><div><a href="/2023/03/23/hadoop-java/" title="Java与Hadoop"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://pic.imgdb.cn/item/642157d0a682492fcc8d0a06.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-23</div><div class="title">Java与Hadoop</div></div></a></div><div><a href="/2023/03/15/hbase-install/" title="HBase三种搭建方式"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://pic.imgdb.cn/item/64120132ebf10e5d533c6ba9.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-15</div><div class="title">HBase三种搭建方式</div></div></a></div><div><a href="/2023/03/27/zookeeper3-8-1/" title="Zookeeper部署"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://pic.imgdb.cn/item/642156eda682492fcc8b5b4e.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-27</div><div class="title">Zookeeper部署</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Waline</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "/img/loading.gif" data-lazy-src="/img/avatar002.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Shiqing Huang</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">16</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cmwlvip"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/cmwlvip" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://gitee.com/cmwlvip" target="_blank" title="Gitee"><i class="fab fa-git"></i></a><a class="social-icon" href="mailto:2689050828@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AF%BC%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">大数据导论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%81%E4%B8%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E5%90%91"><span class="toc-number">1.1.</span> <span class="toc-text">企业数据分析方向</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.1.1.</span> <span class="toc-text">数据是什么</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A6%82%E4%BD%95%E4%BA%A7%E7%94%9F"><span class="toc-number">1.1.2.</span> <span class="toc-text">数据如何产生</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E6%9E%90%E6%96%B9%E5%90%91"><span class="toc-number">1.1.3.</span> <span class="toc-text">分析方向</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.2.</span> <span class="toc-text">数据分析基本步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step1%EF%BC%9A%E6%98%8E%E7%A1%AE%E5%88%86%E6%9E%90%E7%9B%AE%E7%9A%84%E5%92%8C%E6%80%9D%E8%B7%AF"><span class="toc-number">1.2.1.</span> <span class="toc-text">Step1：明确分析目的和思路</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step2%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="toc-number">1.2.2.</span> <span class="toc-text">Step2：数据收集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step3%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">1.2.3.</span> <span class="toc-text">Step3：数据处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step4%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-number">1.2.4.</span> <span class="toc-text">Step4：数据分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step5%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%B1%95%E7%8E%B0"><span class="toc-number">1.2.5.</span> <span class="toc-text">Step5：数据展现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step6%EF%BC%9A%E6%8A%A5%E5%91%8A%E6%92%B0%E5%86%99"><span class="toc-number">1.2.6.</span> <span class="toc-text">Step6：报告撰写</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E4%BB%A3"><span class="toc-number">1.3.</span> <span class="toc-text">大数据时代</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E4%BB%A3%E8%83%8C%E6%99%AF"><span class="toc-number">1.3.1.</span> <span class="toc-text">大数据时代背景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89"><span class="toc-number">1.3.2.</span> <span class="toc-text">大数据定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE5V%E7%89%B9%E5%BE%81"><span class="toc-number">1.3.3.</span> <span class="toc-text">大数据5V特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.3.4.</span> <span class="toc-text">应用场景</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E9%9B%86%E7%BE%A4"><span class="toc-number">1.4.</span> <span class="toc-text">分布式与集群</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8"><span class="toc-number">1.4.1.</span> <span class="toc-text">应用</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Apache-Hadoop%E3%80%81HDFS"><span class="toc-number">2.</span> <span class="toc-text">Apache Hadoop、HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Apache-Hadoop%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">Apache Hadoop概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E4%BB%8B%E7%BB%8D%E3%80%81%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2%E3%80%81%E7%8E%B0%E7%8A%B6"><span class="toc-number">2.1.1.</span> <span class="toc-text">Hadoop介绍、发展简史、现状</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">Hadoop介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">Hadoop发展简史</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E7%8E%B0%E7%8A%B6"><span class="toc-number">2.1.1.3.</span> <span class="toc-text">Hadoop现状</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E7%89%B9%E6%80%A7%E4%BC%98%E7%82%B9%E3%80%81%E5%9B%BD%E5%86%85%E5%A4%96%E5%BA%94%E7%94%A8"><span class="toc-number">2.1.2.</span> <span class="toc-text">Hadoop特性优点、国内外应用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E7%89%B9%E6%80%A7%E4%BC%98%E7%82%B9"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">Hadoop特性优点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E5%9B%BD%E5%A4%96%E5%BA%94%E7%94%A8"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">Hadoop国外应用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E5%9B%BD%E5%86%85%E5%BA%94%E7%94%A8"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">Hadoop国内应用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC%E3%80%81%E6%9E%B6%E6%9E%84%E5%8F%98%E8%BF%81"><span class="toc-number">2.1.3.</span> <span class="toc-text">Hadoop发行版本、架构变迁</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">Hadoop发行版本</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E6%9E%B6%E6%9E%84%E5%8F%98%E8%BF%81%EF%BC%881-0-2-0%E5%8F%98%E8%BF%81%EF%BC%89"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">Hadoop架构变迁（1.0-2.0变迁）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E6%9E%B6%E6%9E%84%E5%8F%98%E8%BF%81%EF%BC%883-0%E6%96%B0%E7%89%88%E6%9C%AC%EF%BC%89"><span class="toc-number">2.1.3.3.</span> <span class="toc-text">Hadoop架构变迁（3.0新版本）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Apache-Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-number">2.2.</span> <span class="toc-text">Apache Hadoop集群搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E9%9B%86%E7%BE%A4%E7%AE%80%E4%BB%8B"><span class="toc-number">2.2.1.</span> <span class="toc-text">Hadoop集群简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F-%E5%88%86%E5%B8%83%E5%BC%8F-%E5%AE%89%E8%A3%85%EF%BC%88Cluster-mode%EF%BC%89"><span class="toc-number">2.2.2.</span> <span class="toc-text">Hadoop集群模式(分布式)安装（Cluster mode）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">Hadoop源码编译</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step1-%E9%9B%86%E7%BE%A4%E8%A7%92%E8%89%B2%E8%A7%84%E5%88%92"><span class="toc-number">2.2.2.2.</span> <span class="toc-text">Step1:集群角色规划</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">2.2.2.3.</span> <span class="toc-text">Step2:服务器基础环境准备</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step3-%E4%B8%8A%E4%BC%A0%E5%AE%89%E8%A3%85%E5%8C%85%E3%80%81%E8%A7%A3%E5%8E%8B%E5%AE%89%E8%A3%85%E5%8C%85"><span class="toc-number">2.2.2.4.</span> <span class="toc-text">Step3:上传安装包、解压安装包</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step4-Hadoop%E5%AE%89%E8%A3%85%E5%8C%85%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="toc-number">2.2.2.5.</span> <span class="toc-text">Step4:Hadoop安装包目录结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%A6%82%E8%BF%B0"><span class="toc-number">2.2.2.6.</span> <span class="toc-text">配置文件概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-hadoop-env-sh"><span class="toc-number">2.2.2.7.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 hadoop-env.sh</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-core-site-xml"><span class="toc-number">2.2.2.8.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 core-site.xml</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6hdfs-site-xml"><span class="toc-number">2.2.2.9.</span> <span class="toc-text">Step5:编辑Hadoop配置文件hdfs-site.xml</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-mapred-site-xml"><span class="toc-number">2.2.2.10.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 mapred-site.xml</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-yarn-site-xml"><span class="toc-number">2.2.2.11.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 yarn-site.xml</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-workers"><span class="toc-number">2.2.2.12.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 workers</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step6-%E5%88%86%E5%8F%91%E5%90%8C%E6%AD%A5%E5%AE%89%E8%A3%85%E5%8C%85"><span class="toc-number">2.2.2.13.</span> <span class="toc-text">Step6:分发同步安装包</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step7-%E9%85%8D%E7%BD%AEHadoop%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">2.2.2.14.</span> <span class="toc-text">Step7:配置Hadoop环境变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step8-NameNode-format%EF%BC%88%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%93%8D%E4%BD%9C%EF%BC%89"><span class="toc-number">2.2.2.15.</span> <span class="toc-text">Step8:NameNode format（格式化操作）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E9%9B%86%E7%BE%A4%E5%90%AF%E5%81%9C%E5%91%BD%E4%BB%A4%E3%80%81Web-UI"><span class="toc-number">2.2.3.</span> <span class="toc-text">Hadoop集群启停命令、Web UI</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E9%80%90%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%81%9C"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">手动逐个进程启停</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#shell%E8%84%9A%E6%9C%AC%E4%B8%80%E9%94%AE%E5%90%AF%E5%81%9C"><span class="toc-number">2.2.3.2.</span> <span class="toc-text">shell脚本一键启停</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81%E3%80%81%E6%97%A5%E5%BF%97%E6%9F%A5%E7%9C%8B"><span class="toc-number">2.2.3.3.</span> <span class="toc-text">进程状态、日志查看</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS%E9%9B%86%E7%BE%A4web%E7%95%8C%E9%9D%A2"><span class="toc-number">2.2.3.4.</span> <span class="toc-text">HDFS集群web界面</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E9%9B%86%E7%BE%A4web%E7%95%8C%E9%9D%A2"><span class="toc-number">2.2.3.5.</span> <span class="toc-text">YARN集群web界面</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-number">2.2.4.</span> <span class="toc-text">Hadoop初体验</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS-%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-number">2.2.4.1.</span> <span class="toc-text">HDFS 初体验</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#shell%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.4.1.1.</span> <span class="toc-text">shell命令操作</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Web-UI%E9%A1%B5%E9%9D%A2%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.4.1.2.</span> <span class="toc-text">Web UI页面操作</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce-YARN%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-number">2.2.4.2.</span> <span class="toc-text">MapReduce+YARN初体验</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80"><span class="toc-number">2.3.</span> <span class="toc-text">HDFS分布式文件系统基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-number">2.3.1.</span> <span class="toc-text">文件系统、分布式文件系统</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AE%9A%E4%B9%89"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">文件系统定义</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">传统常见的文件系统</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E3%80%81%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.3.1.3.</span> <span class="toc-text">数据、元数据</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">2.3.1.3.1.</span> <span class="toc-text">数据</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.3.1.3.2.</span> <span class="toc-text">元数据</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">2.3.1.4.</span> <span class="toc-text">海量数据存储遇到的问题</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%A0%B8%E5%BF%83%E5%B1%9E%E6%80%A7%E5%8F%8A%E5%8A%9F%E8%83%BD%E5%90%AB%E4%B9%89"><span class="toc-number">2.3.2.</span> <span class="toc-text">分布式存储系统的核心属性及功能含义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">分布式存储的优点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E8%AE%B0%E5%BD%95%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">元数据记录的功能</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E5%9D%97%E5%AD%98%E5%82%A8%E5%A5%BD%E5%A4%84"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">分块存储好处</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">2.3.2.4.</span> <span class="toc-text">副本机制的作用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E7%AE%80%E4%BB%8B"><span class="toc-number">2.3.3.</span> <span class="toc-text">HDFS简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E8%B5%B7%E6%BA%90%E5%8F%91%E5%B1%95%E3%80%81%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87"><span class="toc-number">2.3.4.</span> <span class="toc-text">HDFS起源发展、设计目标</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS%E8%B5%B7%E6%BA%90%E5%8F%91%E5%B1%95"><span class="toc-number">2.3.4.1.</span> <span class="toc-text">HDFS起源发展</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87"><span class="toc-number">2.3.4.2.</span> <span class="toc-text">HDFS设计目标</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">2.3.5.</span> <span class="toc-text">HDFS应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E9%87%8D%E8%A6%81%E7%89%B9%E6%80%A7"><span class="toc-number">2.3.6.</span> <span class="toc-text">HDFS重要特性</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%A6%82%E8%BF%B0"><span class="toc-number">2.3.6.1.</span> <span class="toc-text">整体概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84"><span class="toc-number">2.3.6.2.</span> <span class="toc-text">（1）主从架构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%88%86%E5%9D%97%E5%AD%98%E5%82%A8"><span class="toc-number">2.3.6.3.</span> <span class="toc-text">（2）分块存储</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6"><span class="toc-number">2.3.6.4.</span> <span class="toc-text">（3）副本机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86"><span class="toc-number">2.3.6.5.</span> <span class="toc-text">（4）元数据管理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%885%EF%BC%89namespace"><span class="toc-number">2.3.6.6.</span> <span class="toc-text">（5）namespace</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%886%EF%BC%89%E6%95%B0%E6%8D%AE%E5%9D%97%E5%AD%98%E5%82%A8"><span class="toc-number">2.3.6.7.</span> <span class="toc-text">（6）数据块存储</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-shell%E6%93%8D%E4%BD%9C"><span class="toc-number">2.4.</span> <span class="toc-text">HDFS shell操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS-shell%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%A7%A3%E9%87%8A%E8%AF%B4%E6%98%8E"><span class="toc-number">2.4.1.</span> <span class="toc-text">HDFS shell命令行解释说明</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%8D%8F%E8%AE%AE"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">文件系统协议</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8C%BA%E5%88%AB"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">区别</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="toc-number">2.4.1.3.</span> <span class="toc-text">参数说明</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS-shell%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">2.4.2.</span> <span class="toc-text">HDFS shell命令行常用操作</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-number">2.4.2.1.</span> <span class="toc-text">创建文件夹</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%8C%87%E5%AE%9A%E7%9B%AE%E5%BD%95%E4%B8%8B%E5%86%85%E5%AE%B9"><span class="toc-number">2.4.2.2.</span> <span class="toc-text">查看指定目录下内容</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%88%B0HDFS%E6%8C%87%E5%AE%9A%E7%9B%AE%E5%BD%95%E4%B8%8B"><span class="toc-number">2.4.2.3.</span> <span class="toc-text">上传文件到HDFS指定目录下</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8BHDFS%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9"><span class="toc-number">2.4.2.4.</span> <span class="toc-text">查看HDFS文件内容</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BDHDFS%E6%96%87%E4%BB%B6"><span class="toc-number">2.4.2.5.</span> <span class="toc-text">下载HDFS文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8B%B7%E8%B4%9DHDFS%E6%96%87%E4%BB%B6"><span class="toc-number">2.4.2.6.</span> <span class="toc-text">拷贝HDFS文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%BD%E5%8A%A0%E6%95%B0%E6%8D%AE%E5%88%B0HDFS%E6%96%87%E4%BB%B6%E4%B8%AD"><span class="toc-number">2.4.2.7.</span> <span class="toc-text">追加数据到HDFS文件中</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS%E6%95%B0%E6%8D%AE%E7%A7%BB%E5%8A%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">2.4.2.8.</span> <span class="toc-text">HDFS数据移动操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS-shell%E5%85%B6%E4%BB%96%E5%91%BD%E4%BB%A4"><span class="toc-number">2.4.2.9.</span> <span class="toc-text">HDFS shell其他命令</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E4%B8%8E%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.</span> <span class="toc-text">HDFS工作流程与机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E9%9B%86%E7%BE%A4%E8%A7%92%E8%89%B2%E4%B8%8E%E8%81%8C%E8%B4%A3"><span class="toc-number">2.5.1.</span> <span class="toc-text">HDFS集群角色与职责</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%98%E6%96%B9%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-number">2.5.1.1.</span> <span class="toc-text">官方架构图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BB%E8%A7%92%E8%89%B2%EF%BC%9Anamenode"><span class="toc-number">2.5.1.2.</span> <span class="toc-text">主角色：namenode</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%8E%E8%A7%92%E8%89%B2%EF%BC%9Adatanode"><span class="toc-number">2.5.1.3.</span> <span class="toc-text">从角色：datanode</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BB%E8%A7%92%E8%89%B2%E8%BE%85%E5%8A%A9%E8%A7%92%E8%89%B2%EF%BC%9Asecondarynamenode"><span class="toc-number">2.5.1.4.</span> <span class="toc-text">主角色辅助角色：secondarynamenode</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#namenode%E8%81%8C%E8%B4%A3"><span class="toc-number">2.5.1.5.</span> <span class="toc-text">namenode职责</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#datanode%E8%81%8C%E8%B4%A3"><span class="toc-number">2.5.1.6.</span> <span class="toc-text">datanode职责</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%EF%BC%88%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%EF%BC%89"><span class="toc-number">2.5.2.</span> <span class="toc-text">HDFS写数据流程（上传文件）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%86%99%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">2.5.2.1.</span> <span class="toc-text">写数据完整流程图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E2%80%94Pipeline%E7%AE%A1%E9%81%93"><span class="toc-number">2.5.2.2.</span> <span class="toc-text">核心概念—Pipeline管道</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E2%80%94ACK%E5%BA%94%E7%AD%94%E5%93%8D%E5%BA%94"><span class="toc-number">2.5.2.3.</span> <span class="toc-text">核心概念—ACK应答响应</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E2%80%94%E9%BB%98%E8%AE%A43%E5%89%AF%E6%9C%AC%E5%AD%98%E5%82%A8%E7%AD%96%E7%95%A5"><span class="toc-number">2.5.2.4.</span> <span class="toc-text">核心概念—默认3副本存储策略</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%86%99%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B%E5%9B%BE%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0"><span class="toc-number">2.5.2.5.</span> <span class="toc-text">写数据完整流程图文字描述</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%EF%BC%88%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%EF%BC%89"><span class="toc-number">2.5.3.</span> <span class="toc-text">HDFS读数据流程（下载文件）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%BB%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">2.5.3.1.</span> <span class="toc-text">读数据完整流程图</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop-MapReduce%E4%B8%8EHadoop-YARN"><span class="toc-number">3.</span> <span class="toc-text">Hadoop MapReduce与Hadoop YARN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop-MapReduce"><span class="toc-number">3.1.</span> <span class="toc-text">Hadoop MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MapReduce%E6%80%9D%E6%83%B3"><span class="toc-number">3.1.1.</span> <span class="toc-text">MapReduce思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-MapReduce%E8%AE%BE%E8%AE%A1%E6%9E%84%E6%80%9D"><span class="toc-number">3.1.2.</span> <span class="toc-text">Hadoop MapReduce设计构思</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%A6%82%E4%BD%95%E5%AF%B9%E4%BB%98%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%9C%BA%E6%99%AF"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">（1）如何对付大数据处理场景</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%9E%84%E5%BB%BA%E6%8A%BD%E8%B1%A1%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">（2）构建抽象编程模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E7%BB%9F%E4%B8%80%E6%9E%B6%E6%9E%84%E3%80%81%E9%9A%90%E8%97%8F%E5%BA%95%E5%B1%82%E7%BB%86%E8%8A%82"><span class="toc-number">3.1.2.3.</span> <span class="toc-text">（3）统一架构、隐藏底层细节</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-MapReduce%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.1.3.</span> <span class="toc-text">Hadoop MapReduce介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A6%82%E5%BF%B5"><span class="toc-number">3.1.3.1.</span> <span class="toc-text">分布式计算概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop-MapReduce%E6%A6%82%E8%BF%B0"><span class="toc-number">3.1.3.2.</span> <span class="toc-text">Hadoop MapReduce概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E4%BA%A7%E7%94%9F%E8%83%8C%E6%99%AF"><span class="toc-number">3.1.3.3.</span> <span class="toc-text">MapReduce产生背景</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E7%89%B9%E7%82%B9"><span class="toc-number">3.1.3.4.</span> <span class="toc-text">MapReduce特点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">3.1.3.5.</span> <span class="toc-text">MapReduce局限性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E5%AE%9E%E4%BE%8B%E8%BF%9B%E7%A8%8B"><span class="toc-number">3.1.3.6.</span> <span class="toc-text">MapReduce实例进程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E9%98%B6%E6%AE%B5%E7%BB%84%E6%88%90"><span class="toc-number">3.1.3.7.</span> <span class="toc-text">MapReduce阶段组成</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.1.3.8.</span> <span class="toc-text">MapReduce数据类型</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-MapReduce%E5%AE%98%E6%96%B9%E7%A4%BA%E4%BE%8B"><span class="toc-number">3.1.4.</span> <span class="toc-text">Hadoop MapReduce官方示例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E7%A4%BA%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="toc-number">3.1.4.1.</span> <span class="toc-text">MapReduce示例说明</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E5%9C%86%E5%91%A8%E7%8E%87%CF%80%EF%BC%88PI%EF%BC%89%E7%9A%84%E5%80%BC"><span class="toc-number">3.1.4.2.</span> <span class="toc-text">评估圆周率π（PI）的值</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Monte-Carlo%E6%96%B9%E6%B3%95"><span class="toc-number">3.1.4.2.1.</span> <span class="toc-text">Monte Carlo方法</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E5%9C%86%E5%91%A8%E7%8E%87%CF%80%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">3.1.4.2.2.</span> <span class="toc-text">评估圆周率π参数设置</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#wordcount%E5%8D%95%E8%AF%8D%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1"><span class="toc-number">3.1.4.3.</span> <span class="toc-text">wordcount单词词频统计</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#WordCount%E6%A6%82%E8%BF%B0"><span class="toc-number">3.1.4.3.1.</span> <span class="toc-text">WordCount概述</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#WordCount%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF"><span class="toc-number">3.1.4.3.2.</span> <span class="toc-text">WordCount编程实现思路</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#WordCount%E7%A8%8B%E5%BA%8F%E6%8F%90%E4%BA%A4"><span class="toc-number">3.1.4.3.3.</span> <span class="toc-text">WordCount程序提交</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Map%E9%98%B6%E6%AE%B5%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.5.</span> <span class="toc-text">Map阶段执行流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#WordCount%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">3.1.5.1.</span> <span class="toc-text">WordCount执行流程图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E6%95%B4%E4%BD%93%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">3.1.5.2.</span> <span class="toc-text">MapReduce整体执行流程图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Map%E9%98%B6%E6%AE%B5%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="toc-number">3.1.5.3.</span> <span class="toc-text">Map阶段执行过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reduce%E9%98%B6%E6%AE%B5%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.6.</span> <span class="toc-text">Reduce阶段执行流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Shuffle%E6%9C%BA%E5%88%B6"><span class="toc-number">3.1.7.</span> <span class="toc-text">Shuffle机制</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#shuffle%E6%A6%82%E5%BF%B5"><span class="toc-number">3.1.7.1.</span> <span class="toc-text">shuffle概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Map%E7%AB%AFShuffle"><span class="toc-number">3.1.7.2.</span> <span class="toc-text">Map端Shuffle</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Reducer%E7%AB%AFshuffle"><span class="toc-number">3.1.7.3.</span> <span class="toc-text">Reducer端shuffle</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#shuffle%E6%9C%BA%E5%88%B6%E5%BC%8A%E7%AB%AF"><span class="toc-number">3.1.7.4.</span> <span class="toc-text">shuffle机制弊端</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop-YARN"><span class="toc-number">3.2.</span> <span class="toc-text">Hadoop YARN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-YARN%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.2.1.</span> <span class="toc-text">Hadoop YARN介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E7%AE%80%E4%BB%8B"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">YARN简介</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E5%8A%9F%E8%83%BD%E8%AF%B4%E6%98%8E"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">YARN功能说明</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E6%A6%82%E8%BF%B0"><span class="toc-number">3.2.1.3.</span> <span class="toc-text">YARN概述</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-YARN%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%84%E4%BB%B6"><span class="toc-number">3.2.2.</span> <span class="toc-text">Hadoop YARN架构、组件</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E5%AE%98%E6%96%B9%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">YARN官方架构图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%98%E6%96%B9%E6%9E%B6%E6%9E%84%E5%9B%BE%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">3.2.2.2.</span> <span class="toc-text">官方架构图中出现的概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN3%E5%A4%A7%E7%BB%84%E4%BB%B6"><span class="toc-number">3.2.2.3.</span> <span class="toc-text">YARN3大组件</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E6%8F%90%E4%BA%A4YARN%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.3.</span> <span class="toc-text">程序提交YARN交互流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.3.1.</span> <span class="toc-text">核心交互流程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B%E6%A6%82%E8%BF%B0"><span class="toc-number">3.2.3.2.</span> <span class="toc-text">交互流程概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MR%E6%8F%90%E4%BA%A4YARN%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.3.3.</span> <span class="toc-text">MR提交YARN交互流程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#YARN%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8Scheduler"><span class="toc-number">3.2.4.</span> <span class="toc-text">YARN资源调度器Scheduler</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MR%E7%A8%8B%E5%BA%8F%E6%8F%90%E4%BA%A4YARN%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.4.1.</span> <span class="toc-text">MR程序提交YARN交互流程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6"><span class="toc-number">3.2.4.2.</span> <span class="toc-text">如何理解资源调度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E5%99%A8%E7%AD%96%E7%95%A5"><span class="toc-number">3.2.4.3.</span> <span class="toc-text">调度器策略</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%9F%BA%E7%A1%80%E4%B8%8EApache-Hive%E5%85%A5%E9%97%A8"><span class="toc-number">4.</span> <span class="toc-text">数据仓库基础与Apache Hive入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.</span> <span class="toc-text">数据仓库基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.1.</span> <span class="toc-text">数据仓库概念</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E4%BB%93%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.1.1.</span> <span class="toc-text">数仓概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E4%BB%93%E4%B8%93%E6%B3%A8%E5%88%86%E6%9E%90"><span class="toc-number">4.1.1.2.</span> <span class="toc-text">数仓专注分析</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E6%A1%88%E4%BE%8B%EF%BC%9A%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%BA%E4%BD%95%E8%80%8C%E6%9D%A5"><span class="toc-number">4.1.2.</span> <span class="toc-text">场景案例：数据仓库为何而来</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#OLTP%E7%8E%AF%E5%A2%83%E5%BC%80%E5%B1%95%E5%88%86%E6%9E%90%E5%8F%AF%E8%A1%8C%E5%90%97%EF%BC%9F"><span class="toc-number">4.1.2.1.</span> <span class="toc-text">OLTP环境开展分析可行吗？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E9%9D%A2%E4%B8%96"><span class="toc-number">4.1.2.2.</span> <span class="toc-text">数据仓库面世</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E6%9E%84%E5%BB%BA"><span class="toc-number">4.1.2.3.</span> <span class="toc-text">数据仓库的构建</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%BB%E8%A6%81%E7%89%B9%E5%BE%81"><span class="toc-number">4.1.3.</span> <span class="toc-text">数据仓库主要特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%BB%E6%B5%81%E5%BC%80%E5%8F%91%E8%AF%AD%E8%A8%80%E2%80%94SQL"><span class="toc-number">4.1.4.</span> <span class="toc-text">数据仓库主流开发语言—SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E8%AF%AD%E8%A8%80%E6%A6%82%E8%BF%B0"><span class="toc-number">4.1.4.1.</span> <span class="toc-text">数仓开发语言概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SQL%E8%AF%AD%E8%A8%80%E4%BB%8B%E7%BB%8D"><span class="toc-number">4.1.4.2.</span> <span class="toc-text">SQL语言介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E4%BB%93%E4%B8%8ESQL"><span class="toc-number">4.1.4.3.</span> <span class="toc-text">数仓与SQL</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE"><span class="toc-number">4.1.4.4.</span> <span class="toc-text">结构化数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%8C%E7%BB%B4%E8%A1%A8%E7%BB%93%E6%9E%84"><span class="toc-number">4.1.4.5.</span> <span class="toc-text">二维表结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SQL%E8%AF%AD%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-number">4.1.4.6.</span> <span class="toc-text">SQL语法分类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Apache-Hive%E5%85%A5%E9%97%A8"><span class="toc-number">4.2.</span> <span class="toc-text">Apache Hive入门</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E6%A6%82%E8%BF%B0"><span class="toc-number">4.2.1.</span> <span class="toc-text">Apache Hive概述</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFHive"><span class="toc-number">4.2.1.1.</span> <span class="toc-text">什么是Hive</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8Hive"><span class="toc-number">4.2.1.2.</span> <span class="toc-text">为什么使用Hive</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E5%92%8CHadoop%E5%85%B3%E7%B3%BB"><span class="toc-number">4.2.1.3.</span> <span class="toc-text">Hive和Hadoop关系</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1%EF%BC%9A%E5%A6%82%E4%BD%95%E6%A8%A1%E6%8B%9F%E5%AE%9E%E7%8E%B0Hive%E5%8A%9F%E8%83%BD"><span class="toc-number">4.2.2.</span> <span class="toc-text">场景设计：如何模拟实现Hive功能</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%98%A0%E5%B0%84%E4%BF%A1%E6%81%AF%E8%AE%B0%E5%BD%95"><span class="toc-number">4.2.2.1.</span> <span class="toc-text">映射信息记录</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SQL%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E3%80%81%E7%BC%96%E8%AF%91"><span class="toc-number">4.2.2.2.</span> <span class="toc-text">SQL语法解析、编译</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9Hive%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number">4.2.2.3.</span> <span class="toc-text">对Hive的理解</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%84%E4%BB%B6"><span class="toc-number">4.2.3.</span> <span class="toc-text">Apache Hive架构、组件</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-number">4.2.3.1.</span> <span class="toc-text">Hive架构图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E7%BB%84%E4%BB%B6"><span class="toc-number">4.2.3.2.</span> <span class="toc-text">Hive组件</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Apache-Hive%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-number">4.3.</span> <span class="toc-text">Apache Hive安装部署</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">4.3.1.</span> <span class="toc-text">Apache Hive元数据</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive-Metadata"><span class="toc-number">4.3.1.1.</span> <span class="toc-text">Hive Metadata</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#metastore%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F"><span class="toc-number">4.3.1.2.</span> <span class="toc-text">metastore配置方式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#metastore%E8%BF%9C%E7%A8%8B%E6%A8%A1%E5%BC%8F"><span class="toc-number">4.3.1.3.</span> <span class="toc-text">metastore远程模式</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98"><span class="toc-number">4.3.2.</span> <span class="toc-text">Apache Hive部署实战</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#metastore%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">4.3.2.1.</span> <span class="toc-text">metastore服务启动方式</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8"><span class="toc-number">4.3.3.</span> <span class="toc-text">Apache Hive客户端使用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89Hive%E8%87%AA%E5%B8%A6%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-number">4.3.3.1.</span> <span class="toc-text">（1）Hive自带客户端</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HiveServer2%E6%9C%8D%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">4.3.3.2.</span> <span class="toc-text">HiveServer2服务介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%92%8C%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">4.3.3.3.</span> <span class="toc-text">Hive客户端和服务的关系</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#bin-beeline%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8"><span class="toc-number">4.3.3.4.</span> <span class="toc-text">bin&#x2F;beeline客户端使用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-Hive%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-number">4.3.3.5.</span> <span class="toc-text">(2)Hive可视化工具客户端</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Hive%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7"><span class="toc-number">4.3.3.5.1.</span> <span class="toc-text">Hive可视化工具</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#DataGrip"><span class="toc-number">4.3.3.5.2.</span> <span class="toc-text">DataGrip</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8DataGrip%E8%BD%AF%E4%BB%B6%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5Hiveserver2%E6%9C%8D%E5%8A%A1"><span class="toc-number">4.3.3.6.</span> <span class="toc-text">使用DataGrip软件远程连接Hiveserver2服务</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive-SQL%E8%AF%AD%E8%A8%80%EF%BC%9ADDL%E5%BB%BA%E5%BA%93%E3%80%81%E5%BB%BA%E8%A1%A8"><span class="toc-number">4.4.</span> <span class="toc-text">Hive SQL语言：DDL建库、建表</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DDL"><span class="toc-number">4.4.1.</span> <span class="toc-text">DDL</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E6%80%BB%E8%A7%88"><span class="toc-number">4.4.1.1.</span> <span class="toc-text">Hive数据模型总览</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SQL%E4%B8%ADDDL%E8%AF%AD%E6%B3%95%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">4.4.1.2.</span> <span class="toc-text">SQL中DDL语法的作用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E4%B8%ADDDL%E8%AF%AD%E6%B3%95%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">4.4.1.3.</span> <span class="toc-text">Hive中DDL语法的使用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive-SQL%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E5%BB%BA%E5%BA%93"><span class="toc-number">4.4.2.</span> <span class="toc-text">Hive SQL之数据库与建库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive-SQL%E4%B9%8B%E8%A1%A8%E4%B8%8E%E5%BB%BA%E8%A1%A8"><span class="toc-number">4.4.3.</span> <span class="toc-text">Hive SQL之表与建表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Demo%E2%80%94%E2%80%94Hive%E5%BB%BA%E8%A1%A8%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%E7%BB%83%E4%B9%A0"><span class="toc-number">4.4.4.</span> <span class="toc-text">Demo——Hive建表基础语法练习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive-Show%E8%AF%AD%E6%B3%95"><span class="toc-number">4.5.</span> <span class="toc-text">Hive Show语法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#show%E8%AF%AD%E6%B3%95%E5%8A%9F%E8%83%BD"><span class="toc-number">4.5.1.</span> <span class="toc-text">show语法功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8show%E8%AF%AD%E5%8F%A5"><span class="toc-number">4.5.2.</span> <span class="toc-text">常用show语句</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E9%87%8Acomment%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E8%A7%A3%E5%86%B3"><span class="toc-number">4.6.</span> <span class="toc-text">注释comment中文乱码解决</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Apache-Hive-DML%E8%AF%AD%E5%8F%A5%E4%B8%8E%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text">Apache Hive DML语句与函数使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive-SQL-DML%E8%AF%AD%E6%B3%95%E4%B9%8B%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">5.1.</span> <span class="toc-text">Hive SQL DML语法之加载数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive-SQL-DML-Load%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">5.1.1.</span> <span class="toc-text">Hive SQL-DML-Load加载数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive-SQL-DML-Insert%E6%8F%92%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-number">5.1.2.</span> <span class="toc-text">Hive SQL-DML-Insert插入数据</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#insert-select"><span class="toc-number">5.1.2.1.</span> <span class="toc-text">insert+select</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive-SQL-DML%E8%AF%AD%E6%B3%95%E4%B9%8B%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE"><span class="toc-number">5.2.</span> <span class="toc-text">Hive SQL DML语法之查询数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Select%E8%AF%AD%E6%B3%95%E6%A0%91"><span class="toc-number">5.2.1.</span> <span class="toc-text">Select语法树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Demo%E2%80%94%E2%80%94%E7%BE%8E%E5%9B%BDCovid-19%E6%96%B0%E5%86%A0%E6%95%B0%E6%8D%AE%E4%B9%8Bselect%E6%9F%A5%E8%AF%A2"><span class="toc-number">5.2.2.</span> <span class="toc-text">Demo——美国Covid-19新冠数据之select查询</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive-SQL-Join%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2"><span class="toc-number">5.3.</span> <span class="toc-text">Hive SQL Join关联查询</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive-Join%E8%AF%AD%E6%B3%95%E8%A7%84%E5%88%99"><span class="toc-number">5.3.1.</span> <span class="toc-text">Hive Join语法规则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive%E4%B8%BB%E8%A6%81%E7%9A%84Join%E6%96%B9%E5%BC%8F"><span class="toc-number">5.3.2.</span> <span class="toc-text">Hive主要的Join方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Demo%E2%80%94%E2%80%94join%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">5.3.3.</span> <span class="toc-text">Demo——join的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#inner-join%E5%86%85%E8%BF%9E%E6%8E%A5"><span class="toc-number">5.3.3.1.</span> <span class="toc-text">inner join内连接</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#left-join%E5%B7%A6%E8%BF%9E%E6%8E%A5"><span class="toc-number">5.3.3.2.</span> <span class="toc-text">left join左连接</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive-SQL%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8"><span class="toc-number">5.4.</span> <span class="toc-text">Hive SQL中的函数使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive%E5%87%BD%E6%95%B0%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%88%86%E7%B1%BB%E6%A0%87%E5%87%86"><span class="toc-number">5.4.1.</span> <span class="toc-text">Hive函数概述及分类标准</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive%E5%B8%B8%E7%94%A8%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-number">5.4.2.</span> <span class="toc-text">Hive常用的内置函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E7%94%9F%E6%80%81%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B"><span class="toc-number">6.</span> <span class="toc-text">Hadoop生态综合案例</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/06/10/hello-world/" title="Hello World"><img src= "/img/loading.gif" data-lazy-src="https://pic1.imgdb.cn/item/63676da116f2c2beb11fa14a.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2024/06/10/hello-world/" title="Hello World">Hello World</a><time datetime="2024-06-10T11:15:39.470Z" title="发表于 2024-06-10 11:15:39">2024-06-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/10/virtualBox/" title="VirtualBox虚拟机磁盘扩容"><img src= "/img/loading.gif" data-lazy-src="https://pic.imgdb.cn/item/648456981ddac507cc049e1e.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="VirtualBox虚拟机磁盘扩容"/></a><div class="content"><a class="title" href="/2023/06/10/virtualBox/" title="VirtualBox虚拟机磁盘扩容">VirtualBox虚拟机磁盘扩容</a><time datetime="2023-06-10T19:35:23.000Z" title="发表于 2023-06-10 19:35:23">2023-06-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/02/dataStructure/" title="数据结构"><img src= "/img/loading.gif" data-lazy-src="https://pic.imgdb.cn/item/6484545f1ddac507cc022402.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构"/></a><div class="content"><a class="title" href="/2023/06/02/dataStructure/" title="数据结构">数据结构</a><time datetime="2023-06-02T21:12:25.000Z" title="发表于 2023-06-02 21:12:25">2023-06-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/27/HadoopDemo/" title="Hadoop生态综合案例"><img src= "/img/loading.gif" data-lazy-src="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hadoop生态综合案例"/></a><div class="content"><a class="title" href="/2023/05/27/HadoopDemo/" title="Hadoop生态综合案例">Hadoop生态综合案例</a><time datetime="2023-05-27T14:32:15.000Z" title="发表于 2023-05-27 14:32:15">2023-05-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/17/rdd/" title="RDD"><img src= "/img/loading.gif" data-lazy-src="https://pic.imgdb.cn/item/64219d12a682492fcc086ebb.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RDD"/></a><div class="content"><a class="title" href="/2023/05/17/rdd/" title="RDD">RDD</a><time datetime="2023-05-17T09:43:41.000Z" title="发表于 2023-05-17 09:43:41">2023-05-17</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic.imgdb.cn/item/63766f6616f2c2beb1356f73.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Shiqing Huang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.huangshiqing.website/',
      region: 'ap-shanghai',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo.huangshiqing.website/',
      region: 'ap-shanghai',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script><script>function loadWaline () {
  function insertCSS () {
    const link = document.createElement("link")
    link.rel = "stylesheet"
    link.href = "https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css"
    document.head.appendChild(link)
  }

  function initWaline () {
    const waline = Waline.init(Object.assign({
      el: '#waline-wrap',
      serverURL: 'https://waline.huangshiqing.website/',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      path: window.location.pathname,
      comment: false,
    }, null))
  }

  if (typeof Waline === 'function') initWaline()
  else {
    insertCSS()
    getScript('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js').then(initWaline)
  }
}

if ('Twikoo' === 'Waline' || !false) {
  if (false) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
  else setTimeout(loadWaline, 0)
} else {
  function loadOtherComment () {
    loadWaline()
  }
}</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://twikoo.huangshiqing.website/',
        region: 'ap-shanghai',
        pageSize: 6,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/movies/"]):not([href="/books/"]):not([href="/games/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>