<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Hadoop 生态 | Ofra Serendipity</title><meta name="author" content="Shiqing Huang"><meta name="copyright" content="Shiqing Huang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="HDFS shell 命令行常用操作  大数据导论企业数据分析方向数据是什么 数据是指对客观事件进行记录并可以鉴别的符号，是对客观事物的性质、状态以及相互关系等进行记载的物理符号或这些物理符号的组合，它是可识别的、抽象的符号。 它不仅指狭义上的数字，还可以是具有一定意义的文字、字母、数字符号的组合、图形、图像、视频、音频等，也是客观事物的属性、数量、位置及其相互关系的抽象表示。例如，“0、1、2…">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop 生态">
<meta property="og:url" content="https://cmwlvip.github.io/2022/11/12/HadoopBase/index.html">
<meta property="og:site_name" content="Ofra Serendipity">
<meta property="og:description" content="HDFS shell 命令行常用操作  大数据导论企业数据分析方向数据是什么 数据是指对客观事件进行记录并可以鉴别的符号，是对客观事物的性质、状态以及相互关系等进行记载的物理符号或这些物理符号的组合，它是可识别的、抽象的符号。 它不仅指狭义上的数字，还可以是具有一定意义的文字、字母、数字符号的组合、图形、图像、视频、音频等，也是客观事物的属性、数量、位置及其相互关系的抽象表示。例如，“0、1、2…">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg">
<meta property="article:published_time" content="2022-11-12T13:31:16.000Z">
<meta property="article:modified_time" content="2023-01-12T16:00:00.000Z">
<meta property="article:author" content="Shiqing Huang">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg"><link rel="shortcut icon" href="/img/favicon01.png"><link rel="canonical" href="https://cmwlvip.github.io/2022/11/12/HadoopBase/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Shiqing Huang","link":"链接: ","source":"来源: Ofra Serendipity","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop 生态',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-01-13 00:00:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "/img/loading.gif" data-lazy-src="/img/avatar002.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-heart-pulse"></i><span> Fun</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book-open"></i><span> Book</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fab fa-steam"></i><span> Game</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.imgdb.cn/item/63766f6616f2c2beb1356f73.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ofra Serendipity</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-heart-pulse"></i><span> Fun</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book-open"></i><span> Book</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fab fa-steam"></i><span> Game</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Hadoop 生态</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-12T13:31:16.000Z" title="发表于 2022-11-12 21:31:16">2022-11-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-01-12T16:00:00.000Z" title="更新于 2023-01-13 00:00:00">2023-01-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Hadoop/">Hadoop</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">23.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>77分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Hadoop 生态"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><div class="note default modern"><p><a href="#HDFS-shell命令行常用操作">HDFS shell 命令行常用操作</a></p>
</div>
<h2 id="大数据导论"><a href="#大数据导论" class="headerlink" title="大数据导论"></a>大数据导论</h2><h3 id="企业数据分析方向"><a href="#企业数据分析方向" class="headerlink" title="企业数据分析方向"></a>企业数据分析方向</h3><h4 id="数据是什么"><a href="#数据是什么" class="headerlink" title="数据是什么"></a>数据是什么</h4><ul>
<li>数据是指对<strong>客观事件进行记录并可以鉴别的符号</strong>，是对客观事物的性质、状态以及相互关系等进行记载的物理符号或这些物理符号的组合，它是可识别的、抽象的符号。</li>
<li>它不仅指狭义上的<strong>数字</strong>，还可以是具有一定意义的<strong>文字、字母、数字符号的组合、图形、图像、视频、音频</strong>等，也是客观事物的属性、数量、位置及其相互关系的抽象表示。例如，“0、1、2…”、“阴、雨、下降”、“学生的档案记录、货物的运输情况”等都是数据。<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-02-20.jpg" style="height:300px">
</li>
</ul>
<h4 id="数据如何产生"><a href="#数据如何产生" class="headerlink" title="数据如何产生"></a>数据如何产生</h4><p>对客观事物的<strong>计量和记录产生</strong>数据<br><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-07-17.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-07-30.png" style="height:200px"></p>
<h4 id="分析方向"><a href="#分析方向" class="headerlink" title="分析方向"></a>分析方向</h4><p><strong>把隐藏在数据背后的信息集中和提炼出来，总结出所研究对象的内在规律，帮助管理者进行有效的判断和决策</strong>。</p>
<p>数据分析在企业日常经营分析中主要有三大方向：<br><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-21-20.png" alt="企业数据分析方向"></p>
</blockquote></p>
<ul>
<li><strong>现状分析</strong>（分析<strong>当下</strong>的数据）：现阶段的整体情况，各个部分的构成占比、发展、变动；</li>
<li><strong>原因分析</strong>（分析<strong>过去</strong>的数据）：某一现状为什么发生，确定原因，做出调整优化；</li>
<li><strong>预测分析</strong>（结合数据预测<strong>未来</strong>）：结合已有数据预测未来发展趋势。</li>
</ul>
<div class="tabs" id="分析方向"><ul class="nav-tabs"><li class="tab active"><button type="button" data-href="#分析方向-1">现状分析</button></li><li class="tab"><button type="button" data-href="#分析方向-2">原因分析</button></li><li class="tab"><button type="button" data-href="#分析方向-3">预测分析</button></li></ul><div class="tab-contents"><div class="tab-item-content active" id="分析方向-1"><p><strong>实时分析</strong>（Real Time Processing |<strong>Streaming</strong>）</p>
<p>面向当下，分析实时产生的数据；<br>所谓的实时是指从数据产生到数据分析到数据应用的时间间隔很短，可细分秒级、毫秒级。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-27-18.png" alt="实时分析"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分析方向-2"><p><strong>离线分析</strong>（<strong>Batch</strong> Processing）</p>
<p>面向过去，面向<strong>历史</strong>，分析已有的数据；<br>在时间维度明显成<strong>批次性变化</strong>。一周一分析(T+7)，一天一分析（T+1），所以也叫做<strong>批处理</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-29-41.png" alt="离线分析"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="分析方向-3"><p><strong>机器学习</strong>（<strong>Machine Learning</strong>）</p>
<p>基于历史数据和当下产生的实时数据预测未来发生的事情；<br>侧重于<strong>数学算法</strong>的运用，如分类、聚类、关联、预测。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-30-55.png" alt="机器学习"></p><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h3 id="数据分析基本步骤"><a href="#数据分析基本步骤" class="headerlink" title="数据分析基本步骤"></a>数据分析基本步骤</h3><p>数据分析步骤（流程）的重要性体现在：对<strong>如何开展数据分析提供了强有力的逻辑支撑</strong>;<br>张文霖在《数据分析六步曲》说，典型的数据分析应该包含以下几个步骤<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-33-05.png" alt="数据分析六步曲"></p>
<h4 id="Step1：明确分析目的和思路"><a href="#Step1：明确分析目的和思路" class="headerlink" title="Step1：明确分析目的和思路"></a>Step1：明确分析目的和思路</h4><ul>
<li>目的是整个分析流程的起点，为数据的收集、处理及分析提供清晰的指引方向；</li>
<li>思路是使<strong>分析框架体系化</strong>，比如先分析什么，后分析什么，使各分析点之间具有逻辑联系，保证分析维度的<strong>完整性</strong>，分析结果的<strong>有效性</strong>以及<strong>正确性</strong>，需要数据分析方法论进行支撑；</li>
<li>数据分析方法论是一些营销管理类相关理论，比如用户行为理论、<strong>PEST分析法</strong>、5W2H分析法等。</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-35-53.png" alt="PEST分析法"></p>
<h4 id="Step2：数据收集"><a href="#Step2：数据收集" class="headerlink" title="Step2：数据收集"></a>Step2：数据收集</h4><ul>
<li>数据<strong>从无到有</strong>的过程：比如传感器收集气象数据、埋点收集用户行为数据</li>
<li>数据<strong>传输搬运</strong>的过程：比如采集数据库数据到数据分析平台</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-37-13.png" alt="数据收集"></p>
<h4 id="Step3：数据处理"><a href="#Step3：数据处理" class="headerlink" title="Step3：数据处理"></a>Step3：数据处理</h4><ul>
<li>准确来说，应该称之为<strong>数据预处理</strong>。</li>
<li>数据预处理需要对收集到的数据进行加工整理，形成适合数据分析的样式，主要包括<strong>数据清洗、数据转化、数据提取、数据计算</strong>；</li>
<li>数据预处理可以保证数据的一致性和有效性，让数据变成干净规整的<strong>结构化数据</strong>。</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-38-59.png" style="height:300px">
<h4 id="Step4：数据分析"><a href="#Step4：数据分析" class="headerlink" title="Step4：数据分析"></a>Step4：数据分析</h4><ul>
<li>用适当的分析方法及分析工具，对处理过的数据进行分析，提取有价值的信息，形成有效结论的过程；</li>
<li>需要掌握各种<strong>数据分析方法</strong>，还要熟悉<strong>数据分析软件</strong>的操作；</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-42-20.png" style="height:300px">
<h4 id="Step5：数据展现"><a href="#Step5：数据展现" class="headerlink" title="Step5：数据展现"></a>Step5：数据展现</h4><ul>
<li>数据展现又称之为<strong>数据可视化</strong>，指的是<strong>分析结果图表展示</strong>，因为人类是视觉动物；</li>
<li>数据可视化（Data Visualization）属于数据应用的一种；</li>
<li>注意，<strong>数据分析的结果不是只有可视化展示</strong>，还可以继续数据挖掘（Data Mining）、即席查询（Ad Hoc）等。</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-42-09.png" style="height:300px">
<h4 id="Step6：报告撰写"><a href="#Step6：报告撰写" class="headerlink" title="Step6：报告撰写"></a>Step6：报告撰写</h4><ul>
<li>数据分析报告是对整个数据分析过程的一个总结与呈现</li>
<li>把数据分析的起因、过程、结果及建议完整地呈现出来，供决策者参考</li>
<li>需要有明确的结论，最好有建议或解决方案</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-14-44-29.png" style="height:400px">
<div class="note info modern"><p>数据分析</p>
<ul>
<li><strong>一切围绕着数据</strong></li>
<li>通俗描述：<strong>数据从哪里来、数据到哪里去</strong></li>
<li>核心步骤:采集、处理、分析、应用</li>
</ul>
</div>
<h3 id="大数据时代"><a href="#大数据时代" class="headerlink" title="大数据时代"></a>大数据时代</h3><div class="note info modern"><ul>
<li>解决海量数据<strong>储存</strong>问题</li>
<li>解决海量数据<strong>计算</strong>问题</li>
</ul>
</div>
<h4 id="大数据时代背景"><a href="#大数据时代背景" class="headerlink" title="大数据时代背景"></a>大数据时代背景</h4><ul>
<li>最早提出“<strong>大数据</strong>”<strong>时代</strong>到来的是全球知名咨询公司<strong>麦肯锡</strong>，其称：“数据，已经渗透到当今每一个行业和业务职能领域，成为重要的生产因素。人们对于海量数据的挖掘和运用，预示着新一波生产率增长和消费者盈余浪潮的到来。”</li>
<li>2019年，央视推出了国内首部大数据产业题材纪录片《大数据时代》，节目细致而生动地讲述了大数据技术在<strong>政府治理、民生服务、数据安全、工业转型、未来生活</strong>等方面给我们带来的改变和影响。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-29-35.png" alt="大数据时代"></li>
</ul>
<h4 id="大数据定义"><a href="#大数据定义" class="headerlink" title="大数据定义"></a>大数据定义</h4><ul>
<li><strong>大数据（big data）</strong>是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合；</li>
<li>是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-30-31.png" alt="big data"></li>
</ul>
<h4 id="大数据5V特征"><a href="#大数据5V特征" class="headerlink" title="大数据5V特征"></a>大数据5V特征</h4><p>5个V开头的单词，从5个方面准确、生动、形象的介绍了大数据特征。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-31-43.png" alt="大数据5V特征"></p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-32-24.png" alt="大数据5V特征"></p>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><ul>
<li><p><strong>电商领域</strong><br>精准广告位、个性化推荐、大数据杀熟</p>
</li>
<li><p><strong>传媒领域</strong><br>精准营销、猜你喜欢、交互推荐</p>
</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-36-52.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-37-00.png" style="height:200px">
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-41-40.png" style="height:200px">
<ul>
<li><p><strong>金融方面</strong><br>理财投资，通过对个人的信用评估，风险承担能力评估，集合众多理财产品、推荐响应的投资理财产品。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-46-35.png" alt="金融方面"></p>
</li>
<li><p><strong>交通领域</strong><br>拥堵预测、智能红绿灯、导航最优规划</p>
</li>
<li><p><strong>电信领域</strong><br>基站选址优化、舆情监控、客户用户画像</p>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-47-53.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-47-59.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-48-04.png" style="height:200px">
</li>
<li><p><strong>安防领域</strong><br>犯罪预防、天网监控</p>
</li>
<li><p><strong>医疗领域</strong><br>智慧医疗、疾病预防、病源追踪</p>
</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-50-27.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-50-32.png" style="height:200px">
<h3 id="分布式与集群"><a href="#分布式与集群" class="headerlink" title="分布式与集群"></a>分布式与集群</h3><p>分布式、集群是两个不同的概念，但口语中经常混淆二者。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-52-16.png" alt="分布式与集群"></p>
<ul>
<li>分布式、集群的共同点是：<strong>都是多台机器（服务器）组成的</strong>；</li>
<li>因此口语中混淆两者概念的时候都是：<strong>相对于单机来说的</strong>。</li>
</ul>
<img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-53-33.png" style="height:200px"><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-53-38.png" style="height:200px">
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>数据大爆炸，海量数据处理场景面临问题<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-19-15-57-22.png" alt="大数据背景下"></p>
<h2 id="Apache-Hadoop、HDFS"><a href="#Apache-Hadoop、HDFS" class="headerlink" title="Apache Hadoop、HDFS"></a>Apache Hadoop、HDFS</h2><h3 id="Apache-Hadoop概述"><a href="#Apache-Hadoop概述" class="headerlink" title="Apache Hadoop概述"></a>Apache Hadoop概述</h3><h4 id="Hadoop介绍、发展简史、现状"><a href="#Hadoop介绍、发展简史、现状" class="headerlink" title="Hadoop介绍、发展简史、现状"></a>Hadoop介绍、发展简史、现状</h4><h5 id="Hadoop介绍"><a href="#Hadoop介绍" class="headerlink" title="Hadoop介绍"></a>Hadoop介绍</h5><ul>
<li><mark class="hl-label blue">狭义上Hadoop指的是Apache软件基金会的一款开源软件。</mark> 
<p>用java语言实现，开源<br>允许用户使用<strong>简单的编程模型</strong>实现<strong>跨机器</strong>集群对海量数据进行<strong>分布式计算</strong>处理</p>
</li>
<li><mark class="hl-label blue">Hadoop核心组件</mark> 
<p>Hadoop HDFS（分布式文件<strong>存储</strong>系统）：解决海量数据存储<br>Hadoop YARN（集群<strong>资源管理</strong>和任务调度框架）：解决资源任务调度<br>Hadoop MapReduce（分布式<strong>计算</strong>框架）：解决海量数据计算</p>
</li>
<li><mark class="hl-label blue">官网</mark> 
<p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p>
</li>
<li><mark class="hl-label red">广义上Hadoop指的是围绕Hadoop打造的大数据生态圈。</mark> 
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-22-38-08.png" alt="广义Hadoop"></p>
</li>
</ul>
<h5 id="Hadoop发展简史"><a href="#Hadoop发展简史" class="headerlink" title="Hadoop发展简史"></a>Hadoop发展简史</h5><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-22-42-09.png" alt="Hadoop之父"></p>
</blockquote>
<ul>
<li><mark class="hl-label blue">Hadoop之父</mark> 
<p><strong>Doug Cutting</strong></p>
</li>
<li><mark class="hl-label blue">Hadoop起源于ApacheLucene子项目：Nutch</mark> 
<p>Nutch的设计目标是构建一个大型的全网搜索引擎。<br>遇到瓶颈：如何解决数十亿网页的存储和索引问题</p>
</li>
<li><mark class="hl-label red">Google三篇论文</mark> 
<p>《The Google file system》：谷歌分布式文件系统GFS<br>《MapReduce: Simplified Data Processing on Large Clusters》：谷歌分布式计算框架MapReduce<br>《Bigtable: A Distributed Storage System for Structured Data》：谷歌结构化数据存储系统</p>
</li>
</ul>
<h5 id="Hadoop现状"><a href="#Hadoop现状" class="headerlink" title="Hadoop现状"></a>Hadoop现状</h5><ul>
<li><strong>HDFS</strong>作为分布式文件存储系统，处在<strong>生态圈的底层与核心地位</strong>；</li>
<li><strong>YARN</strong>作为分布式通用的集群资源管理系统和任务调度平台，<strong>支撑各种计算引擎运行</strong>，保证了Hadoop地位；</li>
<li><strong>MapReduce</strong>作为大数据生态圈第一代分布式计算引擎，由于自身设计的模型所产生的弊端，导致企业一线<strong>几乎不再直接使用</strong>MapReduce进行编程处理，但是很多软件的底层依然在使用MapReduce引擎来处理数据。</li>
</ul>
<div class="note info modern"><ol>
<li>狭义上Hadoop指软件，广义上Hadoop指生态圈</li>
<li>Hadoop之父Doug Cutting</li>
<li>Hadoop起源于Nutch项目</li>
<li>受Google3篇论文启发</li>
<li>2008年开源给Apache软件基金会</li>
</ol>
</div>
<h4 id="Hadoop特性优点、国内外应用"><a href="#Hadoop特性优点、国内外应用" class="headerlink" title="Hadoop特性优点、国内外应用"></a>Hadoop特性优点、国内外应用</h4><h5 id="Hadoop特性优点"><a href="#Hadoop特性优点" class="headerlink" title="Hadoop特性优点"></a>Hadoop特性优点</h5><ol>
<li><p><strong>扩容能力</strong> scalability<br>Hadoop是在可用的计算机集群间分配数据并完成计算任务的，这些集群可方便灵活的方式扩展到数以千计的节点。</p>
</li>
<li><p><strong>成本低</strong> Economical<br>Hadoop集群允许通过部署普通廉价的机器组成集群来处理大数据，以至于成本很低。看重的是集群整体能力。</p>
</li>
<li><p><strong>效率高</strong> efficiency<br>通过<strong>并发数据</strong>，Hadoop可以在节点之间动态<strong>并行</strong>的移动数据，使得速度非常快。</p>
</li>
<li><p>可靠性 reliability<br>能自动维护数据的多份复制，并且在任务失败后能自动地重新部署（redeploy）计算任务。所以Hadoop的按位存储和处理数据的能力值得人们信赖。</p>
</li>
</ol>
<h5 id="Hadoop国外应用"><a href="#Hadoop国外应用" class="headerlink" title="Hadoop国外应用"></a>Hadoop国外应用</h5><ul>
<li><p>Yahoo<br>支持广告系统<br>用户行为分析<br>支持Web搜索<br>反垃圾邮件系统</p>
</li>
<li><p>Facebook<br>存储处理数据挖掘和日志统计<br>构建基于Hadoop数据仓库平台（Apache Hive来自FB）</p>
</li>
<li><p>IBM<br>蓝云基础设施构建<br>商业化Hadoop发行、解决方案支持</p>
</li>
</ul>
<h5 id="Hadoop国内应用"><a href="#Hadoop国内应用" class="headerlink" title="Hadoop国内应用"></a>Hadoop国内应用</h5><ul>
<li><p>百度<br>用户搜索表征的需求数据、阿拉丁爬虫数据存储<br>数据分析和挖掘竞价排名</p>
</li>
<li><p>阿里巴巴<br>为电子商务网络平台提供底层的基础计算和存储服务<br>交易数据、信用数据</p>
</li>
<li><p>腾讯<br>用户关系数据<br>基于Hadoop、Hive构建TDW（腾讯分布式数据仓库）</p>
</li>
<li><p>华为<br>对Hadoop的HA方案，以及HBase领域有深入研究</p>
</li>
</ul>
<div class="note info modern"><ul>
<li><p>Hadoop成功的魅力—<strong>通用性</strong><br>精准区分做什么和怎么做<br>做什么属于业务问题怎么做属于技术问题。<br>用户负责业务Hadoop负责技术</p>
</li>
<li><p>Hadoop成功的魅力—<strong>简单</strong><br><strong>一个东西你用起来比较简单，可不是你的能力！</strong></p>
</li>
</ul>
</div>
<h4 id="Hadoop发行版本、架构变迁"><a href="#Hadoop发行版本、架构变迁" class="headerlink" title="Hadoop发行版本、架构变迁"></a>Hadoop发行版本、架构变迁</h4><h5 id="Hadoop发行版本"><a href="#Hadoop发行版本" class="headerlink" title="Hadoop发行版本"></a>Hadoop发行版本</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-32-21.png" alt="Hadoop发行版本"></p>
<ul>
<li><p>Apache开源社区版本<br><a target="_blank" rel="noopener" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></p>
</li>
<li><p>商业发行版本<br>Cloudera: <a target="_blank" rel="noopener" href="https://www.cloudera.com/products/open-source/apache-hadoop.html">https://www.cloudera.com/products/open-source/apache-hadoop.html</a><br>Hortonworks: <a target="_blank" rel="noopener" href="https://www.cloudera.com/products/hdp.html">https://www.cloudera.com/products/hdp.html</a></p>
</li>
</ul>
<h5 id="Hadoop架构变迁（1-0-2-0变迁）"><a href="#Hadoop架构变迁（1-0-2-0变迁）" class="headerlink" title="Hadoop架构变迁（1.0-2.0变迁）"></a>Hadoop架构变迁（1.0-2.0变迁）</h5><ul>
<li><p>Hadoop 1.0<br>HDFS（分布式文件存储）<br>MapReduce（资源管理和分布式数据处理）</p>
</li>
<li><p>Hadoop 2.0<br>HDFS（分布式文件存储）<br>MapReduce（分布式数据处理）<br><strong>YARN</strong>（集群资源管理、任务调度）</p>
</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-38-22.png" alt="Hadoop架构变迁（1.0-2.0变迁）"></p>
<h5 id="Hadoop架构变迁（3-0新版本）"><a href="#Hadoop架构变迁（3-0新版本）" class="headerlink" title="Hadoop架构变迁（3.0新版本）"></a>Hadoop架构变迁（3.0新版本）</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-39-05.png" alt="Hadoop架构变迁（3.0新版本）"><br>Hadoop 3.0架构组件和Hadoop 2.0类似,<strong>3.0着重于性能优化</strong>。</p>
<ul>
<li><p>通用方面<br>精简内核、类路径隔离、shell脚本重构</p>
</li>
<li><p>Hadoop HDFS<br>EC纠删码、多NameNode支持</p>
</li>
<li><p>Hadoop MapReduce<br>任务本地化优化、内存参数自动推断</p>
</li>
<li><p>Hadoop YARN<br>Timeline Service V2、队列配置</p>
</li>
</ul>
<h3 id="Apache-Hadoop集群搭建"><a href="#Apache-Hadoop集群搭建" class="headerlink" title="Apache Hadoop集群搭建"></a>Apache Hadoop集群搭建</h3><h4 id="Hadoop集群简介"><a href="#Hadoop集群简介" class="headerlink" title="Hadoop集群简介"></a>Hadoop集群简介</h4><ul>
<li>Hadoop集群包括两个集群：HDFS集群、YARN集群</li>
<li>两个集群<strong>逻辑上分离、通常物理上在一起</strong>(可单独启动，部署于一台计算机)</li>
<li>两个集群都是标准的<strong>主从架构</strong>集群</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-46-13.png" alt="Hadoop集群"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-23-48-36.png" alt="Hadoop集群"></p>
<ul>
<li><p>逻辑上分离<br>两个集群<strong>互相之间没有依赖、互不影响</strong></p>
</li>
<li><p>物理上在一起<br>某些角色进程往往<strong>部署在同一台物理服务器上</strong></p>
</li>
<li><p>MapReduce集群呢？<br>MapReduce是计算框架、代码层面的组件没有集群之说</p>
</li>
</ul>
<h4 id="Hadoop集群模式-分布式-安装（Cluster-mode）"><a href="#Hadoop集群模式-分布式-安装（Cluster-mode）" class="headerlink" title="Hadoop集群模式(分布式)安装（Cluster mode）"></a>Hadoop集群模式(分布式)安装（Cluster mode）</h4><p>详细的集群搭建步骤可参考<a href="/2022/10/25/HadoopClusterBuilding3-3-4/" title="Hadoop 3.3.4 集群搭建">Hadoop 3.3.4 集群搭建</a></p>
<h5 id="Hadoop源码编译"><a href="#Hadoop源码编译" class="headerlink" title="Hadoop源码编译"></a>Hadoop源码编译</h5><ul>
<li>安装包、源码包下载地址<br><a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/common/">https://archive.apache.org/dist/hadoop/common/</a><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-00-08-55.png" alt="Hadoop版本"></li>
<li>为什么要重新编译Hadoop源码?<br>匹配不同<strong>操作系统本地库环境</strong>，Hadoop某些操作比如压缩、IO需要调用系统本地库（<em>.so|</em>.dll）<br><strong>修改源码、重构源码</strong></li>
<li>如何编译Hadoop<br>源码包根目录下文件：BUILDING.txt<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-00-09-25.png" alt="如何编译Hadoop"></li>
</ul>
<h5 id="Step1-集群角色规划"><a href="#Step1-集群角色规划" class="headerlink" title="Step1:集群角色规划"></a>Step1:集群角色规划</h5><ul>
<li><p>角色规划的准则<br>根据软件工作特性和服务器硬件资源情况合理分配<br>比如依赖内存工作的NameNode是不是部署在大内存机器上？</p>
</li>
<li><p>角色规划注意事项<br><strong>资源上有抢夺冲突的，尽量不要部署在一起</strong><br><strong>工作上需要互相配合的。尽量部署在一起</strong></p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>服务器</th>
<th>运行角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>node1.itcast.cn</td>
<td>namenode datanode resourcemanager nodemanager</td>
</tr>
<tr>
<td>node2.itcast.cn</td>
<td>secondarynamenode datanode nodemanager</td>
</tr>
<tr>
<td>node3.itcast.cn</td>
<td>datanode nodemanager</td>
</tr>
</tbody>
</table>
</div>
<h5 id="Step2-服务器基础环境准备"><a href="#Step2-服务器基础环境准备" class="headerlink" title="Step2:服务器基础环境准备"></a>Step2:服务器基础环境准备</h5><ul>
<li><p>主机名（3台机器）<br><code>vim /etc/hostname</code></p>
</li>
<li><p>Hosts映射（3台机器）<br><code>vim /etc/hosts</code></p>
</li>
<li><p>防火墙关闭（3台机器）<br><code>systemctl stop firewalld.service</code> #关闭防火墙<br><code>systemctl disable firewalld.service</code> #禁止防火墙开启自启</p>
</li>
<li><p>ssh免密登录<br><code>ssh-keygen</code>#4个回车生成公钥、私钥<br><code>ssh-copy-id node1</code>、<code>ssh-copy-id node2</code>、<code>ssh-copy-id node3</code></p>
</li>
<li><p>集群时间同步（3台机器）<br><code>yum -y install ntpdate</code><br><code>ntpdate ntp4.aliyun.com</code></p>
</li>
<li><p>创建统一工作目录（3台机器）</p>
</li>
</ul>
<h5 id="Step3-上传安装包、解压安装包"><a href="#Step3-上传安装包、解压安装包" class="headerlink" title="Step3:上传安装包、解压安装包"></a>Step3:上传安装包、解压安装包</h5><ul>
<li>JDK 1.8安装（3台机器）</li>
<li>上传、解压Hadoop安装包</li>
</ul>
<h5 id="Step4-Hadoop安装包目录结构"><a href="#Step4-Hadoop安装包目录结构" class="headerlink" title="Step4:Hadoop安装包目录结构"></a>Step4:Hadoop安装包目录结构</h5><div class="table-container">
<table>
<thead>
<tr>
<th>目录</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>bin</strong></td>
<td>Hadoop最基本的<strong>管理脚本</strong>和使用脚本的目录，这些脚本是sbin目录下管理脚本的基础实现，用户可以直接使用这些脚本管理和使用Hadoop。</td>
</tr>
<tr>
<td><strong>etc</strong></td>
<td>Hadoop<strong>配置文件</strong>所在的目录</td>
</tr>
<tr>
<td>include</td>
<td>对外提供的编程库头文件（具体动态库和静态库在lib目录中），这些头文件均是用C++定义的，通常用于C++程序访问HDFS或者编写MapReduce程序。</td>
</tr>
<tr>
<td>lib</td>
<td>该目录包含了Hadoop对外提供的编程动态库和静态库，与include目录中的头文件结合使用。</td>
</tr>
<tr>
<td>libexec</td>
<td>各个服务对用的shell配置文件所在的目录，可用于配置日志输出、启动参数（比如JVM参数）等基本信息。</td>
</tr>
<tr>
<td><strong>sbin</strong></td>
<td>Hadoop管理脚本所在的目录，主要包含HDFS和YARN中各类服务的<strong>启动/关闭脚本</strong>。</td>
</tr>
<tr>
<td><strong>share</strong></td>
<td>Hadoop各个模块编译后的<strong>jar包</strong>所在的目录，<strong>官方自带示例</strong>。</td>
</tr>
</tbody>
</table>
</div>
<h5 id="配置文件概述"><a href="#配置文件概述" class="headerlink" title="配置文件概述"></a>配置文件概述</h5><ul>
<li><p>官网文档<br><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/">https://hadoop.apache.org/docs/</a></p>
</li>
<li><p>第一类1个: <strong>hadoop-env.sh</strong></p>
</li>
<li><p>第二类4个：xxxx-site.xml ,site表示的是用户定义的配置，会覆盖default中的默认配置。</p>
<ul>
<li><strong>core-site.xml</strong> 核心模块配置</li>
<li><strong>hdfs-site.xml</strong> hdfs文件系统模块配置</li>
<li><strong>mapred-site.xml</strong> MapReduce模块配置</li>
<li><strong>yarn-site.xml</strong> yarn模块配置</li>
</ul>
</li>
<li><p>第三类1个：<strong>workers</strong></p>
</li>
<li>上述的配置文件目录：$HADOOP_HOME/etc/hadoop</li>
</ul>
<h5 id="Step5-编辑Hadoop配置文件-hadoop-env-sh"><a href="#Step5-编辑Hadoop配置文件-hadoop-env-sh" class="headerlink" title="Step5:编辑Hadoop配置文件 hadoop-env.sh"></a>Step5:编辑Hadoop配置文件 hadoop-env.sh</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=jdk安装路径</span><br><span class="line"><span class="comment">#文件最后添加</span></span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件-core-site-xml"><a href="#Step5-编辑Hadoop配置文件-core-site-xml" class="headerlink" title="Step5:编辑Hadoop配置文件 core-site.xml"></a>Step5:编辑Hadoop配置文件 core-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--设置默认使用的文件系统Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--设置Hadoop本地保存数据路径--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/export/data/hadoop-3.3.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--设置HDFS web UI用户身份--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--整合hive 用户代理设置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--垃圾桶文件保存时间--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1440<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件hdfs-site-xml"><a href="#Step5-编辑Hadoop配置文件hdfs-site-xml" class="headerlink" title="Step5:编辑Hadoop配置文件hdfs-site.xml"></a>Step5:编辑Hadoop配置文件hdfs-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--设置SNN进程运行机器位置信息--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node2:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件-mapred-site-xml"><a href="#Step5-编辑Hadoop配置文件-mapred-site-xml" class="headerlink" title="Step5:编辑Hadoop配置文件 mapred-site.xml"></a>Step5:编辑Hadoop配置文件 mapred-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--设置MR程序默认运行模式：yarn集群模式local本地模式--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--MR程序历史服务器端地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--历史服务器web端地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件-yarn-site-xml"><a href="#Step5-编辑Hadoop配置文件-yarn-site-xml" class="headerlink" title="Step5:编辑Hadoop配置文件 yarn-site.xml"></a>Step5:编辑Hadoop配置文件 yarn-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--设置YARN集群主角色运行机器位置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否将对容器实施物理内存限制--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否将对容器实施虚拟内存限制。--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--开启日志聚集--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--设置yarn历史服务器地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://node1:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--保存的时间7天--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Step5-编辑Hadoop配置文件-workers"><a href="#Step5-编辑Hadoop配置文件-workers" class="headerlink" title="Step5:编辑Hadoop配置文件 workers"></a>Step5:编辑Hadoop配置文件 workers</h5><pre><code class="highlight plaintext">node1.itcast.cn
node2.itcast.cn
node3.itcast.cn</code></pre>
<h5 id="Step6-分发同步安装包"><a href="#Step6-分发同步安装包" class="headerlink" title="Step6:分发同步安装包"></a>Step6:分发同步安装包</h5><ul>
<li>在node1机器上将Hadoop安装包scp同步到其他机器</li>
</ul>
<h5 id="Step7-配置Hadoop环境变量"><a href="#Step7-配置Hadoop环境变量" class="headerlink" title="Step7:配置Hadoop环境变量"></a>Step7:配置Hadoop环境变量</h5><ul>
<li><p>在node1上配置Hadoop环境变量<br><code>vim /etc/profile</code><br><code>export HADOOP_HOME=hadoop安装路径</code><br><code>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</code></p>
</li>
<li><p>将修改后的环境变量同步其他机器<br><code>scp /etc/profile root@node2:/etc/</code><br><code>scp /etc/profile root@node3:/etc/</code></p>
</li>
<li><p>重新加载环境变量验证是否生效（3台机器）<br><code>source /etc/profile</code><br><code>hadoop</code> #验证环境变量是否生效</p>
</li>
</ul>
<h5 id="Step8-NameNode-format（格式化操作）"><a href="#Step8-NameNode-format（格式化操作）" class="headerlink" title="Step8:NameNode format（格式化操作）"></a>Step8:NameNode format（格式化操作）</h5><ul>
<li>首次启动HDFS时，必须对其进行格式化操作</li>
<li>format本质上是<strong>初始化工作，进行HDFS清理和准备工作</strong></li>
<li>命令：<br><code>hdfs namenode -format</code></li>
</ul>
<div class="note info modern"><ol>
<li>首次启动之前需要format操作;</li>
<li>format只能进行一次后续不再需要;</li>
<li>如果多次format除了造成数据丢失外，还会导致hdfs集群主从角色之间互不识别。通过删除所有机器hadoop.tmp.dir目录重新format解决</li>
</ol>
</div>
<h4 id="Hadoop集群启停命令、Web-UI"><a href="#Hadoop集群启停命令、Web-UI" class="headerlink" title="Hadoop集群启停命令、Web UI"></a>Hadoop集群启停命令、Web UI</h4><h5 id="手动逐个进程启停"><a href="#手动逐个进程启停" class="headerlink" title="手动逐个进程启停"></a>手动逐个进程启停</h5><p>每台机器上每次手动启动关闭一个角色进程,可以精准控制每个进程启停，避免群起群停。</p>
<ul>
<li><p>HDFS集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop2.x版本命令</span></span><br><span class="line">hadoop-daemon.sh start|stop namenode|datanode|secondarynamenode</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop3.x版本命令</span></span><br><span class="line">hdfs --daemon start|stop namenode|datanode|secondarynamenode</span><br></pre></td></tr></table></figure>
</li>
<li><p>YARN集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop2.x版本命令</span></span><br><span class="line">yarn-daemon.sh start|stop resourcemanager|nodemanager</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop3.x版本命令</span></span><br><span class="line">yarn --daemon start|stop resourcemanager|nodemanager</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="shell脚本一键启停"><a href="#shell脚本一键启停" class="headerlink" title="shell脚本一键启停"></a>shell脚本一键启停</h5><p>在node1上，使用软件自带的shell脚本一键启动。前提：<strong>配置好机器之间的SSH免密登录和workers文件</strong>。</p>
<ul>
<li><p>HDFS集群<br><code>start-dfs.sh</code><br><code>stop-dfs.sh</code></p>
</li>
<li><p>YARN集群<br><code>start-yarn.sh</code><br><code>stop-yarn.sh</code></p>
</li>
<li><p>Hadoop集群<br><code>start-all.sh</code><br><code>stop-all.sh</code></p>
</li>
</ul>
<h5 id="进程状态、日志查看"><a href="#进程状态、日志查看" class="headerlink" title="进程状态、日志查看"></a>进程状态、日志查看</h5><ul>
<li>启动完毕之后可以使用<strong>jps命令</strong>查看进程是否启动成功</li>
<li>Hadoop启动日志路径：$HADOOP_HOME/logs/</li>
</ul>
<h5 id="HDFS集群web界面"><a href="#HDFS集群web界面" class="headerlink" title="HDFS集群web界面"></a>HDFS集群web界面</h5><p>地址：<a target="_blank" rel="noopener" href="http://namenode_host:9870">http://namenode_host:9870</a></p>
<p>其中namenode_host是namenode运行所在机器的主机名或者ip<br>如果使用主机名访问，别忘了在Windows配置hosts<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-27-37.png" alt="HDFS集群web界面"><br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-27-57.png" alt="HDFS集群web界面"></p>
<h5 id="YARN集群web界面"><a href="#YARN集群web界面" class="headerlink" title="YARN集群web界面"></a>YARN集群web界面</h5><p>地址：<a target="_blank" rel="noopener" href="http://resourcemanager_host:8088">http://resourcemanager_host:8088</a></p>
<p>其中resourcemanager_host是resourcemanager运行所在机器的主机名或者ip<br>如果使用主机名访问，别忘了在Windows配置hosts<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-28-12.png" alt="YARN集群web界面"></p>
<h4 id="Hadoop初体验"><a href="#Hadoop初体验" class="headerlink" title="Hadoop初体验"></a>Hadoop初体验</h4><h5 id="HDFS-初体验"><a href="#HDFS-初体验" class="headerlink" title="HDFS 初体验"></a>HDFS 初体验</h5><h6 id="shell命令操作"><a href="#shell命令操作" class="headerlink" title="shell命令操作"></a>shell命令操作</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoopfs-mkdir /itcast</span><br><span class="line">hadoopfs-put zookeeper.out/itcast</span><br><span class="line">hadoopfs-ls/</span><br></pre></td></tr></table></figure>
<h6 id="Web-UI页面操作"><a href="#Web-UI页面操作" class="headerlink" title="Web UI页面操作"></a>Web UI页面操作</h6><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-34-40.png" alt="Web UI页面操作"></p>
<h5 id="MapReduce-YARN初体验"><a href="#MapReduce-YARN初体验" class="headerlink" title="MapReduce+YARN初体验"></a>MapReduce+YARN初体验</h5><p>执行Hadoop官方自带的MapReduce案例，评估圆周率π的值。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $HADOOP_HOME/share/hadoop/mapreduce/</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-3.3.0.jar pi 2 4</span><br></pre></td></tr></table></figure>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-42-07.png" alt="MapReduce+YARN初体验"></p>
<h3 id="HDFS分布式文件系统基础"><a href="#HDFS分布式文件系统基础" class="headerlink" title="HDFS分布式文件系统基础"></a>HDFS分布式文件系统基础</h3><h4 id="文件系统、分布式文件系统"><a href="#文件系统、分布式文件系统" class="headerlink" title="文件系统、分布式文件系统"></a>文件系统、分布式文件系统</h4><h5 id="文件系统定义"><a href="#文件系统定义" class="headerlink" title="文件系统定义"></a>文件系统定义</h5><ul>
<li>文件系统是一种<strong>存储</strong>和<strong>组织数据</strong>的方法，实现了数据的存储、分级组织、访问和获取等操作，使得用户对文件访问和查找变得容易；</li>
<li>文件系统使用<strong>树形目录</strong>的<strong>抽象逻辑</strong>概念代替了硬盘等物理设备使用数据块的概念，用户不必关心数据底层存在硬盘哪里，只需要记住这个文件的所属目录和文件名即可；</li>
<li>文件系统通常使用硬盘和光盘这样的存储设备，并<strong>维护文件在设备中的物理位置</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-17-56-16.png" alt="文件系统"></li>
</ul>
<h5 id="传统常见的文件系统"><a href="#传统常见的文件系统" class="headerlink" title="传统常见的文件系统"></a>传统常见的文件系统</h5><ul>
<li>所谓传统常见的文件系统更多指的的<strong>单机的文件系统</strong>，也就是<strong>底层不会横跨多台机器</strong>实现。比如windows操作系统上的文件系统、Linux上的文件系统、FTP文件系统等等。</li>
<li>这些文件系统的共同特征包括：<ol>
<li>带有<strong>抽象的目录树结构</strong>，树都是从<strong>\/根目录开始</strong>往下蔓延；</li>
<li>树中节点分为两类：<strong>目录</strong>和<strong>文件</strong>；</li>
<li>从根目录开始，节点<strong>路径具有唯一性</strong>。</li>
</ol>
</li>
</ul>
<h5 id="数据、元数据"><a href="#数据、元数据" class="headerlink" title="数据、元数据"></a>数据、元数据</h5><h6 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h6><p>指存储的内容本身，比如文件、视频、图片等，这些<strong>数据底层最终是存储在磁盘</strong>等存储介质上的，一般<strong>用户无需关心</strong>，只需要基于目录树进行增删改查即可，实际针对数据的操作由文件系统完成。</p>
<h6 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h6><p>元数据（metadata）又称之为解释性数据，记录数据的数据；<br>文件系统元数据一般指<strong>文件大小、最后修改时间、底层存储位置、属性、所属用户、权限等信息</strong>。</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-00-22.png" alt="元数据"></p>
<h5 id="海量数据存储遇到的问题"><a href="#海量数据存储遇到的问题" class="headerlink" title="海量数据存储遇到的问题"></a>海量数据存储遇到的问题</h5><ul>
<li><p><strong>成本高</strong><br>传统存储硬件通用性差，设备投资加上后期维护、<strong>升级扩容的成本非常高</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-12-07.png" alt="成本高"></p>
</li>
<li><p>如何支撑高效率的计算分析<br>传统存储方式意味着数据：存储是存储，计算是计算，当<strong>需要处理数据的时候把数据移动过来</strong>。<br>程序和数据存储是属于不同的技术厂商实现，无法有机统一整合在一起。</p>
</li>
<li><p><strong>性能低</strong><br><strong>单节点I/O性能瓶</strong>颈无法逾越，难以支撑海量数据的<strong>高并发高吞吐</strong>场景。</p>
</li>
<li><p><strong>可扩展性差</strong><br>无法实现快速部署和弹性扩展，动态扩容、缩容成本高，技术实现难度大。</p>
</li>
</ul>
<h4 id="分布式存储系统的核心属性及功能含义"><a href="#分布式存储系统的核心属性及功能含义" class="headerlink" title="分布式存储系统的核心属性及功能含义"></a>分布式存储系统的核心属性及功能含义</h4><p>分布式存储系统核心属性</p>
<ul>
<li>分布式存储</li>
<li>元数据记录</li>
<li>分块存储</li>
<li>副本机制</li>
</ul>
<h5 id="分布式存储的优点"><a href="#分布式存储的优点" class="headerlink" title="分布式存储的优点"></a>分布式存储的优点</h5><ul>
<li>问题：数据量大，单机存储遇到瓶颈</li>
<li>解决：<br>单机纵向扩展：磁盘不够加磁盘，有上限瓶颈限制<br><strong>多机横向扩展</strong>：机器不够加机器，理论上<strong>无限扩展</strong></li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-16-54.png" alt="分布式存储"></p>
<h5 id="元数据记录的功能"><a href="#元数据记录的功能" class="headerlink" title="元数据记录的功能"></a>元数据记录的功能</h5><ul>
<li>问题：文件分布在不同机器上不利于寻找</li>
<li>解决：元数据记录下文件及其存储位置信息，<strong>快速定位文件位置</strong></li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-18-30.png" alt="元数据记录的功能"></p>
<h5 id="分块存储好处"><a href="#分块存储好处" class="headerlink" title="分块存储好处"></a>分块存储好处</h5><ul>
<li>问题：文件过大导致单机存不下、上传下载效率低</li>
<li>解决：文件分块存储在不同机器，<strong>针对块并行操作提高效率</strong></li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-20-30.png" alt="分块存储好处"></p>
<h5 id="副本机制的作用"><a href="#副本机制的作用" class="headerlink" title="副本机制的作用"></a>副本机制的作用</h5><ul>
<li>问题：硬件故障难以避免，数据易丢失</li>
<li>解决：不同机器设置备份，<strong>冗余存储，保障数据安全</strong></li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-22-03.png" alt="副本机制的作用"></p>
<div class="note info modern"><ol>
<li><div class="hide-block"><button type="button" class="hide-button" style>分布式存储的优点是什么？
 </button><div class="hide-content"><p><strong>无限扩展</strong>支撑海量数据存储</p>
</div></div>
</li>
<li><div class="hide-block"><button type="button" class="hide-button" style>元数据记录的功能是什么？
 </button><div class="hide-content"><p>快速<strong>定位文件</strong>位置便于查找</p>
</div></div>
</li>
<li><div class="hide-block"><button type="button" class="hide-button" style>文件分块存储好处是什么？
 </button><div class="hide-content"><p>针对块<strong>并行操作</strong>提高效率</p>
</div></div>
</li>
<li><div class="hide-block"><button type="button" class="hide-button" style>设置副本备份的作用是什么？
 </button><div class="hide-content"><p>冗余存储保障<strong>数据安全</strong></p>
</div></div>
</li>
</ol>
</div>
<h4 id="HDFS简介"><a href="#HDFS简介" class="headerlink" title="HDFS简介"></a>HDFS简介</h4><ul>
<li><p>HDFS（Hadoop Distributed File System ），意为：<strong>Hadoop分布式文件系统</strong>。</p>
</li>
<li><p>是Apache Hadoop核心组件之一，作为<strong>大数据生态圈最底层</strong>的分布式存储服务而存在。也可以说大数据首先要解决的问题就是海量数据的存储问题。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-12-22-38-08.png" alt="广义Hadoop"></p>
</li>
<li><p>HDFS主要是<strong>解决大数据如何存储问题的</strong>。分布式意味着是HDFS是横跨在多台计算机上的存储系统。</p>
</li>
<li><p>HDFS是一种能够在普通硬件上运行的分布式文件系统，它是<strong>高度容错</strong>的，适应于具有大数据集的应用程序，它非常适于存储大型数据(比如TB 和PB)。</p>
</li>
<li><p>HDFS使用多台计算机存储文件, 并且提供<strong>统一的访问接口</strong>, 像是访问一个普通文件系统一样使用分布式文件系统。</p>
</li>
<li><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-18-38-51.png" alt="HDFS简介"></p>
</li>
</ul>
<h4 id="HDFS起源发展、设计目标"><a href="#HDFS起源发展、设计目标" class="headerlink" title="HDFS起源发展、设计目标"></a>HDFS起源发展、设计目标</h4><h5 id="HDFS起源发展"><a href="#HDFS起源发展" class="headerlink" title="HDFS起源发展"></a>HDFS起源发展</h5><ul>
<li><p><strong>Doug Cutting</strong> 领导<strong>Nutch项目</strong>研发，Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能。</p>
</li>
<li><p>随着爬虫抓取网页数量的增加，遇到了严重的可扩展性问题——<strong>如何解决数十亿网页的存储和索引问题</strong>。</p>
</li>
<li><p>2003年的时候, Google发表的论文为该问题提供了可行的解决方案。<br>《<strong>分布式文件系统（GFS）</strong>，可用于处理海量网页的存储》<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-19-16-18.png" alt="GFS"></p>
</li>
<li><p>Nutch的开发人员完成了相应的开源实现HDFS，并从Nutch中剥离和MapReduce成为独立项目HADOOP。</p>
</li>
</ul>
<h5 id="HDFS设计目标"><a href="#HDFS设计目标" class="headerlink" title="HDFS设计目标"></a>HDFS设计目标</h5><ul>
<li>硬件故障（Hardware Failure）是常态，HDFS可能有成百上千的服务器组成，每一个组件都有可能出现故障。因此<strong>故障检测和自动快速恢复</strong>是HDFS的核心架构目标。</li>
<li>HDFS上的应用主要是以流式读取数据（Streaming Data Access）。HDFS被设计成用于批处理，而不是用户交互式的。相较于数据访问的反应时间，更<strong>注重数据访问的高吞吐量</strong>。</li>
<li>典型的HDFS文件大小是GB到TB的级别。所以，HDFS被调整成<strong>支持大文件（Large Data Sets）</strong>。它应该提供很高的聚合数据带宽，一个集群中支持数百个节点，一个集群中还应该支持千万级别的文件。</li>
<li>大部分HDFS应用对文件要求的是<strong>write-one-read-many</strong>访问模型。一个文件一旦<strong>创建、写入、关闭之后就不需要修改</strong>了。这一假设简化了数据一致性问题，使高吞吐量的数据访问成为可能。</li>
<li><strong>移动计算的代价比之移动数据的代价低</strong>。一个应用请求的计算，离它操作的数据越近就越高效。将计算移动到数据附近，比之将数据移动到应用所在显然更好。</li>
<li>HDFS被设计为可从一个平台<strong>轻松移植</strong>到另一个平台。这有助于将HDFS广泛用作大量应用程序的首选平台。</li>
</ul>
<h4 id="HDFS应用场景"><a href="#HDFS应用场景" class="headerlink" title="HDFS应用场景"></a>HDFS应用场景</h4><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-20-17-58.png" alt="HDFS应用场景"></p>
<h4 id="HDFS重要特性"><a href="#HDFS重要特性" class="headerlink" title="HDFS重要特性"></a>HDFS重要特性</h4><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-20-19-47.png" alt="HDFS"></p>
</blockquote>
<h5 id="整体概述"><a href="#整体概述" class="headerlink" title="整体概述"></a>整体概述</h5><ul>
<li>主从架构</li>
<li>分块存储</li>
<li>副本机制</li>
<li>元数据记录</li>
<li>抽象统一的目录树结构（namespace）</li>
</ul>
<h5 id="（1）主从架构"><a href="#（1）主从架构" class="headerlink" title="（1）主从架构"></a>（1）主从架构</h5><ul>
<li>HDFS集群是标准的master/slave主从架构集群。</li>
<li>一般一个HDFS集群是有一个Namenode和一定数目的Datanode组成。</li>
<li><strong>Namenode是HDFS主节点，Datanode是HDFS从节点，两种角色各司其职，共同协调</strong>完成分布式的文件存储服务。</li>
<li>官方架构图中是<strong>一主五从</strong>模式，其中五个从角色位于两个机架（Rack）的不同服务器上。</li>
</ul>
<h5 id="（2）分块存储"><a href="#（2）分块存储" class="headerlink" title="（2）分块存储"></a>（2）分块存储</h5><ul>
<li>HDFS中的文件在<strong>物理上是分块存储（block）</strong>的，默认大小是128M（134217728），不足128M则本身就是一块。</li>
<li>块的大小可以通过配置参数来规定，参数位于hdfs-default.xml中：dfs.blocksize。</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-20-30-29.png" alt="分块存储"></p>
<h5 id="（3）副本机制"><a href="#（3）副本机制" class="headerlink" title="（3）副本机制"></a>（3）副本机制</h5><ul>
<li>文件的所有block都会有副本。副本系数可以在文件创建的时候指定，也可以在之后通过命令改变。</li>
<li>副本数由参数dfs.replication控制，<strong>默认值是3</strong>，也就是会<strong>额外再复制2份</strong>，连同本身总共3份副本。</li>
</ul>
<h5 id="（4）元数据管理"><a href="#（4）元数据管理" class="headerlink" title="（4）元数据管理"></a>（4）元数据管理</h5><p>在HDFS中，Namenode管理的元数据具有两种类型：</p>
<ul>
<li><p><strong>文件自身属性信息</strong><br>文件名称、权限，修改时间，文件大小，复制因子，数据块大小。</p>
</li>
<li><p><strong>文件块位置映射信息</strong><br>记录文件块和DataNode之间的映射信息，即哪个块位于哪个节点上。</p>
</li>
</ul>
<h5 id="（5）namespace"><a href="#（5）namespace" class="headerlink" title="（5）namespace"></a>（5）namespace</h5><ul>
<li>HDFS支持传统的<strong>层次型文件组织结构</strong>。用户可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。</li>
<li>Namenode负责维护文件系统的namespace名称空间，任何对文件系统名称空间或属性的修改都将被Namenode记录下来。</li>
<li>HDFS会给客户端提供一个<strong>统一的抽象目录树</strong>，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data。</li>
</ul>
<h5 id="（6）数据块存储"><a href="#（6）数据块存储" class="headerlink" title="（6）数据块存储"></a>（6）数据块存储</h5><ul>
<li>文件的各个block的<strong>具体存储管理由DataNode节点承担</strong>。</li>
<li>每一个block都可以在多个DataNode上存储。</li>
</ul>
<h3 id="HDFS-shell操作"><a href="#HDFS-shell操作" class="headerlink" title="HDFS shell操作"></a>HDFS shell操作</h3><h4 id="HDFS-shell命令行解释说明"><a href="#HDFS-shell命令行解释说明" class="headerlink" title="HDFS shell命令行解释说明"></a>HDFS shell命令行解释说明</h4><p><strong>命令行界面</strong>（英语：command-line interface，缩写：CLI），是指用户通过键盘输入指令，计算机接收到指令后，予以执行一种人际交互方式。</p>
<p>Hadoop提供了文件系统的shell命令行客户端: <code>hadoop fs [generic options]</code></p>
<h5 id="文件系统协议"><a href="#文件系统协议" class="headerlink" title="文件系统协议"></a>文件系统协议</h5><ul>
<li>HDFS Shell CLI支持操作多种文件系统，包括本地文件系统（file:///）、分布式文件系统（hdfs://nn:8020）等</li>
<li>具体操作的是什么文件系统取决于命令中文件路径<strong>URL中的前缀协议</strong>。</li>
<li>如果没有指定前缀，则将会读取环境变量中的<code>fs.defaultFS</code>属性，以该属性值作为默认文件系统。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls file:/// #操作本地文件系统</span><br><span class="line">hadoop fs -ls hdfs://node1:8020/ #操作HDFS分布式文件系统</span><br><span class="line">hadoop fs -ls / #直接根目录，没有指定协议将加载读取fs.defaultFS值</span><br></pre></td></tr></table></figure>
<h5 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h5><ul>
<li>hadoop dfs 只能操作HDFS文件系统（包括与Local FS间的操作），不过已经Deprecated；</li>
<li>hdfs dfs 只能操作HDFS文件系统相关（包括与Local FS间的操作）,常用；</li>
<li><code>hadoop fs</code> 可操作任意文件系统，不仅仅是hdfs文件系统，使用范围更广；</li>
</ul>
<p>目前版本来看，官方最终推荐使用的是hadoop fs。当然hdfs dfs在市面上的使用也比较多。</p>
<h5 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h5><ul>
<li>HDFS文件系统的操作命令很多和Linux类似，因此学习成本相对较低。</li>
<li>可以通过<code>hadoop fs -help</code>命令来查看每个命令的详细用法。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">[-appendToFile&lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">……</span><br><span class="line">-appendToFile&lt;localsrc&gt; ... &lt;dst&gt; :</span><br><span class="line">Appends the contents of all the given local files to the given dst file. The dst</span><br><span class="line">file will be created if it does not exist. If &lt;localSrc&gt; is -, then the input is</span><br><span class="line">read from stdin.</span><br><span class="line">-cat [-ignoreCrc] &lt;src&gt; ... :</span><br><span class="line">Fetch all files that match the file pattern &lt;src&gt; and display their content on</span><br><span class="line">stdout.</span><br></pre></td></tr></table></figure>
<h4 id="HDFS-shell命令行常用操作"><a href="#HDFS-shell命令行常用操作" class="headerlink" title="HDFS shell命令行常用操作"></a>HDFS shell命令行常用操作</h4><h5 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir [-p] &lt;path&gt; ...</span><br></pre></td></tr></table></figure>
<ul>
<li><code>path</code> 为待创建的目</li>
<li><code>-p</code>选项的行为与Unix mkdir -p非常相似，它<strong>会创建路径中的各级父目录</strong>。</li>
</ul>
<h5 id="查看指定目录下内容"><a href="#查看指定目录下内容" class="headerlink" title="查看指定目录下内容"></a>查看指定目录下内容</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls [-h] [-R] [&lt;path&gt; ...]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>path</code> 指定目录路径</li>
<li><code>-h</code> 人性化显示文件size</li>
<li><code>-R</code> 递归查看指定目录及其子目录</li>
</ul>
<h5 id="上传文件到HDFS指定目录下"><a href="#上传文件到HDFS指定目录下" class="headerlink" title="上传文件到HDFS指定目录下"></a>上传文件到HDFS指定目录下</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>-f</code> 覆盖目标文件（已存在下）</li>
<li><code>-p</code> 保留访问和修改时间，所有权和权限。</li>
<li><code>localsrc</code> 本地文件系统（客户端所在机器）</li>
<li><code>dst</code> 目标文件系统（HDFS）</li>
</ul>
<h5 id="查看HDFS文件内容"><a href="#查看HDFS文件内容" class="headerlink" title="查看HDFS文件内容"></a>查看HDFS文件内容</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat &lt;src&gt; ...</span><br></pre></td></tr></table></figure>
<p>读取指定文件全部内容，显示在标准输出控制台。<br>注意：对于<strong>大文件内容读取，慎重</strong>。</p>
<h5 id="下载HDFS文件"><a href="#下载HDFS文件" class="headerlink" title="下载HDFS文件"></a>下载HDFS文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get [-f] [-p] &lt;src&gt; ... &lt;localdst&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>下载文件到本地文件系统指定目录，localdst必须是目录</li>
<li><code>-f</code> 覆盖目标文件（已存在下）</li>
<li><code>-p</code> 保留访问和修改时间，所有权和权限。</li>
</ul>
<h5 id="拷贝HDFS文件"><a href="#拷贝HDFS文件" class="headerlink" title="拷贝HDFS文件"></a>拷贝HDFS文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp [-f] &lt;src&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>-f</code> 覆盖目标文件（已存在下）</li>
</ul>
<h5 id="追加数据到HDFS文件中"><a href="#追加数据到HDFS文件中" class="headerlink" title="追加数据到HDFS文件中"></a>追加数据到HDFS文件中</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -appendToFile&lt;localsrc&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<p>将所有给定本地文件的内容追加到给定dst文件。<br>dst如果文件不存在，将创建该文件。<br>注意：<strong>appendToFile 是将当地文件内容追加的到 hadoop 上的文件（不能hadoop上的文件1 追加给 hadoop上的文件2）</strong></p>
<h5 id="HDFS数据移动操作"><a href="#HDFS数据移动操作" class="headerlink" title="HDFS数据移动操作"></a>HDFS数据移动操作</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv &lt;src&gt; ... &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<p>移动文件到指定文件夹下<br>可以使用该命令移动数据，重命名文件的名称</p>
<h5 id="HDFS-shell其他命令"><a href="#HDFS-shell其他命令" class="headerlink" title="HDFS shell其他命令"></a>HDFS shell其他命令</h5><p>命令官方指导文档<br><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/">https://hadoop.apache.org/docs/</a><br><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/FileSystemShell.html">https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/FileSystemShell.html</a></p>
<p>常见的操作自己最好能够记住，其他操作可以根据需要查询文档使用。<br>命令属于<strong>多用多会，孰能生巧，不用就忘</strong>。</p>
<h3 id="HDFS工作流程与机制"><a href="#HDFS工作流程与机制" class="headerlink" title="HDFS工作流程与机制"></a>HDFS工作流程与机制</h3><h4 id="HDFS集群角色与职责"><a href="#HDFS集群角色与职责" class="headerlink" title="HDFS集群角色与职责"></a>HDFS集群角色与职责</h4><h5 id="官方架构图"><a href="#官方架构图" class="headerlink" title="官方架构图"></a>官方架构图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-13-20-19-47.png" alt="HDFS"></p>
<h5 id="主角色：namenode"><a href="#主角色：namenode" class="headerlink" title="主角色：namenode"></a>主角色：namenode</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-10-53.png" alt="namenode"></p>
<ul>
<li><code>NameNode</code>是Hadoop分布式文件系统的核心，架构中的主角色。</li>
<li><strong>NameNode维护和管理文件系统元数据</strong>，包括名称空间目录树结构、文件和块的位置信息、访问权限等信息。</li>
<li>基于此，<strong>NameNode成为了访问HDFS的唯一入口</strong>。</li>
<li>NameNode内部通过<strong>内存</strong>和<strong>磁盘文件</strong>两种方式管理元数据。</li>
<li>其中磁盘上的元数据文件包括Fsimage内存元数据镜像文件和edits log（Journal）编辑日志。</li>
</ul>
<h5 id="从角色：datanode"><a href="#从角色：datanode" class="headerlink" title="从角色：datanode"></a>从角色：datanode</h5><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-16-51.png" alt="datanode"></p>
</blockquote>
<ul>
<li><code>DataNode</code>是Hadoop HDFS中的从角色，负责<strong>具体的数据块存储</strong>。</li>
<li>DataNode的数量决定了HDFS集群的整体数据存储能力。通过和NameNode配合维护着数据块。</li>
</ul>
<h5 id="主角色辅助角色：secondarynamenode"><a href="#主角色辅助角色：secondarynamenode" class="headerlink" title="主角色辅助角色：secondarynamenode"></a>主角色辅助角色：secondarynamenode</h5><ul>
<li>Secondary NameNode充当NameNode的辅助节点，但不能替代NameNode。</li>
<li>主要是帮助主角色进行元数据文件的合并动作。可以通俗的理解为主角色的“秘书”。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-18-47.png" alt="secondarynamenode"></li>
</ul>
<h5 id="namenode职责"><a href="#namenode职责" class="headerlink" title="namenode职责"></a>namenode职责</h5><ul>
<li>NameNode仅<strong>存储HDFS的元数据</strong>：文件系统中所有文件的目录树，并跟踪整个集群中的文件，不存储实际数据。</li>
<li>NameNode知道HDFS中任何<strong>给定文件的块列表及其位置</strong>。使用此信息NameNode知道如何从块中构建文件。</li>
<li>NameNode<strong>不持久化存储每个文件中各个块所在的datanode的位置信息</strong>，这些信息会在系统启动时从DataNode重建。</li>
<li>NameNode是Hadoop集群中的<strong>单点故障</strong>。</li>
<li>NameNode所在机器通常会配置有<strong>大量内存（RAM）</strong>。</li>
</ul>
<h5 id="datanode职责"><a href="#datanode职责" class="headerlink" title="datanode职责"></a>datanode职责</h5><ul>
<li>DataNode负责<strong>最终数据块block的存储</strong>。是集群的<strong>从角色</strong>，也称为Slave。</li>
<li>DataNode启动时，会将自己<strong>注册</strong>到NameNode并<strong>汇报</strong>自己负责持有的块列表。</li>
<li>当某个DataNode关闭时，不会影响数据的可用性。NameNode将安排由其他DataNode管理的块进行副本复制。</li>
<li>DataNode所在机器通常配置有大量的<strong>硬盘</strong>空间，因为实际数据存储在DataNode中。</li>
</ul>
<h4 id="HDFS写数据流程（上传文件）"><a href="#HDFS写数据流程（上传文件）" class="headerlink" title="HDFS写数据流程（上传文件）"></a>HDFS写数据流程（上传文件）</h4><h5 id="写数据完整流程图"><a href="#写数据完整流程图" class="headerlink" title="写数据完整流程图"></a>写数据完整流程图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-25-38.png" alt="写数据完整流程图"></p>
<h5 id="核心概念—Pipeline管道"><a href="#核心概念—Pipeline管道" class="headerlink" title="核心概念—Pipeline管道"></a>核心概念—Pipeline管道</h5><ul>
<li><code>Pipeline</code>，中文翻译为管道。这是HDFS在上传文件写数据过程中采用的一种数据传输方式。</li>
<li>客户端将数据块写入第一个数据节点，第一个数据节点保存数据之后再将块复制到第二个数据节点，后者保存后将其复制到第三个数据节点。</li>
<li>为什么datanode之间采用pipeline线性传输，而不是一次给三个datanode拓扑式传输呢？</li>
<li>因为数据以管道的方式，<strong>顺序的沿着一个方向传输，这样能够充分利用每个机器的带宽，避免网络瓶颈和高延迟时的连接，最小化推送所有数据的延时</strong>。</li>
<li>在线性推送模式下，每台机器所有的出口宽带都用于以最快的速度传输数据，而不是在多个接受者之间分配宽带。</li>
</ul>
<h5 id="核心概念—ACK应答响应"><a href="#核心概念—ACK应答响应" class="headerlink" title="核心概念—ACK应答响应"></a>核心概念—ACK应答响应</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-31-42.png" alt="核心概念"></p>
<ul>
<li>ACK (Acknowledge character）即是确认字符，在数据通信中，接收方发给发送方的一种传输类控制字符。表示发来的数据已确认接收无误。</li>
<li>在HDFS pipeline管道传输数据的过程中，传输的反方向会进行ACK校验，确保数据传输安全。</li>
</ul>
<h5 id="核心概念—默认3副本存储策略"><a href="#核心概念—默认3副本存储策略" class="headerlink" title="核心概念—默认3副本存储策略"></a>核心概念—默认3副本存储策略</h5><ul>
<li>默认副本存储策略是由BlockPlacementPolicyDefault指定。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-34-22.png" alt="默认3副本存储策略"></li>
<li>第一块副本：优先客户端本地，否则随机</li>
<li>第二块副本：不同于第一块副本的不同机架。</li>
<li>第三块副本：第二块副本相同机架不同机器。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-35-49.png" alt="默认3副本存储策略"></li>
</ul>
<h5 id="写数据完整流程图文字描述"><a href="#写数据完整流程图文字描述" class="headerlink" title="写数据完整流程图文字描述"></a>写数据完整流程图文字描述</h5><ol>
<li>HDFS客户端创建对象实例<code>DistributedFileSystem</code>，该对象中封装了与HDFS文件系统操作的相关方法。</li>
<li>调用DistributedFileSystem对象的create()方法，通过<code>RPC</code>(远程过程调用)请求NameNode创建文件。<br>NameNode执行各种检查判断：目标文件是否存在、父目录是否存在、客户端是否具有创建该文件的权限。检查通过，NameNode就会为本次请求记下一条记录，返回<code>FSDataOutputStream输出流</code>对象给客户端用于写数据。</li>
<li>客户端通过FSDataOutputStream输出流开始写入数据。</li>
<li>客户端写入数据时，将数据分成一个个数据包（<strong>packet 默认64k</strong>）,内部组件<code>DataStreamer</code>请求NameNode挑选出适合存储数据副本的一组DataNode地址，默认是3副本存储。<br>DataStreamer将数据包流式传输到<code>pipeline</code>的第一个DataNode,该DataNode存储数据包并将它发送到pipeline的第二个DataNode。同样，第二个DataNode存储数据包并且发送给第三个（也是最后一个）DataNode。</li>
<li>传输的反方向上，会通过<code>ACK机制</code>校验数据包传输是否成功；</li>
<li>客户端完成数据写入后，在FSDataOutputStream输出流上调用close()方法关闭。</li>
<li>DistributedFileSystem联系NameNode告知其文件写入完成，等待NameNode确认。<br>因为namenode已经知道文件由哪些块组成（DataStream请求分配数据块），因此仅需等待最小复制块即可成功返回。<br>最小复制是由参数dfs.namenode.replication.min指定，默认是1.</li>
</ol>
<h4 id="HDFS读数据流程（下载文件）"><a href="#HDFS读数据流程（下载文件）" class="headerlink" title="HDFS读数据流程（下载文件）"></a>HDFS读数据流程（下载文件）</h4><h5 id="读数据完整流程图"><a href="#读数据完整流程图" class="headerlink" title="读数据完整流程图"></a>读数据完整流程图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-00-46-15.png" alt="读数据完整流程图"></p>
<ol>
<li>HDFS客户端创建对象实例<code>DistributedFileSystem</code>，调用该对象的open()方法来打开希望读取的文件。</li>
<li>DistributedFileSystem使用RPC调用namenode来确定<strong>文件中前几个块的块位置（分批次读取）信息</strong>。<br>对于每个块，namenode返回具有该块所有副本的datanode位置地址列表，并且该地址列表是排序好的，与客户端的网络拓扑距离近的排序靠前。</li>
<li>DistributedFileSystem将FSDataInputStream输入流返回到客户端以供其读取数据。</li>
<li>客户端在FSDataInputStream输入流上调用read()方法。然后，已存储DataNode地址的InputStream连接到文件中第一个块的最近的DataNode。数据从DataNode流回客户端，结果客户端可以在流上重复调用read（）。</li>
<li>当该块结束时，FSDataInputStream将关闭与DataNode的连接，然后寻找下一个block块的最佳datanode位置。这些操作对用户来说是透明的。所以用户感觉起来它一直在读取一个连续的流。<br>客户端从流中读取数据时，也会根据需要询问NameNode来<strong>检索下一批数据块的DataNode位置信息</strong>。</li>
<li>一旦客户端完成读取，就对FSDataInputStream调用close()方法。</li>
</ol>
<h2 id="Hadoop-MapReduce与Hadoop-YARN"><a href="#Hadoop-MapReduce与Hadoop-YARN" class="headerlink" title="Hadoop MapReduce与Hadoop YARN"></a>Hadoop MapReduce与Hadoop YARN</h2><h3 id="Hadoop-MapReduce"><a href="#Hadoop-MapReduce" class="headerlink" title="Hadoop MapReduce"></a>Hadoop MapReduce</h3><h4 id="MapReduce思想"><a href="#MapReduce思想" class="headerlink" title="MapReduce思想"></a>MapReduce思想</h4><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-15-37.png" alt="分而治之"></p>
</blockquote>
<ul>
<li>MapReduce的思想核心是“<strong>先分再合，分而治之</strong>”。</li>
<li>所谓“分而治之”就是<strong>把一个复杂的问题，按照一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，分别找出各部分的结果，然后把各部分的结果组成整个问题的最终结果</strong>。</li>
<li>这种思想来源于日常生活与工作时的经验。即使是发布过论文实现分布式计算的谷歌也只是实现了这种思想，而不是自己原创。</li>
<li>Map表示第一阶段，负责“<strong>拆分</strong>”：即把复杂的任务<strong>分解为若干个“简单的子任务”来并行处理</strong>。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎<strong>没有依赖关系</strong>。</li>
<li>Reduce表示第二阶段，负责“<strong>合并</strong>”：即对map阶段的结果进行全局汇总。</li>
<li>这两个阶段合起来正是MapReduce思想的体现。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-17-49.png" alt="apReduce思想"></li>
<li>一个比较形象的例子解释MapReduce<br>要数停车场中的所有停放车的总数量。<br>你数第一列，我数第二列…这就是Map阶段，人越多，能够同时数车的人就越多，速度就越快。<br>数完之后，聚到一起，把所有人的统计数加在一起。这就是Reduce合并汇总阶段。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-19-06.png" alt="MapReduce思想"></li>
</ul>
<h4 id="Hadoop-MapReduce设计构思"><a href="#Hadoop-MapReduce设计构思" class="headerlink" title="Hadoop MapReduce设计构思"></a>Hadoop MapReduce设计构思</h4><h5 id="（1）如何对付大数据处理场景"><a href="#（1）如何对付大数据处理场景" class="headerlink" title="（1）如何对付大数据处理场景"></a>（1）如何对付大数据处理场景</h5><ul>
<li>对相互间不具有计算依赖关系的大数据计算任务，实现并行最自然的办法就是<strong>采取MapReduce分而治之</strong>的策略。</li>
<li>首先Map阶段进行拆分，把大数据拆分成若干份小数据，多个程序同时并行计算产生中间结果；然后是Reduce聚合阶段，通过程序对并行的结果进行最终的汇总计算，得出最终的结果。</li>
<li><strong>不可拆分的计算任务或相互间有依赖关系的数据无法进行并行计算</strong>！</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-24-30.png" alt="MapReduce思想"></li>
</ul>
<h5 id="（2）构建抽象编程模型"><a href="#（2）构建抽象编程模型" class="headerlink" title="（2）构建抽象编程模型"></a>（2）构建抽象编程模型</h5><ul>
<li><p>MapReduce借鉴了<strong>函数式</strong>语言中的思想，用<strong>Map</strong>和<strong>Reduce</strong>两个函数提供了高层的并行编程抽象模型。<br>map: 对一组数据元素进行某种重复式的处理；<br>reduce: 对Map的中间结果进行某种进一步的结果整理。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-29-25.png" alt="构建抽象编程模型"></p>
</li>
<li><p>MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现:<br>map: (k1; v1) → (k2; v2)<br>reduce: (k2; [v2]) → (k3; v3)</p>
</li>
<li><p>通过以上两个编程接口，大家可以看出MapReduce处理的数据类型是<code>&lt;key,value&gt;键值对</code>。</p>
</li>
</ul>
<h5 id="（3）统一架构、隐藏底层细节"><a href="#（3）统一架构、隐藏底层细节" class="headerlink" title="（3）统一架构、隐藏底层细节"></a>（3）统一架构、隐藏底层细节</h5><ul>
<li>如何提供统一的计算框架，如果没有统一封装底层细节，那么程序员则需要考虑诸如数据存储、划分、分发、结果收集、错误恢复等诸多细节；为此，MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。</li>
<li>MapReduce最大的亮点在于通过抽象模型和计算框架把需要<strong>做什么(what need to do)</strong>与具体<strong>怎么做(how to do)</strong>分开了，为程序员提供一个抽象和高层的编程接口和框架。</li>
<li><strong>程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的业务程序代码</strong>。</li>
<li>至于如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理：从分布代码的执行，到大到数千小到单个节点集群的自动调度使用。</li>
</ul>
<h4 id="Hadoop-MapReduce介绍"><a href="#Hadoop-MapReduce介绍" class="headerlink" title="Hadoop MapReduce介绍"></a>Hadoop MapReduce介绍</h4><h5 id="分布式计算概念"><a href="#分布式计算概念" class="headerlink" title="分布式计算概念"></a>分布式计算概念</h5><ul>
<li><strong>分布式计算</strong>是一种计算方法，和<strong>集中式计算</strong>是相对的。</li>
<li>随着计算技术的发展，有些应用需要非常巨大的计算能力才能完成，如果采用集中式计算，需要耗费相当长的时间来完成。</li>
<li>分布式计算<strong>将该应用分解成许多小的部分，分配给多台计算机进行处理</strong>。这样可以节约整体计算时间，大大提高计算效率。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-37-36.png" alt="分布式计算概念"></li>
</ul>
<h5 id="Hadoop-MapReduce概述"><a href="#Hadoop-MapReduce概述" class="headerlink" title="Hadoop MapReduce概述"></a>Hadoop MapReduce概述</h5><ul>
<li>Hadoop MapReduce是一个<strong>分布式计算框架</strong>，用于轻松编写分布式应用程序，这些应用程序以可靠，容错的方式并行处理大型硬件集群（数千个节点）上的大量数据（多TB数据集）。</li>
<li>MapReduce是一种面向海量数据处理的一种指导思想，也是一种用于对大规模数据进行分布式计算的编程模型。</li>
</ul>
<h5 id="MapReduce产生背景"><a href="#MapReduce产生背景" class="headerlink" title="MapReduce产生背景"></a>MapReduce产生背景</h5><ul>
<li>MapReduce最早由<strong>Google</strong>于2004年在一篇名为《MapReduce:SimplifiedData Processingon Large Clusters》的<strong>论文</strong>中提出。</li>
<li>论文中谷歌把分布式数据处理的过程拆分为Map和Reduce两个操作函数（受到函数式编程语言的启发），随后被Apache Hadoop参考并作为开源版本提供支持，叫做Hadoop MapReduce。</li>
<li>它的出现解决了人们在最初面临海量数据束手无策的问题，同时它还是<strong>易于使用和高度可扩展的</strong>，使得开发者无需关系分布式系统底层的复杂性即可很容易的编写分布式数据处理程序，并在成千上万台普通的商用服务器中运行。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-51-39.png" alt="MapReduce产生背景"></li>
</ul>
<h5 id="MapReduce特点"><a href="#MapReduce特点" class="headerlink" title="MapReduce特点"></a>MapReduce特点</h5><ul>
<li><p><strong>易于编程</strong><br>Mapreduce框架提供了用于二次开发的接口；简单地实现一些接口，就可以完成一个分布式程序。任务计算交给计算框架去处理，将分布式程序部署到hadoop集群上运行，集群节点可以扩展到成百上千个等。</p>
</li>
<li><p><strong>良好的扩展性</strong><br>当计算机资源不能得到满足的时候，可以通过增加机器来扩展它的计算能力。基于MapReduce的分布式计算得特点可以随节点数目增长保持近似于线性的增长，这个特点是MapReduce处理海量数据的关键，通过将计算节点增至几百或者几千可以很容易地处理数百TB甚至PB级别的离线数据。</p>
</li>
<li><p><strong>高容错性</strong><br>Hadoop集群是分布式搭建和部署得，任何单一机器节点宕机了，它可以把上面的计算任务转移到另一个节点上运行，不影响整个作业任务得完成，过程完全是由Hadoop内部完成的。</p>
</li>
<li><p><strong>适合海量数据的离线处理</strong><br>可以处理GB、TB和PB级别得数据量</p>
</li>
</ul>
<h5 id="MapReduce局限性"><a href="#MapReduce局限性" class="headerlink" title="MapReduce局限性"></a>MapReduce局限性</h5><p>MapReduce虽然有很多的优势，也有相对得局限性，局限性不代表不能做，而是在有些场景下实现的效果比较差，并不适合用MapReduce来处理，主要表现在以下结果方面：</p>
<ul>
<li><p><strong>实时计算性能差</strong><br>MapReduce主要应用于离线作业，无法作到秒级或者是亚秒级得数据响应。</p>
</li>
<li><p><strong>不能进行流式计算</strong><br>流式计算特点是数据是源源不断得计算，并且数据是动态的；而MapReduce作为一个离线计算框架，主要是针对静态数据集得，数据是不能动态变化得。</p>
</li>
</ul>
<h5 id="MapReduce实例进程"><a href="#MapReduce实例进程" class="headerlink" title="MapReduce实例进程"></a>MapReduce实例进程</h5><p>一个完整的MapReduce程序在分布式运行时有<strong>三类</strong></p>
<ul>
<li><code>MRAppMaster</code>：负责整个MR程序的过程调度及状态协调</li>
<li><code>MapTask</code>：负责map阶段的整个数据处理流程</li>
<li><code>ReduceTask</code>：负责reduce阶段的整个数据处理流程</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-56-12.png" alt="MapReduce实例进程"></li>
</ul>
<h5 id="MapReduce阶段组成"><a href="#MapReduce阶段组成" class="headerlink" title="MapReduce阶段组成"></a>MapReduce阶段组成</h5><ul>
<li>一个MapReduce编程模型中<strong>只能包含一个Map阶段和一个Reduce阶段，或者只有Map阶段</strong>；</li>
<li>不能有诸如多个map阶段、多个reduce阶段的情景出现；</li>
<li>如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序串行运行。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-57-30.png" alt="阶段组成"></li>
</ul>
<h5 id="MapReduce数据类型"><a href="#MapReduce数据类型" class="headerlink" title="MapReduce数据类型"></a>MapReduce数据类型</h5><ul>
<li>注意：整个MapReduce程序中，数据都是以<strong>kv键值对的形式流转</strong>的；</li>
<li>在实际编程解决各种业务问题中，需要考虑每个阶段的输入输出kv分别是什么；</li>
<li>MapReduce内置了很多默认属性，比如排序、分组等，都和数据的k有关，所以说kv的类型数据确定及其重要的</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-10-58-42.png" alt="MapReduce数据类型"></li>
</ul>
<h4 id="Hadoop-MapReduce官方示例"><a href="#Hadoop-MapReduce官方示例" class="headerlink" title="Hadoop MapReduce官方示例"></a>Hadoop MapReduce官方示例</h4><ul>
<li>一个最终完整版本的MR程序需要<strong>用户编写的代码</strong>和<strong>Hadoop自己实现的代码</strong>整合在一起才可以；</li>
<li>其中用户负责map、reduce两个阶段的业务问题，Hadoop负责底层所有的技术问题；</li>
<li>由于MapReduce计算引擎天生的弊端（慢），当下企业中直接使用率已经日薄西山了，所以在<strong>企业中工作很少涉及到MapReduce直接编程</strong>，但是某些软件的背后还依赖MapReduce引擎。</li>
<li>可以通过官方提供的示例来<strong>感受MapReduce及其内部执行流程</strong>，因为后续的新的计算引擎比如Spark，当中就有MapReduce深深的影子存在。</li>
</ul>
<h5 id="MapReduce示例说明"><a href="#MapReduce示例说明" class="headerlink" title="MapReduce示例说明"></a>MapReduce示例说明</h5><ul>
<li>示例程序路径：<code>$HADOOP_HOME/share/hadoop/mapreduce/</code></li>
<li>示例程序：hadoop-mapreduce-examples-3.3.0.jar</li>
<li>MapReduce程序提交命令：<code>[hadoop jar|yarn jar] hadoop-mapreduce-examples-3.3.0.jar args…</code></li>
<li>提交到哪里去？<strong>提交到YARN集群上分布式执行</strong>。</li>
</ul>
<h5 id="评估圆周率π（PI）的值"><a href="#评估圆周率π（PI）的值" class="headerlink" title="评估圆周率π（PI）的值"></a>评估圆周率π（PI）的值</h5><p>Hadoop MapReduce示例提供了Monte Carlo方法计算圆周率。</p>
<h6 id="Monte-Carlo方法"><a href="#Monte-Carlo方法" class="headerlink" title="Monte Carlo方法"></a>Monte Carlo方法</h6><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-22-42-30.png" alt="评估圆周率π（PI）的值"></p>
</blockquote>
<p>假设正方形边长为1，圆半径也为1，那么1/4圆的面积为：$\frac{1}{4} \pi r^2$</p>
<p>在正方形内随机撒点，分布于1/4圆内的数量假设为a ，分布于圆外的数量为b，N则是所产生的总数：$N=a+b$</p>
<p>那么数量a与N的比值应与1/4圆面积及正方形面积成正比，于是：$\frac{\pi}{4}:1=a:N$</p>
<script type="math/tex; mode=display">
\pi = \frac{4a}{N}</script><h6 id="评估圆周率π参数设置"><a href="#评估圆周率π参数设置" class="headerlink" title="评估圆周率π参数设置"></a>评估圆周率π参数设置</h6><p>运行MapReduce程序评估一下圆周率的值，执行中可以去YARN页面上观察程序的执行的情况。</p>
<ul>
<li>第一个参数：pi表示MapReduce程序执行圆周率计算任务；</li>
<li>第二个参数：用于指定map阶段运行的任务task次数，并发度，这里是10；</li>
<li>第三个参数：用于指定每个map任务取样的个数，这里是50。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-3.3.0.jar pi 10 50</span><br></pre></td></tr></table></figure>
<h5 id="wordcount单词词频统计"><a href="#wordcount单词词频统计" class="headerlink" title="wordcount单词词频统计"></a>wordcount单词词频统计</h5><details class="toggle"><summary class="toggle-button" style>1.txt</summary><div class="toggle-content"><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello tom hello allen hello</span><br><span class="line">allen tom mac apple</span><br><span class="line">hello allen apple</span><br><span class="line">hello apple spark allen hadoop spark</span><br></pre></td></tr></table></figure>
</div></details>
<p>WordCount中文叫做单词统计、词频统计；<br>指的是统计指定文件中，每个<strong>单词出现的总次数</strong>。</p>
<h6 id="WordCount概述"><a href="#WordCount概述" class="headerlink" title="WordCount概述"></a>WordCount概述</h6><p>WordCount算是大数据计算领域经典的入门案例，相当于Hello World。</p>
<p>虽然WordCount业务极其简单，但关键是能够通过案例<strong>感受背后MapReduce的执行流程和默认的行为机制</strong>。</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-22-49-46.png" alt="WordCount"></p>
<h6 id="WordCount编程实现思路"><a href="#WordCount编程实现思路" class="headerlink" title="WordCount编程实现思路"></a>WordCount编程实现思路</h6><ul>
<li>map阶段的核心：把输入的<strong>数据经过切割，全部标记1</strong>，因此输出就是&lt;单词，1&gt;。</li>
<li><strong>shuffle阶段核心：经过MR程序内部自带默认的排序分组等功能，把key相同的单词会作为一组数据构成新的kv对</strong>。</li>
<li>reduce阶段核心：处理shuffle完的一组数据，该组数据就是该单词所有的键值对。对所有的1进行累加求和，就是单词的总次数。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-22-51-54.png" alt="WordCount"></li>
</ul>
<h6 id="WordCount程序提交"><a href="#WordCount程序提交" class="headerlink" title="WordCount程序提交"></a>WordCount程序提交</h6><ul>
<li><p>上传文本文件1.txt(写入一些单词)到HDFS文件系统的/input目录下，如果没有这个目录，使用shell创建<br><code>hadoop fs -mkdir /input</code><br><code>hadoop fs -put 1.txt /input</code></p>
</li>
<li><p>执行官方MapReduce实例，对上述文件进行单词次数统计<br>第一个参数：wordcount表示执行单词统计任务；<br>第二个参数：指定输入文件的路径；<br>第三个参数：指定输出结果的路径（该路径不能已存在）；</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-3.3.0.jar wordcount /input /output</span><br></pre></td></tr></table></figure>
<h4 id="Map阶段执行流程"><a href="#Map阶段执行流程" class="headerlink" title="Map阶段执行流程"></a>Map阶段执行流程</h4><h5 id="WordCount执行流程图"><a href="#WordCount执行流程图" class="headerlink" title="WordCount执行流程图"></a>WordCount执行流程图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-22-51-54.png" alt="WordCount"></p>
<h5 id="MapReduce整体执行流程图"><a href="#MapReduce整体执行流程图" class="headerlink" title="MapReduce整体执行流程图"></a>MapReduce整体执行流程图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-04-23.png" alt="MapReduce整体执行流程图"></p>
<h5 id="Map阶段执行过程"><a href="#Map阶段执行过程" class="headerlink" title="Map阶段执行过程"></a>Map阶段执行过程</h5><blockquote class="pullquote right"><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-05-01.png" alt="Map阶段执行过程"></p>
</blockquote>
<ul>
<li><p>第一阶段：把输入目录下文件按照一定的标准逐个进行<strong>逻辑切片</strong>，形成切片规划。<br>默认Split size = Block size（128M），每一个切片由一个MapTask处理。（getSplits）</p>
</li>
<li><p>第二阶段：对切片中的数据按照一定的规则读取解析返回<key,value>对。<br>默认是<strong>按行读取数据</strong>。key是每一行的起始位置偏移量，value是本行的文本内容。（TextInputFormat）</key,value></p>
</li>
<li><p>第三阶段：调用Mapper类中的<strong>map方法处理数据</strong>。<br>每读取解析出来的一个<key,value> ，调用一次map方法。</key,value></p>
</li>
<li><p>第四阶段：按照一定的规则对Map输出的键值对进行<strong>分区partition</strong>。默认不分区，因为只有一个reducetask。<br>分区的数量就是reducetask运行的数量。</p>
</li>
<li><p>第五阶段：Map输出数据写入<strong>内存缓冲区</strong>，达到比例溢出到磁盘上。<strong>溢出spill</strong>的时候根据key进行<strong>排序sort</strong>。<br>默认根据key字典序排序。</p>
</li>
<li><p>第六阶段：对所有溢出文件进行最终的<strong>merge合并</strong>，成为一个文件。</p>
</li>
</ul>
<h4 id="Reduce阶段执行流程"><a href="#Reduce阶段执行流程" class="headerlink" title="Reduce阶段执行流程"></a>Reduce阶段执行流程</h4><ul>
<li>第一阶段：ReduceTask会主动从MapTask<strong>复制拉取</strong>属于需要自己处理的数据。</li>
<li>第二阶段：把拉取来数据，全部进行<strong>合并merge</strong>，即把分散的数据合并成一个大的数据。再对合并后的数据<strong>排序</strong>。</li>
<li>第三阶段是对排序后的键值对<strong>调用reduce方法</strong>。<strong>键相等</strong>的键值对调用一次reduce方法。最后把这些输出的键值对写入到HDFS文件中。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-16-08.png" alt="Reduce阶段执行流程"></li>
</ul>
<h4 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h4><h5 id="shuffle概念"><a href="#shuffle概念" class="headerlink" title="shuffle概念"></a>shuffle概念</h5><ul>
<li><code>Shuffle</code>的本意是洗牌、混洗的意思，把一组有规则的数据尽量打乱成无规则的数据。</li>
<li>而在MapReduce中，Shuffle更像是洗牌的<strong>逆</strong>过程，指的是<strong>将map端的无规则输出按指定的规则“打乱”成具有一定规则的数据，以便reduce端接收处理</strong>。</li>
<li><strong>一般把从Map产生输出开始到Reduce取得数据作为输入之前的过程称作shuffle</strong>。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-56-07.png" alt="shuffle概念"></li>
</ul>
<h5 id="Map端Shuffle"><a href="#Map端Shuffle" class="headerlink" title="Map端Shuffle"></a>Map端Shuffle</h5><ul>
<li>Collect阶段：将MapTask的结果收集输出到默认大小为100M的环形缓冲区，保存之前会对key进行分区的计算，默认Hash分区。</li>
<li>Spill阶段：当内存中的数据量达到一定的阀值的时候，就会将数据写入本地磁盘，在将数据写入磁盘之前需要对数据进行一次排序的操作，如果配置了combiner，还会将有相同分区号和key的数据进行排序。</li>
<li>Merge阶段：把所有溢出的临时文件进行一次合并操作，以确保一个MapTask最终只产生一个中间数据文件。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-57-34.png" alt="Map端Shuffle"></li>
</ul>
<h5 id="Reducer端shuffle"><a href="#Reducer端shuffle" class="headerlink" title="Reducer端shuffle"></a>Reducer端shuffle</h5><ul>
<li>Copy阶段：ReduceTask启动Fetcher线程到已经完成MapTask的节点上复制一份属于自己的数据。</li>
<li>Merge阶段：在ReduceTask远程复制数据的同时，会在后台开启两个线程对内存到本地的数据文件进行合并操作。</li>
<li>Sort阶段：在对数据进行合并的同时，会进行排序操作，由于MapTask阶段已经对数据进行了局部的排序，ReduceTask只需保证Copy的数据的最终整体有效性即可。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-17-23-58-30.png" alt="Reducer端shuffle"></li>
</ul>
<h5 id="shuffle机制弊端"><a href="#shuffle机制弊端" class="headerlink" title="shuffle机制弊端"></a>shuffle机制弊端</h5><ul>
<li>Shuffle是MapReduce程序的核心与精髓，是MapReduce的灵魂所在。</li>
<li>Shuffle也是MapReduce被诟病最多的地方所在。MapReduce相比较于Spark、Flink计算引擎慢的原因，跟Shuffle机制有很大的关系。</li>
<li>Shuffle中<strong>频繁涉及到数据在内存、磁盘之间的多次往复</strong>。</li>
</ul>
<h3 id="Hadoop-YARN"><a href="#Hadoop-YARN" class="headerlink" title="Hadoop YARN"></a>Hadoop YARN</h3><h4 id="Hadoop-YARN介绍"><a href="#Hadoop-YARN介绍" class="headerlink" title="Hadoop YARN介绍"></a>Hadoop YARN介绍</h4><h5 id="YARN简介"><a href="#YARN简介" class="headerlink" title="YARN简介"></a>YARN简介</h5><ul>
<li>Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的Hadoop资源管理器。</li>
<li>YARN是一个<code>通用</code><strong>资源管理系统</strong>和<strong>调度平台</strong>，可为上层应用提供统一的资源管理和调度。</li>
<li>它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-02-20.png" alt="YARN简介"></li>
</ul>
<h5 id="YARN功能说明"><a href="#YARN功能说明" class="headerlink" title="YARN功能说明"></a>YARN功能说明</h5><ul>
<li><strong>资源管理系统</strong>：集群的硬件资源，和程序运行相关，比如内存、CPU等。</li>
<li><strong>调度平台</strong>：多个程序同时申请计算资源如何分配，调度的规则（算法）。</li>
<li><strong>通用</strong>：不仅仅支持MapReduce程序，理论上<strong>支持各种计算程序</strong>。YARN不关心你干什么，只关心你要资源，在有的情况下给你，用完之后还我。</li>
</ul>
<h5 id="YARN概述"><a href="#YARN概述" class="headerlink" title="YARN概述"></a>YARN概述</h5><ul>
<li>可以把Hadoop YARN理解为相当于一个分布式的操作系统平台，而MapReduce等计算程序则相当于运行于操作系统之上的应用程序，<strong>YARN为这些程序提供运算所需的资源</strong>（内存、CPU等）。</li>
<li>Hadoop能有今天这个地位，YARN可以说是功不可没。因为有了YARN ，更多计算框架可以接入到HDFS中，而不单单是MapReduce，<strong>正是因为YARN的包容，使得其他计算框架能专注于计算性能的提升</strong>。</li>
<li>HDFS可能不是最优秀的大数据存储系统，但却是应用最广泛的大数据存储系统，YARN功不可没。</li>
</ul>
<h4 id="Hadoop-YARN架构、组件"><a href="#Hadoop-YARN架构、组件" class="headerlink" title="Hadoop YARN架构、组件"></a>Hadoop YARN架构、组件</h4><h5 id="YARN官方架构图"><a href="#YARN官方架构图" class="headerlink" title="YARN官方架构图"></a>YARN官方架构图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-05-28.png" alt="YARN官方架构图"></p>
<h5 id="官方架构图中出现的概念"><a href="#官方架构图中出现的概念" class="headerlink" title="官方架构图中出现的概念"></a>官方架构图中出现的概念</h5><ul>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-06-43.png" alt="官方架构图中出现的概念"></li>
<li>Client</li>
<li>Container容器（资源的抽象）</li>
</ul>
<h5 id="YARN3大组件"><a href="#YARN3大组件" class="headerlink" title="YARN3大组件"></a>YARN3大组件</h5><ul>
<li><p><strong>ResourceManager（RM）</strong><br>YARN集群中的主角色，决定系统中所有应用程序之间<strong>资源分配的最终权限，即最终仲裁者</strong>。<br>接收用户的作业提交，并通过NM分配、管理各个机器上的计算资源。</p>
</li>
<li><p><strong>NodeManager（NM）</strong><br>YARN中的从角色，一台机器上一个，负责<strong>管理本机器上的计算资源</strong>。<br>根据RM命令，启动Container容器、监视容器的资源使用情况。并且向RM主角色汇报资源使用情况。</p>
</li>
<li><p><strong>ApplicationMaster（AM）</strong><br>用户提交的每个应用程序均包含一个AM。<br><strong>应用程序内的“老大”</strong>，负责程序内部各阶段的资源申请，监督程序的执行情况。</p>
</li>
</ul>
<h4 id="程序提交YARN交互流程"><a href="#程序提交YARN交互流程" class="headerlink" title="程序提交YARN交互流程"></a>程序提交YARN交互流程</h4><h5 id="核心交互流程"><a href="#核心交互流程" class="headerlink" title="核心交互流程"></a>核心交互流程</h5><ul>
<li>MR作业提交Client—&gt;RM</li>
<li>资源的申请MrAppMaster—&gt;RM</li>
<li>MR作业状态汇报Container（Map|Reduce Task）—&gt;Container（MrAppMaster）</li>
<li>节点的状态汇报NM—&gt;RM</li>
</ul>
<h5 id="交互流程概述"><a href="#交互流程概述" class="headerlink" title="交互流程概述"></a>交互流程概述</h5><p>当用户向YARN 中提交一个应用程序后，YARN将分两个阶段运行该应用程序。</p>
<ul>
<li>第一个阶段是<strong>客户端申请资源启动运行本次程序的ApplicationMaster</strong>；</li>
<li>第二个阶段是由<strong>ApplicationMaster根据本次程序内部具体情况，为它申请资源，并监控它的整个运行过程</strong>，直到运行完成。</li>
</ul>
<h5 id="MR提交YARN交互流程"><a href="#MR提交YARN交互流程" class="headerlink" title="MR提交YARN交互流程"></a>MR提交YARN交互流程</h5><ul>
<li>第1步、用户通过客户端向YARN中ResourceManager提交应用程序（比如hadoop jar提交MR程序）。</li>
<li>第2步、ResourceManager为该应用程序分配第一个Container（容器），并与对应的NodeManager通信，要求它在这个Container中启动这个应用程序的ApplicationMaster。</li>
<li>第3步、ApplicationMaster启动成功之后，首先向ResourceManager注册并保持通信，这样用户可以直接通过ResourceManage查看应用程序的运行状态（处理了百分之几）。</li>
<li>第4步、AM为本次程序内部的各个Task任务向RM申请资源，并监控它的运行状态。</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-15-11.png" alt="YARN核心交互流程"></p>
<ul>
<li>第5步、一旦ApplicationMaster 申请到资源后，便与对应的NodeManager 通信，要求它启动任务。</li>
<li>第6步、NodeManager 为任务设置好运行环境后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。</li>
<li>第7步、各个任务通过某个RPC 协议向ApplicationMaster 汇报自己的状态和进度，以让ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC 向ApplicationMaster 查询应用程序的当前运行状态。</li>
<li>第8步、应用程序运行完成后，ApplicationMaster 向ResourceManager 注销并关闭自己。</li>
</ul>
<h4 id="YARN资源调度器Scheduler"><a href="#YARN资源调度器Scheduler" class="headerlink" title="YARN资源调度器Scheduler"></a>YARN资源调度器Scheduler</h4><h5 id="MR程序提交YARN交互流程"><a href="#MR程序提交YARN交互流程" class="headerlink" title="MR程序提交YARN交互流程"></a>MR程序提交YARN交互流程</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-24-33.png" alt="YARN资源调度器Scheduler"></p>
<h5 id="如何理解资源调度"><a href="#如何理解资源调度" class="headerlink" title="如何理解资源调度"></a>如何理解资源调度</h5><ul>
<li>在理想情况下，应用程序提出的请求将立即得到YARN批准。但是实际中，<strong>资源是有限的</strong>，并且在<strong>繁忙的群集上</strong>，应用程序通常将需要等待其某些请求得到满足。YARN调度程序的工作是<strong>根据一些定义的策略为应用程序分配资源</strong>。</li>
<li>在YARN中，负责给应用分配资源的就是<code>Scheduler</code>，它是ResourceManager的核心组件之一。Scheduler完全专用于调度作业，它无法跟踪应用程序的状态。</li>
<li>一般而言，调度是一个难题，并且没有一个“最佳”策略，为此，YARN提供了多种调度器和可配置的策略供选择。</li>
</ul>
<h5 id="调度器策略"><a href="#调度器策略" class="headerlink" title="调度器策略"></a>调度器策略</h5><ul>
<li><p>三种调度器<br><code>FIFO Scheduler</code>（先进先出调度器）、<code>Capacity Scheduler</code>（容量调度器）、<code>Fair Scheduler</code>（公平调度器）。</p>
</li>
<li><p>Apache版本YARN默认使用<code>Capacity Scheduler</code>。</p>
</li>
<li>如果需要使用其他的调度器，可以在yarn-site.xml中的yarn.resourcemanager.scheduler.class进行配置。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-27-16.png" alt="三种调度器"></li>
</ul>
<div class="tabs" id="diaoduqi"><ul class="nav-tabs"><li class="tab"><button type="button" data-href="#diaoduqi-1"><b>FIFO Scheduler</b></button></li><li class="tab"><button type="button" data-href="#diaoduqi-2"><b>Capacity Scheduler</b></button></li><li class="tab"><button type="button" data-href="#diaoduqi-3"><b>Fair Scheduler</b></button></li></ul><div class="tab-contents"><div class="tab-item-content" id="diaoduqi-1"><p><mark class="hl-label green">FIFO&nbsp;Scheduler概述</mark> </p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-28-28.png" alt="FIFO Schedule"></p>
<ul>
<li><code>FIFO Scheduler</code>是Hadoop1.x中JobTracker原有的调度器实现，此调度器在YARN中保留了下来。</li>
<li>FIFO Scheduler是一个<strong>先进先出</strong>的思想，即<strong>先提交的应用先运行</strong>。调度工作不考虑优先级和范围，适用于负载较低的小规模集群。当使用大型共享集群时，它的效率较低且会导致一些问题。</li>
<li>FIFO Scheduler拥有一个控制全局的队列queue，默认queue名称为default，该调度器会获取当前集群上所有的资源信息作用于这个全局的queue。</li>
</ul>
<p><mark class="hl-label green">FIFO&nbsp;Scheduler优势、坏处</mark> </p>
<ul>
<li><p>优势：<br>无需配置、先到先得、易于执行</p>
</li>
<li><p>坏处：<br>任务的优先级不会变高，因此高优先级的作业需要等待<br>不适合共享集群</p>
</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="diaoduqi-2"><p><mark class="hl-label green">Capacity&nbsp;Scheduler概述</mark> </p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-34-07.png" alt="Capacity Scheduler"></p>
<ul>
<li>Capacity Scheduler容量调度是<strong>Apache Hadoop3.x默认调度策略</strong>。该策略允许<strong>多个组织共享整个集群资源</strong>，每个组织可以获得集群的一部分计算能力。<strong>通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源</strong>，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。</li>
<li>Capacity可以理解成一个个的资源队列，这个资源队列是用户自己去分配的。队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略。</li>
</ul>
<p><mark class="hl-label green">Capacity&nbsp;Scheduler资源队列划分</mark> </p>
<p>Capacity Scheduler调度器以队列为单位划分资源。简单通俗点来说，就是一个个队列有独立的资源，队列的结构和资源是可以进行配置的。</p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-35-41.png" alt="Capacity Scheduler资源队列划分"></p>
<p><mark class="hl-label green">Capacity&nbsp;Scheduler特性优势</mark> </p>
<ul>
<li><p><strong>层次化的队列设计</strong>（Hierarchical Queues）<br>层次化的管理，可以更容易、更合理分配和限制资源的使用。</p>
</li>
<li><p><strong>容量保证</strong>（Capacity Guarantees）<br>每个队列上都可以设置一个资源的占比，保证每个队列都不会占用整个集群的资源。</p>
</li>
<li><p><strong>安全</strong>（Security）<br>每个队列有严格的访问控制。用户只能向自己的队列里面提交任务，而且不能修改或者访问其他队列的任务。</p>
</li>
<li><p><strong>弹性分配</strong>（Elasticity）<br>空闲的资源可以被分配给任何队列。<br>当多个队列出现争用的时候，则会按照权重比例进行平衡。</p>
</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div><div class="tab-item-content" id="diaoduqi-3"><p><mark class="hl-label green">Fair&nbsp;Scheduler概述</mark> </p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2022-11-18-00-38-13.png" alt="Fair Scheduler概述"></p>
<ul>
<li><strong>Fair Scheduler叫做公平调度</strong>，提供了YARN应用程序<strong>公平地共享大型集群中资源</strong>的另一种方式。使所有应用在平均情况下随着时间的流逝可以获得相等的资源份额。</li>
<li>Fair Scheduler设计目标是为所有的应用分配公平的资源（对公平的定义通过参数来设置）。</li>
<li>公平调度可以在多个队列间工作，允许资源共享和抢占。</li>
</ul>
<p><mark class="hl-label green">如何理解公平共享</mark> </p>
<ul>
<li>有两个用户A和B，每个用户都有自己的队列。</li>
<li>A启动一个作业，由于没有B的需求，它分配了集群所有可用的资源。</li>
<li>然后B在A的作业仍在运行时启动了一个作业，经过一段时间，A,B各自作业都使用了一半的资源。</li>
<li>现在，如果B用户在其他作业仍在运行时开始第二个作业，它将与B的另一个作业共享其资源，因此B的每个作业将拥有资源的四分之一，而A的继续将拥有一半的资源。结果是资源在用户之间公平地共享。</li>
</ul>
<p><mark class="hl-label green">Fair&nbsp;特性优势</mark> </p>
<ul>
<li><strong>分层队列</strong>：队列可以按层次结构排列以划分资源，并可以配置权重以按特定比例共享集群。</li>
<li><strong>基于用户或组的队列映射</strong>：可以根据提交任务的用户名或组来分配队列。如果任务指定了一个队列,则在该队列中提交任务。</li>
<li><strong>资源抢占</strong>：根据应用的配置，抢占和分配资源可以是友好的或是强制的。默认不启用资源抢占。</li>
<li><strong>保证最小配额</strong>：可以设置队列最小资源，允许将保证的最小份额分配给队列，保证用户可以启动任务。当队列不能满足最小资源时,可以从其它队列抢占。当队列资源使用不完时,可以给其它队列使用。这对于确保某些用户、组或生产应用始终获得足够的资源。</li>
<li><strong>允许资源共享</strong>：即当一个应用运行时,如果其它队列没有任务执行,则可以使用其它队列,当其它队列有应用需要资源时再将占用的队列释放出来。所有的应用都从资源队列中分配资源。</li>
<li><strong>默认不限制每个队列和用户可以同时运行应用的数量</strong>。可以配置来限制队列和用户并行执行的应用数量。限制并行执行应用数量不会导致任务提交失败,超出的应用会在队列中等待。</li>
</ul><button type="button" class="tab-to-top" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div></div>
<h2 id="数据仓库基础与Apache-Hive入门"><a href="#数据仓库基础与Apache-Hive入门" class="headerlink" title="数据仓库基础与Apache Hive入门"></a>数据仓库基础与Apache Hive入门</h2><h3 id="数据仓库基本概念"><a href="#数据仓库基本概念" class="headerlink" title="数据仓库基本概念"></a>数据仓库基本概念</h3><h4 id="数据仓库概念"><a href="#数据仓库概念" class="headerlink" title="数据仓库概念"></a>数据仓库概念</h4><h5 id="数仓概念"><a href="#数仓概念" class="headerlink" title="数仓概念"></a>数仓概念</h5><ul>
<li>数据仓库（Data Warehouse，简称<strong>数仓、DW</strong>）,是一个<strong>用于存储、分析、报告的数据系统</strong>。</li>
<li>数据仓库的目的是构建<strong>面向分析</strong>的集成化数据环境，分析结果为企业提供决策支持（Decision Support）。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-23-10.png" alt="数仓"></li>
</ul>
<div class="note red icon-padding modern"><i class="note-icon fas fa-question"></i><p>1.假如你现在手里有2000w,当下的时间点去投资口罩生产，你做不做？能不能赚钱？<br>2.假如你是公司营销总监，是否愿意招聘女主播进行短视频带货直播销售？</p>
</div>
<h5 id="数仓专注分析"><a href="#数仓专注分析" class="headerlink" title="数仓专注分析"></a>数仓专注分析</h5><ul>
<li>数据仓库<strong>本身并不“生产”任何数据</strong>，其数据来源于不同外部系统；</li>
<li>同时数据仓库自身<strong>也不需要“消费”任何的数据</strong>，其结果开放给各个外部应用使用；</li>
<li>这也是为什么叫“仓库”，而不叫“工厂”的原因。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-31-34.png" alt="数仓专注分析"></li>
</ul>
<h4 id="场景案例：数据仓库为何而来"><a href="#场景案例：数据仓库为何而来" class="headerlink" title="场景案例：数据仓库为何而来"></a>场景案例：数据仓库为何而来</h4><div class="note info modern"><p>数仓<strong>为了分析数据而来</strong>，分析结果给企业决策提供支撑。</p>
</div>
<p>下面以中国人寿保险公司（chinalife）发展为例，阐述数据仓库为何而来。<br><strong>1.业务数据存储问题</strong></p>
<ul>
<li>中国人寿保险（集团）公司下辖多条业务线，包括：人寿险、财险、车险，养老险等。各业务线的业务正常运营需要记录维护包括客户、保单、收付费、核保、理赔等信息。这么多<strong>业务数据存储在哪里呢</strong>？</li>
<li><mark class="hl-label red">联机事务处理系统（OLTP）</mark> 正好可以满足上述业务需求开展, 其主要任务是执行联机事务处理。其基本特征是<strong>前台接收的用户数据可以立即传送到后台进行处理，并在很短的时间内给出处理结果</strong>。</li>
<li><mark class="hl-label red">关系型数据库（RDBMS）是OLTP典型应用</mark> ，比如：Oracle、MySQL、SQL Server等。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-36-36.png" alt="人寿例子"></li>
</ul>
<p><strong>2.分析型决策的制定</strong><br>随着集团业务的持续运营，<strong>业务数据将会越来越多</strong>。由此也产生出许多运营相关的困惑：</p>
<ul>
<li>能够确定哪些险种正在恶化或已成为不良险种？</li>
<li>能够用有效的方式制定新增和续保的政策吗？</li>
<li>理赔过程有欺诈的可能吗？</li>
<li>现在得到的报表是否只是某条业务线的？集团整体层面数据如何？</li>
</ul>
<p>为了能够正确认识这些问题，制定相关的解决措施，瞎拍桌子是肯定不行的。<br>最稳妥办法就是：<strong>基于业务数据开展数据分析，基于分析的结果给决策提供支撑</strong>。也就是所谓的数据驱动决策的制定。</p>
<h5 id="OLTP环境开展分析可行吗？"><a href="#OLTP环境开展分析可行吗？" class="headerlink" title="OLTP环境开展分析可行吗？"></a>OLTP环境开展分析可行吗？</h5><p><strong>可以，但是没必要</strong>!</p>
<p>OLTP系统的核心是面向业务，支持业务，支持事务。所有的业务操作可以分为读、写两种操作，一般来说<strong>读的压力明显大于写的压力</strong>。如果在OLTP环境直接开展各种分析，有以下问题需要考虑：</p>
<ul>
<li>数据分析也是对数据进行读取操作，<strong>会让读取压力倍增</strong>；</li>
<li>OLTP<strong>仅存储数周或数月的数据</strong>；</li>
<li><strong>数据分散</strong>在不同系统不同表中，字段类型属性不统一；</li>
</ul>
<h5 id="数据仓库面世"><a href="#数据仓库面世" class="headerlink" title="数据仓库面世"></a>数据仓库面世</h5><ul>
<li>当分析所涉及数据规模较小的时候，在业务低峰期时可以在OLTP系统上开展直接分析。</li>
<li>但<strong>为了更好的进行各种规模的数据分析，同时也不影响OLTP系统运行，此时需要构建一个集成统一的数据分析平台</strong>。该平台的目的很简单：<strong>面向分析，支持分析</strong>，并且和OLTP系统解耦合。</li>
<li>基于这种需求，数据仓库的雏形开始在企业中出现了。</li>
</ul>
<h5 id="数据仓库的构建"><a href="#数据仓库的构建" class="headerlink" title="数据仓库的构建"></a>数据仓库的构建</h5><ul>
<li>就如数仓定义所说,<strong>数仓是一个用于存储、分析、报告的数据系统</strong>，目的是<strong>构建面向分析的集成化数据环境</strong>。我们把这种<strong>面向分析、支持分析的系统</strong>称之为<mark class="hl-label red">OLAP（联机分析处理）系统</mark> 。当然，数据仓库是OLAP系统的一种实现。</li>
<li>中国人寿保险公司就可以基于分析决策需求，构建数仓平台。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-46-08.png" alt="数仓构建"></li>
</ul>
<h4 id="数据仓库主要特征"><a href="#数据仓库主要特征" class="headerlink" title="数据仓库主要特征"></a>数据仓库主要特征</h4><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-47-19.png" alt="数据仓库主要特征"></p>
<p>1.<strong>面向主题性(Subject-Oriented)</strong></p>
<ul>
<li>主题是一个抽象的概念，是较高层次上企业信息系统中的<strong>数据综合、归类</strong>并进行分析利用的抽象。在逻辑意义上，它是对应企业中某一宏观分析领域所涉及的分析对象。</li>
<li>传统OLTP系统对数据的划分并不适用于决策分析。而基于主题组织的数据则不同，它们被划分为各自独立的领域，每个领域有各自的逻辑内涵但互不交叉，在<strong>抽象层次上对数据进行完整、一致和准确的描述</strong>。</li>
</ul>
<p>2.<strong>集成性(Integrated)</strong></p>
<ul>
<li>主题相关的<strong>数据通常会分布在多个操作型系统中，彼此分散、独立、异构</strong>。</li>
<li>因此在数据进入数据仓库之前，必然要经过<strong>统一与综合，对数据进行抽取、清理、转换和汇总</strong>，这一步是数据仓库建设中最关键、最复杂的一步，需要完成的工作有：<ul>
<li>要<strong>统一源数据中所有矛盾之处</strong>；<br>如字段的同名异义、异名同义、单位不统一、字长不一致等等。</li>
<li>进行<strong>数据综合和计算</strong>。<br>数据仓库中的数据综合工作可以在从原有数据库抽取数据时生成，但许多是在数据仓库内部生成的，即进入数据仓库以后进行综合生成的。</li>
</ul>
</li>
<li>下图说明了保险公司综合数据的简单处理过程，其中数据仓库中与“承保”主题有关的数据来自于多个不同的操作型系统。</li>
<li>这些系统内部数据的命名可能不同，数据格式也可能不同。把不同来源的数据存储到数据仓库之前，需要去除这些不一致。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-17-55-35.png" alt="集成性说明"></li>
</ul>
<p>3.<strong>非易失性、非异变性(Non-Volatile)</strong></p>
<ul>
<li><strong>数据仓库是分析数据的平台，而不是创造数据的平台</strong>。我们是通过数仓去分析数据中的规律，而不是去创造修改其中的规律。因此数据进入数据仓库后，它便稳定且不会改变。</li>
<li><strong>数据仓库的数据反映的是一段相当长的时间内历史数据的内容</strong>，数据仓库的用户对数据的操作大多是数据查询或比较复杂的挖掘，一旦数据进入数据仓库以后，一般情况下被较长时间保留。</li>
<li><strong>数据仓库中一般有*</strong>大量的查询操作<em>，<em>*但修改和删除操作很少。</em></em></li>
</ul>
<p>4.<strong>时变性(Time-Variant)</strong></p>
<ul>
<li>数据仓库包含各种粒度的<strong>历史数据</strong>，数据可能与某个特定日期、星期、月份、季度或者年份有关。</li>
<li>当业务变化后会失去时效性。因此数据仓库的<strong>数据需要随着时间更新，以适应决策的需要</strong>。</li>
<li>从这个角度讲，数据仓库建设是一个项目，更是一个过程。</li>
</ul>
<h4 id="数据仓库主流开发语言—SQL"><a href="#数据仓库主流开发语言—SQL" class="headerlink" title="数据仓库主流开发语言—SQL"></a>数据仓库主流开发语言—SQL</h4><h5 id="数仓开发语言概述"><a href="#数仓开发语言概述" class="headerlink" title="数仓开发语言概述"></a>数仓开发语言概述</h5><ul>
<li>数仓作为面向分析的数据平台，其主职工作就是对存储在其中的数据开展分析，那么如何读取数据分析呢？</li>
<li>理论上来说，<strong>任何一款编程语言只要具备读写数据、处理数据的能力，都可以用于数仓的开发</strong>。比如C、java、Python等；</li>
<li><strong>关键在于编程语言是否易学、好用、功能是否强大</strong>。遗憾的是上面所列出的C、Python等编程语言都需要一定的时间进行语法的学习，并且学习语法之后还需要结合分析的业务场景进行编码，跑通业务逻辑。</li>
<li>不管从学习成本还是开发效率来说，上述所说的编程语言都不是十分友好的。</li>
<li>在数据分析领域，不得不提的就是<strong>SQL编程语言，应该称之为分析领域主流开发语言</strong>。</li>
</ul>
<h5 id="SQL语言介绍"><a href="#SQL语言介绍" class="headerlink" title="SQL语言介绍"></a>SQL语言介绍</h5><ul>
<li><strong>结构化查询语言</strong>(Structured Query Language) 简称<code>SQL</code>，是一种数据库查询和程序设计语言，用于<strong>存取</strong>数据以及<strong>查询</strong>、<strong>更新</strong>和<strong>管理</strong>数据。</li>
<li>SQL语言使我们有能力访问数据库，并且SQL是一种ANSI（美国国家标准化组织）的<strong>标准计算机语言</strong>，各大数据库厂商在生产数据库软件的时候，几乎都会去支持SQL的语法，以使得用户在使用软件时更加容易上手，以及在不同厂商软件之间进行切换时更加适应，因为大家的SQL语法都差不多。</li>
<li>SQL语言<strong>功能很强</strong>，十分简洁，核心功能只用了9个动词。语法接近英语口语，所以，很容易学习和使用。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-03-00.png" alt="SQL语言功能很强"></li>
</ul>
<h5 id="数仓与SQL"><a href="#数仓与SQL" class="headerlink" title="数仓与SQL"></a>数仓与SQL</h5><ul>
<li>虽然SQL语言本身是针对数据库软件设计的，但是在<strong>数据仓库领域</strong>，尤其是大数据数仓领域，很多数仓软件<strong>都会去支持SQL语法</strong>；</li>
<li>原因在于一是用户<strong>学习SQL成本低</strong>，二是SQL语言对于<strong>数据分析真的十分友好，爱不释手</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-04-48.png" alt="SQL语言"></li>
</ul>
<div class="note red icon-padding modern"><i class="note-icon fas fa-question"></i><p>1.SQL全称叫做结构化查询语言，结构化是什么意思？<br>2.有没有非结构化之说？</p>
</div>
<h5 id="结构化数据"><a href="#结构化数据" class="headerlink" title="结构化数据"></a>结构化数据</h5><ul>
<li><strong>结构化数据</strong>也称作行数据，是由<strong>二维表结构来逻辑表达和实现的数据</strong>，严格地遵循数据格式与长度规范，主要通过关系型数据库进行存储和管理。</li>
<li>与结构化数据相对的是不适于由数据库二维表来表现的<strong>非结构化数据</strong>，包括所有格式的办公文档、XML、HTML、各类报表、图片和音频、视频信息等。</li>
<li>通俗来说，结构化数据会有严格的行列对齐，便于解读与理解。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-07-07.png" alt="结构化数据与非结构化数据"></li>
</ul>
<h5 id="二维表结构"><a href="#二维表结构" class="headerlink" title="二维表结构"></a>二维表结构</h5><ul>
<li>表由一个名字标识（例如“客户”或者“订单”），叫做表名。表包含带有数据的记录（行）。</li>
<li>下面的例子是一个名为“Persons” 的表，包含三条记录（每一条对应一个人）和五个列（Id、姓、名、地址和城市）。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-08-09.png" alt="二维表结构"></li>
</ul>
<h5 id="SQL语法分类"><a href="#SQL语法分类" class="headerlink" title="SQL语法分类"></a>SQL语法分类</h5><div class="note info modern"><p>SQL主要语法分为两个部分：<strong>数据定义语言(DDL)</strong>和<strong>数据操纵语言(DML)</strong> 。</p>
</div>
<ul>
<li><p>DDL语法使我们有能力<strong>创建或删除表</strong>，以及数据库、索引等各种对象，但是不涉及表中具体数据操作：<br><mark class="hl-label red">CREATE</mark> DATABASE -创建新数据库<br>CREATE TABLE -创建新表</p>
</li>
<li><p>DML语法是我们有能力针对<strong>表中的数据进行插入、更新、删除、查询</strong>操作：<br><mark class="hl-label red">SELECT</mark>  -从数据库表中获取数据<br><mark class="hl-label red">UPDATE</mark>  -更新数据库表中的数据<br><mark class="hl-label red">DELETE</mark>  -从数据库表中删除数据<br><mark class="hl-label red">INSERT</mark>  -向数据库表中插入数据</p>
</li>
</ul>
<h3 id="Apache-Hive入门"><a href="#Apache-Hive入门" class="headerlink" title="Apache Hive入门"></a>Apache Hive入门</h3><h4 id="Apache-Hive概述"><a href="#Apache-Hive概述" class="headerlink" title="Apache Hive概述"></a>Apache Hive概述</h4><h5 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h5><ul>
<li>Apache Hive是一款建立在Hadoop之上的开源<strong>数据仓库</strong>系统，可以将存储在Hadoop文件中的<strong>结构化、半结构化数据文件映射为一张数据库表</strong>，基于表提供了一种类似SQL的查询模型，称为<strong>Hive查询语言（HQL）</strong>，用于访问和分析存储在Hadoop文件中的大型数据集。</li>
<li>Hive核心是将<strong>HQL转换为MapReduce程序</strong>，然后将程序提交到Hadoop群集执行。</li>
<li>Hive由Facebook实现并开源。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-15-37.png" alt="Hive"></li>
</ul>
<h5 id="为什么使用Hive"><a href="#为什么使用Hive" class="headerlink" title="为什么使用Hive"></a>为什么使用Hive</h5><ul>
<li><p>使用Hadoop MapReduce直接处理数据所面临的问题<br>人员学习成本太高需要掌握java语言<br>MapReduce实现复杂查询逻辑开发难度太大</p>
</li>
<li><p>使用Hive处理数据的好处<br>操作接口采用<strong>类SQL语法</strong>，提供快速开发的能力（简单、容易上手）<br>避免直接写MapReduce，减少开发人员的学习成本<br>支持自定义函数，功能扩展很方便<br>背靠Hadoop，<strong>擅长存储分析海量数据集</strong></p>
</li>
</ul>
<h5 id="Hive和Hadoop关系"><a href="#Hive和Hadoop关系" class="headerlink" title="Hive和Hadoop关系"></a>Hive和Hadoop关系</h5><ul>
<li><p>从功能来说，数据仓库软件，至少需要具备下述两种能力：<br>存储数据的能力、分析数据的能力</p>
</li>
<li><p>Apache Hive作为一款大数据时代的数据仓库软件，当然也具备上述两种能力。只不过Hive并不是自己实现了上述两种能力，而是借助Hadoop。<br><strong>Hive利用HDFS存储数据，利用MapReduce查询分析数据</strong>。</p>
</li>
<li><p>这样突然发现Hive没啥用，不过是套壳Hadoop罢了。其实不然，Hive的最大的魅力在于<strong>用户专注于编写HQL，Hive帮您转换成为MapReduce程序完成对数据的分析</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-18-50.png" alt="Hive和Hadoop关系"></p>
</li>
</ul>
<h4 id="场景设计：如何模拟实现Hive功能"><a href="#场景设计：如何模拟实现Hive功能" class="headerlink" title="场景设计：如何模拟实现Hive功能"></a>场景设计：如何模拟实现Hive功能</h4><div class="note red icon-padding modern"><i class="note-icon fas fa-question"></i><p>如果我们来设计Hive这款软件，要求能够实现用户只编写sql语句，Hive自动将<strong>sql转换MapReduce程序</strong>，处理位于HDFS上的结构化数据。如何实现？</p>
</div>
<mark class="hl-label green">案例：如何模拟实现Apache&nbsp;Hive的功能</mark> 
<p>在HDFS文件系统上有一个文件，路径为/data/china_user.txt；<br>需求：统计来自于上海年龄大于25岁的用户有多少个？<br><img class="inline-img" src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-33-08.png" style="height:150px"></p>
<div class="note icon-padding modern"><i class="note-icon fas fa-question"></i><p>Hive能将数据文件映射成为一张表，这个<strong>映射</strong>是指什么？<br>Hive软件本身到底承担了什么<strong>功能职责</strong>？</p>
</div>
<h5 id="映射信息记录"><a href="#映射信息记录" class="headerlink" title="映射信息记录"></a>映射信息记录</h5><ul>
<li><strong>映射</strong>在数学上称之为一种<strong>对应关系</strong>，比如y=x+1，对于每一个x的值都有与之对应的y的值。</li>
<li>在hive中<strong>能够写sql处理的前提是针对表，而不是针对文件</strong>，因此需要将<strong>文件和表之间的对应关系</strong>描述记录清楚。映射信息专业的叫法称之为<strong>元数据信息</strong>（元数据是指用来描述数据的数据metadata）。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-36-11.png" alt="映射信息记录"></li>
<li>具体来看，要记录的元数据信息包括：<ul>
<li>表对应着哪个文件（位置信息）</li>
<li>表的列对应着文件哪一个字段（顺序信息）</li>
<li>文件字段之间的分隔符是什么</li>
</ul>
</li>
</ul>
<h5 id="SQL语法解析、编译"><a href="#SQL语法解析、编译" class="headerlink" title="SQL语法解析、编译"></a>SQL语法解析、编译</h5><ul>
<li>用户写完sql之后，hive需要针对sql进行语法校验，并且根据记录的元数据信息解读sql背后的含义，制定执行计划。</li>
<li>并且把执行计划转换成MapReduce程序来具体执行，把执行的结果封装返回给用户。</li>
</ul>
<h5 id="对Hive的理解"><a href="#对Hive的理解" class="headerlink" title="对Hive的理解"></a>对Hive的理解</h5><ul>
<li><p>Hive能将数据文件映射成为一张表，这个<strong>映射</strong>是指什么？<br><strong>文件和表之间的对应关系</strong></p>
</li>
<li><p>Hive软件本身到底承担了什么<strong>功能职责</strong>？<br><strong>SQL语法解析编译成为MapReduce</strong></p>
</li>
<li><p>基于上述分析，最终要想模拟实现的Hive的功能，大致需要下图所示组件参与其中。</p>
</li>
<li>从中可以感受一下Hive承担了什么职责，当然，也可以把这个理解为Hive的架构图。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-10-18-41-04.png" alt="对Hive的理解"></li>
</ul>
<h4 id="Apache-Hive架构、组件"><a href="#Apache-Hive架构、组件" class="headerlink" title="Apache Hive架构、组件"></a>Apache Hive架构、组件</h4><h5 id="Hive架构图"><a href="#Hive架构图" class="headerlink" title="Hive架构图"></a>Hive架构图</h5><p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-04-11.png" alt="Hive架构图"></p>
<h5 id="Hive组件"><a href="#Hive组件" class="headerlink" title="Hive组件"></a>Hive组件</h5><ul>
<li><p><strong>用户接口</strong><br>包括CLI、JDBC/ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；Hive中的Thrift服务器允许外部客户端通过网络与Hive进行交互，类似于JDBC或ODBC协议。WebGUI是通过浏览器访问Hive。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-07-09.png" alt="用户接口"></p>
</li>
<li><p><strong>元数据存储</strong><br>通常是存储在关系数据库如mysql/derby中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-08-07.png" alt="元数据存储"></p>
</li>
<li><p><strong>Driver驱动程序，包括语法解析器、计划编译器、优化器、执行器</strong><br>完成HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有执行引擎调用执行。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-08-20.png" alt="Driver驱动程序"></p>
</li>
<li><p><strong>执行引擎</strong><br>Hive本身并不直接处理数据文件。而是通过执行引擎处理。当下Hive支持MapReduce、Tez、Spark3种执行引擎。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-08-37.png" alt="执行引擎"></p>
</li>
</ul>
<h3 id="Apache-Hive安装部署"><a href="#Apache-Hive安装部署" class="headerlink" title="Apache Hive安装部署"></a>Apache Hive安装部署</h3><h4 id="Apache-Hive元数据"><a href="#Apache-Hive元数据" class="headerlink" title="Apache Hive元数据"></a>Apache Hive元数据</h4><p><strong>元数据</strong>（Metadata），又称中介数据、中继数据，为<strong>描述数据的数据</strong>（data about data），主要是描述数据属性（property）的信息，用来支持如指示存储位置、历史数据、资源查找、文件记录等功能。</p>
<h5 id="Hive-Metadata"><a href="#Hive-Metadata" class="headerlink" title="Hive Metadata"></a>Hive Metadata</h5><ul>
<li><strong>Hive Metadata即Hive的元数据</strong>。</li>
<li>包含用Hive创建的database、table、表的位置、类型、属性，字段顺序类型等元信息。</li>
<li><strong>元数据存储在关系型数据库中</strong>。如hive内置的Derby、或者第三方如MySQL等。</li>
<li>Metastore即<strong>元数据服务</strong>。Metastore服务的作用是<strong>管理metadata元数据</strong>，对外暴露服务地址，让各种客户端通过连接metastore服务，由metastore再去连接MySQL数据库来存取元数据。</li>
<li>有了metastore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySQL数据库的用户名和密码，只需要连接metastore 服务即可。某种程度上也保证了hive元数据的安全。</li>
<li><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-15-15.png" alt="Hive Metadata"></li>
</ul>
<h5 id="metastore配置方式"><a href="#metastore配置方式" class="headerlink" title="metastore配置方式"></a>metastore配置方式</h5><ul>
<li>metastore服务配置有3种模式：内嵌模式、本地模式、<strong>远程模式</strong>。</li>
<li>区分3种配置方式的关键是弄清楚两个问题：<ul>
<li>Metastore服务是否需要单独配置、单独启动？</li>
<li>Metadata是存储在内置的derby中，还是第三方RDBMS,比如MySQL。</li>
</ul>
</li>
<li>企业推荐模式—远程模式部署。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">内嵌模式</th>
<th style="text-align:center">本地模式</th>
<th style="text-align:center">远程模式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Metastore单独配置、启动</td>
<td style="text-align:center">否</td>
<td style="text-align:center">否</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">Metastore存储介质</td>
<td style="text-align:center">Derby</td>
<td style="text-align:center">Mysql</td>
<td style="text-align:center">Mysql</td>
</tr>
</tbody>
</table>
</div>
<h5 id="metastore远程模式"><a href="#metastore远程模式" class="headerlink" title="metastore远程模式"></a>metastore远程模式</h5><p>在生产环境中，建议用远程模式来配置Hive Metastore。在这种情况下，其他依赖hive的软件都可以通过Metastore访问hive。由于还可以完全屏蔽数据库层，因此这也带来了更好的可管理性/安全性。<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-12-22-24-04.png" alt="metastore远程模式"></p>
<h4 id="Apache-Hive部署实战"><a href="#Apache-Hive部署实战" class="headerlink" title="Apache Hive部署实战"></a>Apache Hive部署实战</h4><mark class="hl-label pink">安装前准备</mark> 
<ul>
<li>由于Apache Hive是一款基于Hadoop的数据仓库软件，通常部署运行在Linux系统之上。因此不管使用何种方式配置Hive Metastore，必须要先保证服务器的基础环境正常，Hadoop集群健康可用。</li>
<li><strong>服务器基础环境</strong><br>集群时间同步、防火墙关闭、主机Host映射、免密登录、JDK安装</li>
<li><strong>Hadoop集群健康可用</strong><br><strong><em>启动Hive之前必须先启动Hadoop集群</em></strong>。特别要注意，需等待HDFS安全模式关闭之后再启动运行Hive。<br>Hive不是分布式安装运行的软件，其分布式的特性主要借由Hadoop完成。包括分布式存储、分布式计算。</li>
</ul>
<mark class="hl-label pink">Hadoop与Hive整合</mark> 
<ul>
<li>因为Hive需要把数据存储在HDFS上，并且通过MapReduce作为执行引擎处理数据；</li>
<li>因此需要在Hadoop中添加相关配置属性，以满足Hive在Hadoop上运行。</li>
<li>修改Hadoop中<code>core-site.xml</code>，并且Hadoop集群同步配置文件，重启生效。</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--整合hive --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step1：MySQL安装</mark> 
<div class="note warning modern"><p>MySQL只需要在一台机器安装并且需要授权远程访问</p>
</div>
<figure class="highlight bash"><figcaption><span>安装mysql</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@hsq01 ~]<span class="comment"># mkdir mysoft/mysql</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#上传 mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar 到上述文件夹下  解压</span></span><br><span class="line"><span class="built_in">cd</span> mysoft/mysql</span><br><span class="line">tar xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"></span><br><span class="line"><span class="comment">#执行安装</span></span><br><span class="line">yum -y install libaio</span><br><span class="line"></span><br><span class="line">rpm -ivh mysql-community-common-5.7.29-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-libs-5.7.29-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-client-5.7.29-1.el7.x86_64.rpm --force --nodeps</span><br><span class="line">rpm -ivh mysql-community-server-5.7.29-1.el7.x86_64.rpm --force --nodeps</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><figcaption><span>mysql初始化设置</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化</span></span><br><span class="line">mysqld --initialize</span><br><span class="line"></span><br><span class="line"><span class="comment">#更改所属组</span></span><br><span class="line"><span class="built_in">chown</span> mysql:mysql /var/lib/mysql -R</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动mysql</span></span><br><span class="line">systemctl start mysqld.service</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看生成的临时root密码</span></span><br><span class="line"><span class="built_in">cat</span>  /var/log/mysqld.log</span><br><span class="line"></span><br><span class="line">[Note] A temporary password is generated <span class="keyword">for</span> root@localhost: o+TU+KDOm004</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><figcaption><span>修改root密码 授权远程访问 设置开机自启动</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[root@hsq01 ~]<span class="comment"># mysql -u root -p</span></span><br><span class="line">Enter password:     <span class="comment">#这里输入在日志中生成的临时密码</span></span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection <span class="built_in">id</span> is 3</span><br><span class="line">Server version: 5.7.29</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;help;&#x27;</span> or <span class="string">&#x27;\h&#x27;</span> <span class="keyword">for</span> <span class="built_in">help</span>. Type <span class="string">&#x27;\c&#x27;</span> to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment">#更新root密码  设置为hadoop</span></span><br><span class="line">mysql&gt; alter user user() identified by <span class="string">&quot;hadoop&quot;</span>;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line"><span class="comment">#授权</span></span><br><span class="line">mysql&gt; use mysql;</span><br><span class="line"></span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON *.* TO <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED BY <span class="string">&#x27;hadoop&#x27;</span> WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">mysql&gt; FLUSH PRIVILEGES; </span><br><span class="line"></span><br><span class="line"><span class="comment">#mysql的启动和关闭 状态查看 （这几个命令必须记住）</span></span><br><span class="line">systemctl stop mysqld</span><br><span class="line">systemctl status mysqld</span><br><span class="line">systemctl start mysqld</span><br><span class="line"></span><br><span class="line"><span class="comment">#建议设置为开机自启动服务</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># systemctl enable  mysqld</span></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/mysqld.service to /usr/lib/systemd/system/mysqld.service.</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看是否已经设置自启动成功</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># systemctl list-unit-files | grep mysqld</span></span><br><span class="line">mysqld.service                                enabled</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><figcaption><span>干净卸载mysql 5.7</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#关闭mysql服务</span></span><br><span class="line">systemctl stop mysqld.service</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找安装mysql的rpm包</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># rpm -qa | grep -i mysql</span></span><br><span class="line">mysql-community-libs-5.7.29-1.el7.x86_64</span><br><span class="line">mysql-community-common-5.7.29-1.el7.x86_64</span><br><span class="line">mysql-community-client-5.7.29-1.el7.x86_64</span><br><span class="line">mysql-community-server-5.7.29-1.el7.x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment">#卸载</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># yum remove mysql-community-libs-5.7.29-1.el7.x86_64 mysql-community-common-5.7.29-1.el7.x86_64 mysql-community-client-5.7.29-1.el7.x86_64 mysql-community-server-5.7.29-1.el7.x86_64</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看是否卸载干净</span></span><br><span class="line">rpm -qa | grep -i mysql</span><br><span class="line"></span><br><span class="line"><span class="comment">#查找mysql相关目录 删除</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># find / -name mysql</span></span><br><span class="line">/var/lib/mysql</span><br><span class="line">/var/lib/mysql/mysql</span><br><span class="line">/usr/share/mysql</span><br><span class="line"></span><br><span class="line">[root@hsq01 ~]<span class="comment"># rm -rf /var/lib/mysql</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># rm -rf /var/lib/mysql/mysql</span></span><br><span class="line">[root@hsq01 ~]<span class="comment"># rm -rf /usr/share/mysql</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#删除默认配置 日志</span></span><br><span class="line"><span class="built_in">rm</span> -rf /etc/my.cnf</span><br><span class="line"><span class="built_in">rm</span> -rf /var/log/mysqld.log</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step2：上传解压Hive安装包（node1安装即可）</mark> 
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /mysoft</span><br><span class="line"></span><br><span class="line"><span class="comment">#上传 apache-hive-3.1.2-bin.tar.gz 到上述文件夹下  解压</span></span><br><span class="line">tar zxvf apache-hive-3.1.2-bin.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">#解决Hive与Hadoop之间guava版本差异</span></span><br><span class="line"><span class="built_in">cd</span> /mysoft/apache-hive-3.1.2-bin/</span><br><span class="line"><span class="built_in">rm</span> -rf lib/guava-19.0.jarcp</span><br><span class="line"><span class="built_in">cp</span> <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/lib/guava-27.0-jre.jar ./lib/</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step3：修改hive-env.sh</mark> 
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /mysoft/apache-hive-3.1.2-bin/conf</span><br><span class="line"><span class="built_in">mv</span> hive-env.sh.template hive-env.sh</span><br><span class="line"></span><br><span class="line">vi hive-env.sh</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/local/hadoop-3.3.4</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/mysoft/apache-hive-3.1.2-bin/conf</span><br><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=/mysoft/apache-hive-3.1.2-bin/lib</span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step4：新增hive-site.xml</mark> 
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 存储元数据mysql相关配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hsq01:3306/hive3?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=UTF-8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- H2S运行绑定host --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hsq01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 远程模式部署metastore metastore地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hsq01:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭元数据存储授权  --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<mark class="hl-label pink">Step5：添加驱动、初始化</mark> 
<ul>
<li>上传MySQL JDBC驱动到Hive安装包lib路径下<code>mysql-connector-java-5.1.32.jar</code></li>
<li>初始化Hive的元数据<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /mysoft/apache-hive-3.1.2-bin/</span><br><span class="line"></span><br><span class="line">bin/schematool -initSchema -dbType mysql -verbos</span><br><span class="line"><span class="comment">#初始化成功会在mysql中创建74张表</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="metastore服务启动方式"><a href="#metastore服务启动方式" class="headerlink" title="metastore服务启动方式"></a>metastore服务启动方式</h5><mark class="hl-label green">前台启动</mark> 
<p><strong>前台启动</strong>，进程会一直占据终端，<code>ctrl + c</code>结束进程，服务关闭。<br>可以根据需求添加参数开启debug日志，获取详细日志信息，便于排错。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#前台启动  关闭ctrl+c</span></span><br><span class="line">/mysoft/apache-hive-3.1.2-bin/bin/hive --service metastore</span><br><span class="line"></span><br><span class="line"><span class="comment">#前台启动开启debug日志</span></span><br><span class="line">/mysoft/apache-hive-3.1.2-bin/bin/hive --service metastore --hiveconf hive.root.logger=DEBUG,console</span><br><span class="line"></span><br><span class="line"><span class="comment">#前台启动关闭方式ctrl+c结束进程</span></span><br></pre></td></tr></table></figure>
<mark class="hl-label green">后台启动</mark> 
<p><strong>后台启动</strong>，输出日志信息在/root目录下nohup.out</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /mysoft/apache-hive-3.1.2-bin/bin/hive --service metastore &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment">#后台挂起启动结束进程使用jps查看进程 使用kill -9 杀死进程</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#nohup 命令，在默认情况下（非重定向时），会输出一个名叫nohup.out 的文件到当前目录下</span></span><br></pre></td></tr></table></figure>
<h4 id="Apache-Hive客户端使用"><a href="#Apache-Hive客户端使用" class="headerlink" title="Apache Hive客户端使用"></a>Apache Hive客户端使用</h4><h5 id="（1）Hive自带客户端"><a href="#（1）Hive自带客户端" class="headerlink" title="（1）Hive自带客户端"></a>（1）Hive自带客户端</h5><p><code>bin/hive</code>、<code>bin/beeline</code></p>
<p><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-13-11-26-19.png" alt="Remote Metastore远程模式"></p>
<ul>
<li>Hive发展至今，总共历经了两代客户端工具。</li>
<li>第一代客户端（deprecated不推荐使用）：<code>$HIVE_HOME/bin/hive</code>, 是一个shellUtil。主要功能：一是可用于以交互或批处理模式运行Hive查询；二是用于Hive相关服务的启动，比如metastore服务。</li>
<li>第二代客户端（recommended 推荐使用）：<code>$HIVE_HOME/bin/beeline</code>，是一个JDBC客户端，是<strong>官方强烈推荐使用</strong>的Hive命令行工具，和第一代客户端相比，性能加强安全性提高。</li>
</ul>
<h5 id="HiveServer2服务介绍"><a href="#HiveServer2服务介绍" class="headerlink" title="HiveServer2服务介绍"></a>HiveServer2服务介绍</h5><ul>
<li><strong>远程模式下beeline通过Thrift 连接到单独的HiveServer2服务上</strong>，这也是官方推荐在生产环境中使用的模式。</li>
<li>HiveServer2支持多客户端的并发和身份认证，旨在为开放API客户端如JDBC、ODBC提供更好的支持。</li>
</ul>
<h5 id="Hive客户端和服务的关系"><a href="#Hive客户端和服务的关系" class="headerlink" title="Hive客户端和服务的关系"></a>Hive客户端和服务的关系</h5><ul>
<li><p>HiveServer2通过Metastore服务读写元数据。所以在远程模式下，<strong>启动HiveServer2之前必须先首先启动metastore服务</strong>。</p>
</li>
<li><p>特别注意：远程模式下，<code>Beeline</code>客户端只能通过HiveServer2服务访问Hive。而<code>bin/hive</code>是通过Metastore服务访问的。具体关系如下：<br><img src= "/img/loading.gif" data-lazy-src="/2022/11/12/HadoopBase/2023-01-13-11-33-50.png" alt="Hive客户端和服务的关系"></p>
</li>
</ul>
<h5 id="bin-beeline客户端使用"><a href="#bin-beeline客户端使用" class="headerlink" title="bin/beeline客户端使用"></a><code>bin/beeline</code>客户端使用</h5><ul>
<li><p>在hive安装的服务器上，<strong>首先启动metastore服务，然后启动hiveserver2服务</strong>。</p>
<figure class="highlight bash"><figcaption><span>先启动metastore服务然后启动hiveserver2服务</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> /mysoft/apache-hive-3.1.2-bin/bin/hive --service metastore &amp;</span><br><span class="line"><span class="built_in">nohup</span> /mysoft/apache-hive-3.1.2-bin/bin/hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure>
<div class="note warning modern"><p>启动hiveserver2需要一定的时间  不要启动之后立即beeline连接 可能连接不上</p>
</div>
</li>
<li><p>在node3上使用beeline客户端进行连接访问。需要注意<strong>hiveserver2服务启动之后需要稍等一会才可以对外提供服务</strong>。</p>
<figure class="highlight bash"><figcaption><span>拷贝node1安装包到beeline客户端机器上（node3）</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /mysoft/apache-hive-3.1.2-bin/ hsq03:/mysoft/</span><br></pre></td></tr></table></figure>
<div class="note info modern"><p>node3中同样需要配置hadoop集群<code>core-site.xml</code>文件，<a href="#Apache-Hive部署实战">同上</a></p>
</div>
</li>
<li><p>Beeline是JDBC的客户端，通过JDBC协议和Hiveserver2服务进行通信，协议的地址是：jdbc:hive2://node1:10000</p>
<figure class="highlight bash"><figcaption><span>连接访问</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/mysoft/apache-hive-3.1.2-bin/bin/beeline</span><br><span class="line"></span><br><span class="line">beeline&gt; ! connect jdbc:hive2://node1:10000</span><br><span class="line">beeline&gt; root</span><br><span class="line">beeline&gt; 直接回车</span><br></pre></td></tr></table></figure>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://cmwlvip.github.io">Shiqing Huang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://cmwlvip.github.io/2022/11/12/HadoopBase/">https://cmwlvip.github.io/2022/11/12/HadoopBase/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://cmwlvip.github.io" target="_blank">Ofra Serendipity</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src= "/img/loading.gif" data-lazy-src="/img/wechat.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "/img/loading.gif" data-lazy-src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/11/07/TypeScript/"><img class="prev-cover" src= "/img/loading.gif" data-lazy-src="/2022/11/07/TypeScript/TypeScript.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">TypeScript</div></div></a></div><div class="next-post pull-right"><a href="/2022/11/20/AboutNPM/"><img class="next-cover" src= "/img/loading.gif" data-lazy-src="/2022/11/20/AboutNPM/npm.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">npm——Node 包管理器</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/10/25/HadoopClusterBuilding3-3-4/" title="Hadoop 3.3.4 集群搭建"><img class="cover" src= "/img/loading.gif" data-lazy-src="https://pic1.imgdb.cn/item/6366396d16f2c2beb1036f1c.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-25</div><div class="title">Hadoop 3.3.4 集群搭建</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Livere</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="lv-container" data-id="city" data-uid="MTAyMC81ODA4MS8zNDU0NA=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "/img/loading.gif" data-lazy-src="/img/avatar002.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Shiqing Huang</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cmwlvip"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/cmwlvip" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://gitee.com/cmwlvip" target="_blank" title="Gitee"><i class="fab fa-git"></i></a><a class="social-icon" href="mailto:2689050828@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AF%BC%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">大数据导论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%81%E4%B8%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E5%90%91"><span class="toc-number">1.1.</span> <span class="toc-text">企业数据分析方向</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.1.1.</span> <span class="toc-text">数据是什么</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A6%82%E4%BD%95%E4%BA%A7%E7%94%9F"><span class="toc-number">1.1.2.</span> <span class="toc-text">数据如何产生</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E6%9E%90%E6%96%B9%E5%90%91"><span class="toc-number">1.1.3.</span> <span class="toc-text">分析方向</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.2.</span> <span class="toc-text">数据分析基本步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step1%EF%BC%9A%E6%98%8E%E7%A1%AE%E5%88%86%E6%9E%90%E7%9B%AE%E7%9A%84%E5%92%8C%E6%80%9D%E8%B7%AF"><span class="toc-number">1.2.1.</span> <span class="toc-text">Step1：明确分析目的和思路</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step2%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="toc-number">1.2.2.</span> <span class="toc-text">Step2：数据收集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step3%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">1.2.3.</span> <span class="toc-text">Step3：数据处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step4%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-number">1.2.4.</span> <span class="toc-text">Step4：数据分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step5%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%B1%95%E7%8E%B0"><span class="toc-number">1.2.5.</span> <span class="toc-text">Step5：数据展现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step6%EF%BC%9A%E6%8A%A5%E5%91%8A%E6%92%B0%E5%86%99"><span class="toc-number">1.2.6.</span> <span class="toc-text">Step6：报告撰写</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E4%BB%A3"><span class="toc-number">1.3.</span> <span class="toc-text">大数据时代</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%B6%E4%BB%A3%E8%83%8C%E6%99%AF"><span class="toc-number">1.3.1.</span> <span class="toc-text">大数据时代背景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89"><span class="toc-number">1.3.2.</span> <span class="toc-text">大数据定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE5V%E7%89%B9%E5%BE%81"><span class="toc-number">1.3.3.</span> <span class="toc-text">大数据5V特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.3.4.</span> <span class="toc-text">应用场景</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E9%9B%86%E7%BE%A4"><span class="toc-number">1.4.</span> <span class="toc-text">分布式与集群</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8"><span class="toc-number">1.4.1.</span> <span class="toc-text">应用</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Apache-Hadoop%E3%80%81HDFS"><span class="toc-number">2.</span> <span class="toc-text">Apache Hadoop、HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Apache-Hadoop%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">Apache Hadoop概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E4%BB%8B%E7%BB%8D%E3%80%81%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2%E3%80%81%E7%8E%B0%E7%8A%B6"><span class="toc-number">2.1.1.</span> <span class="toc-text">Hadoop介绍、发展简史、现状</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">Hadoop介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">Hadoop发展简史</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E7%8E%B0%E7%8A%B6"><span class="toc-number">2.1.1.3.</span> <span class="toc-text">Hadoop现状</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E7%89%B9%E6%80%A7%E4%BC%98%E7%82%B9%E3%80%81%E5%9B%BD%E5%86%85%E5%A4%96%E5%BA%94%E7%94%A8"><span class="toc-number">2.1.2.</span> <span class="toc-text">Hadoop特性优点、国内外应用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E7%89%B9%E6%80%A7%E4%BC%98%E7%82%B9"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">Hadoop特性优点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E5%9B%BD%E5%A4%96%E5%BA%94%E7%94%A8"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">Hadoop国外应用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E5%9B%BD%E5%86%85%E5%BA%94%E7%94%A8"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">Hadoop国内应用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC%E3%80%81%E6%9E%B6%E6%9E%84%E5%8F%98%E8%BF%81"><span class="toc-number">2.1.3.</span> <span class="toc-text">Hadoop发行版本、架构变迁</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">Hadoop发行版本</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E6%9E%B6%E6%9E%84%E5%8F%98%E8%BF%81%EF%BC%881-0-2-0%E5%8F%98%E8%BF%81%EF%BC%89"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">Hadoop架构变迁（1.0-2.0变迁）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E6%9E%B6%E6%9E%84%E5%8F%98%E8%BF%81%EF%BC%883-0%E6%96%B0%E7%89%88%E6%9C%AC%EF%BC%89"><span class="toc-number">2.1.3.3.</span> <span class="toc-text">Hadoop架构变迁（3.0新版本）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Apache-Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-number">2.2.</span> <span class="toc-text">Apache Hadoop集群搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E9%9B%86%E7%BE%A4%E7%AE%80%E4%BB%8B"><span class="toc-number">2.2.1.</span> <span class="toc-text">Hadoop集群简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F-%E5%88%86%E5%B8%83%E5%BC%8F-%E5%AE%89%E8%A3%85%EF%BC%88Cluster-mode%EF%BC%89"><span class="toc-number">2.2.2.</span> <span class="toc-text">Hadoop集群模式(分布式)安装（Cluster mode）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">Hadoop源码编译</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step1-%E9%9B%86%E7%BE%A4%E8%A7%92%E8%89%B2%E8%A7%84%E5%88%92"><span class="toc-number">2.2.2.2.</span> <span class="toc-text">Step1:集群角色规划</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step2-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">2.2.2.3.</span> <span class="toc-text">Step2:服务器基础环境准备</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step3-%E4%B8%8A%E4%BC%A0%E5%AE%89%E8%A3%85%E5%8C%85%E3%80%81%E8%A7%A3%E5%8E%8B%E5%AE%89%E8%A3%85%E5%8C%85"><span class="toc-number">2.2.2.4.</span> <span class="toc-text">Step3:上传安装包、解压安装包</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step4-Hadoop%E5%AE%89%E8%A3%85%E5%8C%85%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="toc-number">2.2.2.5.</span> <span class="toc-text">Step4:Hadoop安装包目录结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%A6%82%E8%BF%B0"><span class="toc-number">2.2.2.6.</span> <span class="toc-text">配置文件概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-hadoop-env-sh"><span class="toc-number">2.2.2.7.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 hadoop-env.sh</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-core-site-xml"><span class="toc-number">2.2.2.8.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 core-site.xml</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6hdfs-site-xml"><span class="toc-number">2.2.2.9.</span> <span class="toc-text">Step5:编辑Hadoop配置文件hdfs-site.xml</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-mapred-site-xml"><span class="toc-number">2.2.2.10.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 mapred-site.xml</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-yarn-site-xml"><span class="toc-number">2.2.2.11.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 yarn-site.xml</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step5-%E7%BC%96%E8%BE%91Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-workers"><span class="toc-number">2.2.2.12.</span> <span class="toc-text">Step5:编辑Hadoop配置文件 workers</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step6-%E5%88%86%E5%8F%91%E5%90%8C%E6%AD%A5%E5%AE%89%E8%A3%85%E5%8C%85"><span class="toc-number">2.2.2.13.</span> <span class="toc-text">Step6:分发同步安装包</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step7-%E9%85%8D%E7%BD%AEHadoop%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">2.2.2.14.</span> <span class="toc-text">Step7:配置Hadoop环境变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Step8-NameNode-format%EF%BC%88%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%93%8D%E4%BD%9C%EF%BC%89"><span class="toc-number">2.2.2.15.</span> <span class="toc-text">Step8:NameNode format（格式化操作）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E9%9B%86%E7%BE%A4%E5%90%AF%E5%81%9C%E5%91%BD%E4%BB%A4%E3%80%81Web-UI"><span class="toc-number">2.2.3.</span> <span class="toc-text">Hadoop集群启停命令、Web UI</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E9%80%90%E4%B8%AA%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%81%9C"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">手动逐个进程启停</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#shell%E8%84%9A%E6%9C%AC%E4%B8%80%E9%94%AE%E5%90%AF%E5%81%9C"><span class="toc-number">2.2.3.2.</span> <span class="toc-text">shell脚本一键启停</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%9B%E7%A8%8B%E7%8A%B6%E6%80%81%E3%80%81%E6%97%A5%E5%BF%97%E6%9F%A5%E7%9C%8B"><span class="toc-number">2.2.3.3.</span> <span class="toc-text">进程状态、日志查看</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS%E9%9B%86%E7%BE%A4web%E7%95%8C%E9%9D%A2"><span class="toc-number">2.2.3.4.</span> <span class="toc-text">HDFS集群web界面</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E9%9B%86%E7%BE%A4web%E7%95%8C%E9%9D%A2"><span class="toc-number">2.2.3.5.</span> <span class="toc-text">YARN集群web界面</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-number">2.2.4.</span> <span class="toc-text">Hadoop初体验</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS-%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-number">2.2.4.1.</span> <span class="toc-text">HDFS 初体验</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#shell%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.4.1.1.</span> <span class="toc-text">shell命令操作</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Web-UI%E9%A1%B5%E9%9D%A2%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.4.1.2.</span> <span class="toc-text">Web UI页面操作</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce-YARN%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-number">2.2.4.2.</span> <span class="toc-text">MapReduce+YARN初体验</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80"><span class="toc-number">2.3.</span> <span class="toc-text">HDFS分布式文件系统基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-number">2.3.1.</span> <span class="toc-text">文件系统、分布式文件系统</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AE%9A%E4%B9%89"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">文件系统定义</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">传统常见的文件系统</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E3%80%81%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.3.1.3.</span> <span class="toc-text">数据、元数据</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">2.3.1.3.1.</span> <span class="toc-text">数据</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.3.1.3.2.</span> <span class="toc-text">元数据</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">2.3.1.4.</span> <span class="toc-text">海量数据存储遇到的问题</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%A0%B8%E5%BF%83%E5%B1%9E%E6%80%A7%E5%8F%8A%E5%8A%9F%E8%83%BD%E5%90%AB%E4%B9%89"><span class="toc-number">2.3.2.</span> <span class="toc-text">分布式存储系统的核心属性及功能含义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">分布式存储的优点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E8%AE%B0%E5%BD%95%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">元数据记录的功能</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E5%9D%97%E5%AD%98%E5%82%A8%E5%A5%BD%E5%A4%84"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">分块存储好处</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">2.3.2.4.</span> <span class="toc-text">副本机制的作用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E7%AE%80%E4%BB%8B"><span class="toc-number">2.3.3.</span> <span class="toc-text">HDFS简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E8%B5%B7%E6%BA%90%E5%8F%91%E5%B1%95%E3%80%81%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87"><span class="toc-number">2.3.4.</span> <span class="toc-text">HDFS起源发展、设计目标</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS%E8%B5%B7%E6%BA%90%E5%8F%91%E5%B1%95"><span class="toc-number">2.3.4.1.</span> <span class="toc-text">HDFS起源发展</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87"><span class="toc-number">2.3.4.2.</span> <span class="toc-text">HDFS设计目标</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">2.3.5.</span> <span class="toc-text">HDFS应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E9%87%8D%E8%A6%81%E7%89%B9%E6%80%A7"><span class="toc-number">2.3.6.</span> <span class="toc-text">HDFS重要特性</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%A6%82%E8%BF%B0"><span class="toc-number">2.3.6.1.</span> <span class="toc-text">整体概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84"><span class="toc-number">2.3.6.2.</span> <span class="toc-text">（1）主从架构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%88%86%E5%9D%97%E5%AD%98%E5%82%A8"><span class="toc-number">2.3.6.3.</span> <span class="toc-text">（2）分块存储</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6"><span class="toc-number">2.3.6.4.</span> <span class="toc-text">（3）副本机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86"><span class="toc-number">2.3.6.5.</span> <span class="toc-text">（4）元数据管理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%885%EF%BC%89namespace"><span class="toc-number">2.3.6.6.</span> <span class="toc-text">（5）namespace</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%886%EF%BC%89%E6%95%B0%E6%8D%AE%E5%9D%97%E5%AD%98%E5%82%A8"><span class="toc-number">2.3.6.7.</span> <span class="toc-text">（6）数据块存储</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-shell%E6%93%8D%E4%BD%9C"><span class="toc-number">2.4.</span> <span class="toc-text">HDFS shell操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS-shell%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%A7%A3%E9%87%8A%E8%AF%B4%E6%98%8E"><span class="toc-number">2.4.1.</span> <span class="toc-text">HDFS shell命令行解释说明</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%8D%8F%E8%AE%AE"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">文件系统协议</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8C%BA%E5%88%AB"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">区别</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="toc-number">2.4.1.3.</span> <span class="toc-text">参数说明</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS-shell%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">2.4.2.</span> <span class="toc-text">HDFS shell命令行常用操作</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-number">2.4.2.1.</span> <span class="toc-text">创建文件夹</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%8C%87%E5%AE%9A%E7%9B%AE%E5%BD%95%E4%B8%8B%E5%86%85%E5%AE%B9"><span class="toc-number">2.4.2.2.</span> <span class="toc-text">查看指定目录下内容</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%88%B0HDFS%E6%8C%87%E5%AE%9A%E7%9B%AE%E5%BD%95%E4%B8%8B"><span class="toc-number">2.4.2.3.</span> <span class="toc-text">上传文件到HDFS指定目录下</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8BHDFS%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9"><span class="toc-number">2.4.2.4.</span> <span class="toc-text">查看HDFS文件内容</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BDHDFS%E6%96%87%E4%BB%B6"><span class="toc-number">2.4.2.5.</span> <span class="toc-text">下载HDFS文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8B%B7%E8%B4%9DHDFS%E6%96%87%E4%BB%B6"><span class="toc-number">2.4.2.6.</span> <span class="toc-text">拷贝HDFS文件</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%BD%E5%8A%A0%E6%95%B0%E6%8D%AE%E5%88%B0HDFS%E6%96%87%E4%BB%B6%E4%B8%AD"><span class="toc-number">2.4.2.7.</span> <span class="toc-text">追加数据到HDFS文件中</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS%E6%95%B0%E6%8D%AE%E7%A7%BB%E5%8A%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">2.4.2.8.</span> <span class="toc-text">HDFS数据移动操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS-shell%E5%85%B6%E4%BB%96%E5%91%BD%E4%BB%A4"><span class="toc-number">2.4.2.9.</span> <span class="toc-text">HDFS shell其他命令</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E4%B8%8E%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.</span> <span class="toc-text">HDFS工作流程与机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E9%9B%86%E7%BE%A4%E8%A7%92%E8%89%B2%E4%B8%8E%E8%81%8C%E8%B4%A3"><span class="toc-number">2.5.1.</span> <span class="toc-text">HDFS集群角色与职责</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%98%E6%96%B9%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-number">2.5.1.1.</span> <span class="toc-text">官方架构图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BB%E8%A7%92%E8%89%B2%EF%BC%9Anamenode"><span class="toc-number">2.5.1.2.</span> <span class="toc-text">主角色：namenode</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%8E%E8%A7%92%E8%89%B2%EF%BC%9Adatanode"><span class="toc-number">2.5.1.3.</span> <span class="toc-text">从角色：datanode</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BB%E8%A7%92%E8%89%B2%E8%BE%85%E5%8A%A9%E8%A7%92%E8%89%B2%EF%BC%9Asecondarynamenode"><span class="toc-number">2.5.1.4.</span> <span class="toc-text">主角色辅助角色：secondarynamenode</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#namenode%E8%81%8C%E8%B4%A3"><span class="toc-number">2.5.1.5.</span> <span class="toc-text">namenode职责</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#datanode%E8%81%8C%E8%B4%A3"><span class="toc-number">2.5.1.6.</span> <span class="toc-text">datanode职责</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%EF%BC%88%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%EF%BC%89"><span class="toc-number">2.5.2.</span> <span class="toc-text">HDFS写数据流程（上传文件）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%86%99%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">2.5.2.1.</span> <span class="toc-text">写数据完整流程图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E2%80%94Pipeline%E7%AE%A1%E9%81%93"><span class="toc-number">2.5.2.2.</span> <span class="toc-text">核心概念—Pipeline管道</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E2%80%94ACK%E5%BA%94%E7%AD%94%E5%93%8D%E5%BA%94"><span class="toc-number">2.5.2.3.</span> <span class="toc-text">核心概念—ACK应答响应</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E2%80%94%E9%BB%98%E8%AE%A43%E5%89%AF%E6%9C%AC%E5%AD%98%E5%82%A8%E7%AD%96%E7%95%A5"><span class="toc-number">2.5.2.4.</span> <span class="toc-text">核心概念—默认3副本存储策略</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%86%99%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B%E5%9B%BE%E6%96%87%E5%AD%97%E6%8F%8F%E8%BF%B0"><span class="toc-number">2.5.2.5.</span> <span class="toc-text">写数据完整流程图文字描述</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%EF%BC%88%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%EF%BC%89"><span class="toc-number">2.5.3.</span> <span class="toc-text">HDFS读数据流程（下载文件）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%BB%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">2.5.3.1.</span> <span class="toc-text">读数据完整流程图</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop-MapReduce%E4%B8%8EHadoop-YARN"><span class="toc-number">3.</span> <span class="toc-text">Hadoop MapReduce与Hadoop YARN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop-MapReduce"><span class="toc-number">3.1.</span> <span class="toc-text">Hadoop MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MapReduce%E6%80%9D%E6%83%B3"><span class="toc-number">3.1.1.</span> <span class="toc-text">MapReduce思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-MapReduce%E8%AE%BE%E8%AE%A1%E6%9E%84%E6%80%9D"><span class="toc-number">3.1.2.</span> <span class="toc-text">Hadoop MapReduce设计构思</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%A6%82%E4%BD%95%E5%AF%B9%E4%BB%98%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%9C%BA%E6%99%AF"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">（1）如何对付大数据处理场景</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%9E%84%E5%BB%BA%E6%8A%BD%E8%B1%A1%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">（2）构建抽象编程模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E7%BB%9F%E4%B8%80%E6%9E%B6%E6%9E%84%E3%80%81%E9%9A%90%E8%97%8F%E5%BA%95%E5%B1%82%E7%BB%86%E8%8A%82"><span class="toc-number">3.1.2.3.</span> <span class="toc-text">（3）统一架构、隐藏底层细节</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-MapReduce%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.1.3.</span> <span class="toc-text">Hadoop MapReduce介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A6%82%E5%BF%B5"><span class="toc-number">3.1.3.1.</span> <span class="toc-text">分布式计算概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hadoop-MapReduce%E6%A6%82%E8%BF%B0"><span class="toc-number">3.1.3.2.</span> <span class="toc-text">Hadoop MapReduce概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E4%BA%A7%E7%94%9F%E8%83%8C%E6%99%AF"><span class="toc-number">3.1.3.3.</span> <span class="toc-text">MapReduce产生背景</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E7%89%B9%E7%82%B9"><span class="toc-number">3.1.3.4.</span> <span class="toc-text">MapReduce特点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">3.1.3.5.</span> <span class="toc-text">MapReduce局限性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E5%AE%9E%E4%BE%8B%E8%BF%9B%E7%A8%8B"><span class="toc-number">3.1.3.6.</span> <span class="toc-text">MapReduce实例进程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E9%98%B6%E6%AE%B5%E7%BB%84%E6%88%90"><span class="toc-number">3.1.3.7.</span> <span class="toc-text">MapReduce阶段组成</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.1.3.8.</span> <span class="toc-text">MapReduce数据类型</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-MapReduce%E5%AE%98%E6%96%B9%E7%A4%BA%E4%BE%8B"><span class="toc-number">3.1.4.</span> <span class="toc-text">Hadoop MapReduce官方示例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E7%A4%BA%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="toc-number">3.1.4.1.</span> <span class="toc-text">MapReduce示例说明</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E5%9C%86%E5%91%A8%E7%8E%87%CF%80%EF%BC%88PI%EF%BC%89%E7%9A%84%E5%80%BC"><span class="toc-number">3.1.4.2.</span> <span class="toc-text">评估圆周率π（PI）的值</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Monte-Carlo%E6%96%B9%E6%B3%95"><span class="toc-number">3.1.4.2.1.</span> <span class="toc-text">Monte Carlo方法</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E5%9C%86%E5%91%A8%E7%8E%87%CF%80%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">3.1.4.2.2.</span> <span class="toc-text">评估圆周率π参数设置</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#wordcount%E5%8D%95%E8%AF%8D%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1"><span class="toc-number">3.1.4.3.</span> <span class="toc-text">wordcount单词词频统计</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#WordCount%E6%A6%82%E8%BF%B0"><span class="toc-number">3.1.4.3.1.</span> <span class="toc-text">WordCount概述</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#WordCount%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF"><span class="toc-number">3.1.4.3.2.</span> <span class="toc-text">WordCount编程实现思路</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#WordCount%E7%A8%8B%E5%BA%8F%E6%8F%90%E4%BA%A4"><span class="toc-number">3.1.4.3.3.</span> <span class="toc-text">WordCount程序提交</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Map%E9%98%B6%E6%AE%B5%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.5.</span> <span class="toc-text">Map阶段执行流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#WordCount%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">3.1.5.1.</span> <span class="toc-text">WordCount执行流程图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MapReduce%E6%95%B4%E4%BD%93%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">3.1.5.2.</span> <span class="toc-text">MapReduce整体执行流程图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Map%E9%98%B6%E6%AE%B5%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="toc-number">3.1.5.3.</span> <span class="toc-text">Map阶段执行过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reduce%E9%98%B6%E6%AE%B5%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.6.</span> <span class="toc-text">Reduce阶段执行流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Shuffle%E6%9C%BA%E5%88%B6"><span class="toc-number">3.1.7.</span> <span class="toc-text">Shuffle机制</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#shuffle%E6%A6%82%E5%BF%B5"><span class="toc-number">3.1.7.1.</span> <span class="toc-text">shuffle概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Map%E7%AB%AFShuffle"><span class="toc-number">3.1.7.2.</span> <span class="toc-text">Map端Shuffle</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Reducer%E7%AB%AFshuffle"><span class="toc-number">3.1.7.3.</span> <span class="toc-text">Reducer端shuffle</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#shuffle%E6%9C%BA%E5%88%B6%E5%BC%8A%E7%AB%AF"><span class="toc-number">3.1.7.4.</span> <span class="toc-text">shuffle机制弊端</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop-YARN"><span class="toc-number">3.2.</span> <span class="toc-text">Hadoop YARN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-YARN%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.2.1.</span> <span class="toc-text">Hadoop YARN介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E7%AE%80%E4%BB%8B"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">YARN简介</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E5%8A%9F%E8%83%BD%E8%AF%B4%E6%98%8E"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">YARN功能说明</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E6%A6%82%E8%BF%B0"><span class="toc-number">3.2.1.3.</span> <span class="toc-text">YARN概述</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop-YARN%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%84%E4%BB%B6"><span class="toc-number">3.2.2.</span> <span class="toc-text">Hadoop YARN架构、组件</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN%E5%AE%98%E6%96%B9%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">YARN官方架构图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%98%E6%96%B9%E6%9E%B6%E6%9E%84%E5%9B%BE%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">3.2.2.2.</span> <span class="toc-text">官方架构图中出现的概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#YARN3%E5%A4%A7%E7%BB%84%E4%BB%B6"><span class="toc-number">3.2.2.3.</span> <span class="toc-text">YARN3大组件</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E6%8F%90%E4%BA%A4YARN%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.3.</span> <span class="toc-text">程序提交YARN交互流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.3.1.</span> <span class="toc-text">核心交互流程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B%E6%A6%82%E8%BF%B0"><span class="toc-number">3.2.3.2.</span> <span class="toc-text">交互流程概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MR%E6%8F%90%E4%BA%A4YARN%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.3.3.</span> <span class="toc-text">MR提交YARN交互流程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#YARN%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8Scheduler"><span class="toc-number">3.2.4.</span> <span class="toc-text">YARN资源调度器Scheduler</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MR%E7%A8%8B%E5%BA%8F%E6%8F%90%E4%BA%A4YARN%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B"><span class="toc-number">3.2.4.1.</span> <span class="toc-text">MR程序提交YARN交互流程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6"><span class="toc-number">3.2.4.2.</span> <span class="toc-text">如何理解资源调度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E5%99%A8%E7%AD%96%E7%95%A5"><span class="toc-number">3.2.4.3.</span> <span class="toc-text">调度器策略</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%9F%BA%E7%A1%80%E4%B8%8EApache-Hive%E5%85%A5%E9%97%A8"><span class="toc-number">4.</span> <span class="toc-text">数据仓库基础与Apache Hive入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.</span> <span class="toc-text">数据仓库基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.1.</span> <span class="toc-text">数据仓库概念</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E4%BB%93%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.1.1.</span> <span class="toc-text">数仓概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E4%BB%93%E4%B8%93%E6%B3%A8%E5%88%86%E6%9E%90"><span class="toc-number">4.1.1.2.</span> <span class="toc-text">数仓专注分析</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E6%A1%88%E4%BE%8B%EF%BC%9A%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%BA%E4%BD%95%E8%80%8C%E6%9D%A5"><span class="toc-number">4.1.2.</span> <span class="toc-text">场景案例：数据仓库为何而来</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#OLTP%E7%8E%AF%E5%A2%83%E5%BC%80%E5%B1%95%E5%88%86%E6%9E%90%E5%8F%AF%E8%A1%8C%E5%90%97%EF%BC%9F"><span class="toc-number">4.1.2.1.</span> <span class="toc-text">OLTP环境开展分析可行吗？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E9%9D%A2%E4%B8%96"><span class="toc-number">4.1.2.2.</span> <span class="toc-text">数据仓库面世</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E6%9E%84%E5%BB%BA"><span class="toc-number">4.1.2.3.</span> <span class="toc-text">数据仓库的构建</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%BB%E8%A6%81%E7%89%B9%E5%BE%81"><span class="toc-number">4.1.3.</span> <span class="toc-text">数据仓库主要特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%BB%E6%B5%81%E5%BC%80%E5%8F%91%E8%AF%AD%E8%A8%80%E2%80%94SQL"><span class="toc-number">4.1.4.</span> <span class="toc-text">数据仓库主流开发语言—SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91%E8%AF%AD%E8%A8%80%E6%A6%82%E8%BF%B0"><span class="toc-number">4.1.4.1.</span> <span class="toc-text">数仓开发语言概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SQL%E8%AF%AD%E8%A8%80%E4%BB%8B%E7%BB%8D"><span class="toc-number">4.1.4.2.</span> <span class="toc-text">SQL语言介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E4%BB%93%E4%B8%8ESQL"><span class="toc-number">4.1.4.3.</span> <span class="toc-text">数仓与SQL</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE"><span class="toc-number">4.1.4.4.</span> <span class="toc-text">结构化数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%8C%E7%BB%B4%E8%A1%A8%E7%BB%93%E6%9E%84"><span class="toc-number">4.1.4.5.</span> <span class="toc-text">二维表结构</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SQL%E8%AF%AD%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-number">4.1.4.6.</span> <span class="toc-text">SQL语法分类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Apache-Hive%E5%85%A5%E9%97%A8"><span class="toc-number">4.2.</span> <span class="toc-text">Apache Hive入门</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E6%A6%82%E8%BF%B0"><span class="toc-number">4.2.1.</span> <span class="toc-text">Apache Hive概述</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFHive"><span class="toc-number">4.2.1.1.</span> <span class="toc-text">什么是Hive</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8Hive"><span class="toc-number">4.2.1.2.</span> <span class="toc-text">为什么使用Hive</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E5%92%8CHadoop%E5%85%B3%E7%B3%BB"><span class="toc-number">4.2.1.3.</span> <span class="toc-text">Hive和Hadoop关系</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E8%AE%BE%E8%AE%A1%EF%BC%9A%E5%A6%82%E4%BD%95%E6%A8%A1%E6%8B%9F%E5%AE%9E%E7%8E%B0Hive%E5%8A%9F%E8%83%BD"><span class="toc-number">4.2.2.</span> <span class="toc-text">场景设计：如何模拟实现Hive功能</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%98%A0%E5%B0%84%E4%BF%A1%E6%81%AF%E8%AE%B0%E5%BD%95"><span class="toc-number">4.2.2.1.</span> <span class="toc-text">映射信息记录</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#SQL%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E3%80%81%E7%BC%96%E8%AF%91"><span class="toc-number">4.2.2.2.</span> <span class="toc-text">SQL语法解析、编译</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9Hive%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number">4.2.2.3.</span> <span class="toc-text">对Hive的理解</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E6%9E%B6%E6%9E%84%E3%80%81%E7%BB%84%E4%BB%B6"><span class="toc-number">4.2.3.</span> <span class="toc-text">Apache Hive架构、组件</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-number">4.2.3.1.</span> <span class="toc-text">Hive架构图</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E7%BB%84%E4%BB%B6"><span class="toc-number">4.2.3.2.</span> <span class="toc-text">Hive组件</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Apache-Hive%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-number">4.3.</span> <span class="toc-text">Apache Hive安装部署</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">4.3.1.</span> <span class="toc-text">Apache Hive元数据</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive-Metadata"><span class="toc-number">4.3.1.1.</span> <span class="toc-text">Hive Metadata</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#metastore%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F"><span class="toc-number">4.3.1.2.</span> <span class="toc-text">metastore配置方式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#metastore%E8%BF%9C%E7%A8%8B%E6%A8%A1%E5%BC%8F"><span class="toc-number">4.3.1.3.</span> <span class="toc-text">metastore远程模式</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E9%83%A8%E7%BD%B2%E5%AE%9E%E6%88%98"><span class="toc-number">4.3.2.</span> <span class="toc-text">Apache Hive部署实战</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#metastore%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">4.3.2.1.</span> <span class="toc-text">metastore服务启动方式</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Apache-Hive%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8"><span class="toc-number">4.3.3.</span> <span class="toc-text">Apache Hive客户端使用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89Hive%E8%87%AA%E5%B8%A6%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-number">4.3.3.1.</span> <span class="toc-text">（1）Hive自带客户端</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HiveServer2%E6%9C%8D%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">4.3.3.2.</span> <span class="toc-text">HiveServer2服务介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Hive%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%92%8C%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">4.3.3.3.</span> <span class="toc-text">Hive客户端和服务的关系</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#bin-beeline%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BD%BF%E7%94%A8"><span class="toc-number">4.3.3.4.</span> <span class="toc-text">bin&#x2F;beeline客户端使用</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/02/23/hexo-plugins/" title="hexo-plugins"><img src= "/img/loading.gif" data-lazy-src="https://pic1.imgdb.cn/item/63676da116f2c2beb11fa14a.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="hexo-plugins"/></a><div class="content"><a class="title" href="/2023/02/23/hexo-plugins/" title="hexo-plugins">hexo-plugins</a><time datetime="2023-02-23T04:22:55.000Z" title="发表于 2023-02-23 12:22:55">2023-02-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/21/Butterfly/" title="Butterfly"><img src= "/img/loading.gif" data-lazy-src="https://pic.imgdb.cn/item/63f471fdf144a0100725e5dc.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Butterfly"/></a><div class="content"><a class="title" href="/2023/02/21/Butterfly/" title="Butterfly">Butterfly</a><time datetime="2023-02-21T07:11:10.000Z" title="发表于 2023-02-21 15:11:10">2023-02-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/03/Cocos/" title="Cocos"><img src= "/img/loading.gif" data-lazy-src="/2022/12/03/Cocos/cocos.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Cocos"/></a><div class="content"><a class="title" href="/2022/12/03/Cocos/" title="Cocos">Cocos</a><time datetime="2022-12-03T14:46:46.000Z" title="发表于 2022-12-03 22:46:46">2022-12-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/03/Git/" title="Git"><img src= "/img/loading.gif" data-lazy-src="/2022/12/03/Git/git.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Git"/></a><div class="content"><a class="title" href="/2022/12/03/Git/" title="Git">Git</a><time datetime="2022-12-03T04:10:02.000Z" title="发表于 2022-12-03 12:10:02">2022-12-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/25/CSharp/" title="C#"><img src= "/img/loading.gif" data-lazy-src="/2022/11/25/CSharp/CSharp.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C#"/></a><div class="content"><a class="title" href="/2022/11/25/CSharp/" title="C#">C#</a><time datetime="2022-11-24T16:22:35.000Z" title="发表于 2022-11-25 00:22:35">2022-11-25</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic.imgdb.cn/item/63766f6616f2c2beb1356f73.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Shiqing Huang</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'rPFUMvWqmNqL8eB1VNIjOaXj-MdYXbMMI',
      appKey: 'LvfcFuXlMuqYKhZFyoFPdCsw',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Valine' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/blueimp-md5/js/md5.min.js"></script><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getIcon = (icon, mail) => {
    if (icon) return icon
    let defaultIcon = '?d=monsterid'
    let iconUrl = `https://gravatar.loli.net/avatar/${md5(mail.toLowerCase()) + defaultIcon}`
    return iconUrl
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const serverURL = 'https://rPFUMvWq.api.lncldglobal.com'

    var settings = {
      "method": "GET",
      "headers": {
        "X-LC-Id": 'rPFUMvWqmNqL8eB1VNIjOaXj-MdYXbMMI',
        "X-LC-Key": 'LvfcFuXlMuqYKhZFyoFPdCsw',
        "Content-Type": "application/json"
      },
    }

    fetch(`${serverURL}/1.1/classes/Comment?limit=6&order=-createdAt`,settings)
      .then(response => response.json())
      .then(data => {
        const valineArray = data.results.map(function (e) {
          return {
            'avatar': getIcon(e.QQAvatar, e.mail),
            'content': changeContent(e.comment),
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.updatedAt,
          }
        })
        saveToLocal.set('valine-newest-comments', JSON.stringify(valineArray), 10/(60*24))
        generateHtml(valineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      }) 
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('valine-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/movies/"]):not([href="/books/"]):not([href="/games/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>